# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëMIT 6.042J ÔΩú ËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÁöÑÊï∞Â≠¶Âü∫Á°Ä(2015¬∑ÂÆåÊï¥Áâà) - P97ÔºöL4.6 DeviationÔºö Markov & Chebyshev Bounds - ShowMeAI - BV1o64y1a7gT

In the last lectureÔºå we spent time talking about the mean or expectation in its properties„ÄÇ

 most important one being linearity„ÄÇBut let's step back now and think about what is it that the mean means„ÄÇ

 Why do we care about itÔºå We have this intuitive idea that if you do things long enough„ÄÇ

 if you keep experimenting with the same random variable collecting its values„ÄÇ

 Its long run average will be about the same as its meanÔºå Now„ÄÇ

 we're going to try to make that more precise„ÄÇ So we're going to talk about the topic of deviation from the mean„ÄÇ

 or as I like to sayÔºå what does the mean really meanÔºå Why do we care about it„ÄÇWell„ÄÇ

 let's look at an example that's familiar to get a grip on the specific ideas that we're interested in„ÄÇ

 So suppose I taught us a fair coin 101 times„ÄÇThen I know that the expected number since all the values from 0 through 101 are possible„ÄÇ

 and the middle value is the expectation„ÄÇIt's 50 and a half heads„ÄÇ Now„ÄÇ

 I'm never going get exactly 50 and a half heads„ÄÇ The probability in 1001 flips of getting 51„ÄÇ

50 and a half heads is 0 because there's no way to flip half head„ÄÇ

So you don't expect the expectation in that sense„ÄÇ No given measurement„ÄÇ

 no given experiment is going to yield the expectation„ÄÇ

 The expectation is this thing that we expect to come out on the average„ÄÇWellÔºå we can ask„ÄÇ

 what's the probability of getting as close as you could hope to get to the expectationÔºå namely„ÄÇ

 what's the probability of getting exactly 50 heads„ÄÇAnd it's about 1Ôºå13th„ÄÇ Or if you ask„ÄÇ

 what's the probability of getting either 50 or 51 heads been being within plus or -1 of the expectation„ÄÇ

 It's about  oneÔºå7th„ÄÇOkayÔºå let's flip more coins and see what happens„ÄÇ This time„ÄÇ

 I'm going to flip 1000 in one coins„ÄÇAnd againÔºå the expected number of heads is 5 and a half„ÄÇ

 which I'll never get exactly„ÄÇ The probability of getting exactly 500 heads is 1 39th„ÄÇ

 and the probability of getting within one of the expectation„ÄÇ

 That is either 50 or 501 heads is about  one19th„ÄÇ Now„ÄÇ

 these numbers have gone down from the previous numbers„ÄÇ Remember„ÄÇ

 it was about one7th and we've gone down to 119th„ÄÇ So it's actually„ÄÇ

 we're less likely to be within a fixed distance within one of the expectation when we flip more coins„ÄÇ

So as the number of tosses growsÔºå the number of heads gets gets less likely to be within any given fixed distance of the mean„ÄÇ

But things get better when we start looking at percentages„ÄÇ

 So what's the probability of being within 1 per cent of the mean if I tos 1000 and1 coins„ÄÇWell„ÄÇ

 1 per of 1001 is about 10„ÄÇ So we're talking about 1 per of the thousand01„ÄÇ

 and the probability of being within 10 of 505„ÄÇ That is to say„ÄÇ

 the probability of being within 510 and 490„ÄÇIs about„ÄÇin49Ôºå It's almost 50Ôºå50„ÄÇ

 which is not really so bad„ÄÇ So we have a 5050 chance of actually being within 1 per cent of the expected number When I flip 1001 coins„ÄÇ

So what we can start to say is that when we're trying to give the meaning to the mean„ÄÇ

 if I let mu be the standard abbreviation for expectation of R„ÄÇ

 I'm doing that just so it'll fit on the slide nicely in formula So mu is the expectation of R„ÄÇ

 The basic question we're asking is two basic questions„ÄÇ One is„ÄÇ

 what's the probability that the random variable is far from its mean muu„ÄÇ

You could praise that as what's the probability that the distance from R to mu„ÄÇ

 the absolute value of R minus mu is greater than some amount x„ÄÇ

 And the second question that we want to ask isÔºå what's the average deviation„ÄÇ

 What's the expectation„ÄÇOfOf the distance between R minus mu„ÄÇ

 What's the expected value of R minus mu„ÄÇNowÔºå of course„ÄÇ

 we're trying to make sense of the meaning of the expectation in terms of the expectation of the distance between Orest expectation„ÄÇ

 So there's a little bit of circularity there„ÄÇ But let's live with it and proceed„ÄÇ

Let's look at an example to crystallize the ideas a little„ÄÇ

 Let's look at two dice with the same mean„ÄÇ The green dye is going to be a fair„ÄÇ

 a standard fair dye in which each of the numbers  one through 6„ÄÇ

 has an equal probability of showing up and its expected value is exactly going to be the midpoint between one and 6„ÄÇ

 or3 and a2Ôºå NowÔºå suppose I look at a loaded dye die 2Ôºå which only throws a one or a 6„ÄÇ

 but with equal probability„ÄÇThen its expectation is also3 and/ half by the same reasoning„ÄÇ

 So here are these two different I„ÄÇ One takes the valuesÔºå1 through 6Ôºå equally likely„ÄÇ

 and the other takes only the two values 1 and 6Ôºå but they have the same expectation„ÄÇ

So how do I capture the differenceÔºå WellÔºå if I look at the expected distance of the fair die to its mean„ÄÇ

I claim it's one and a half„ÄÇBut the expected distance of the loaded die from its mean„ÄÇ same mean„ÄÇ

 Remember 3 and a half is actually 2 and a half„ÄÇ In fact„ÄÇ

 the second die is always exactly2 and a half from its expected value„ÄÇ

Let's look at the Pdfs to get a grip on understanding what's going on„ÄÇ

 So here's the P Df for the fair die„ÄÇ The theyre over one through6„ÄÇ the probability is1Ôºå6„ÄÇ

 So each of those green spikes is columns is oneÔºå6 high„ÄÇ

 and their total is the one is the probability that the fair dye takes one of those values„ÄÇ

1 through 6 with equally likeÔºå with equal likelihood„ÄÇNow„ÄÇ

 the expected value is exactly in the middle at 3 and a2„ÄÇ and the average distance of these points„ÄÇ

 WellÔºå you can see that a third of the time you're a distance a half„ÄÇ

 a third of the time that is when you take the values 2 and 5Ôºå you are at distance„ÄÇ

 exactly one and a half„ÄÇ and another third of the time youre at distanceÔºå2 and a half„ÄÇ

 when you take one in„ÄÇ6Ôºå and that averages out to the middle value of one and a2„ÄÇ

So the expected deviationÔºå the expected distance of the fair die from its mean is one and a half„ÄÇ

 On the other handÔºå from the loaded dieÔºå As we said„ÄÇ

 it's always exactly 2 a half from its expected value„ÄÇ

 which means its expected value is also 2 and a half„ÄÇ

So we can start to see the difference between these two distributions in these two kinds of die by even though they have the same expectation„ÄÇ

 one of them is more likely and has a greater expected distance from its mean and the moral of the short piece is that the mean alone is not a good predictor of a random variables behavior„ÄÇ

 as you might supposeÔºå one parameter„ÄÇ one number is not going to capture the shape of a Pdf„ÄÇ

 which gives you the more full information about the distribution of values of a random variable„ÄÇ

 And we need some more information than just the expectation„ÄÇ and especially a valuable„ÄÇ

 extra piece of informationÔºå that's still well less than the overall shape of the Pdf of the random variable„ÄÇ

 is knowing its probable deviation from its mean„ÄÇ

![](img/285bb4909446867ddd658c0dc4bc1372_1.png)

![](img/285bb4909446867ddd658c0dc4bc1372_2.png)

The simplest bound that a random variable differs by much from its expectation is due to a guy named Mark of a Russian„ÄÇ

Probability theoristÔºå and this is Markov's bound that we're going to talk about„ÄÇ

 Let's illustrate it with a memorable example of IQ in the MIT contextÔºå we it may be a radical idea„ÄÇ

 but IQ was this thing that was invented for intelligence quotient in the late 19th century„ÄÇ

I believe might have been early 20thÔºå It was meant as an effort to break the mold at Harvard of hiring the children of wealthy alumni and the idea was to have merit- based admissions and it was going to be some objective measure that did not depend on social class of the ability that people had in Harvard was going to admit students based on merit and their intelligence quotient„ÄÇ

 so the original design of the intelligence quotient by a bunch of psychologists was that the average was supposed to be 100„ÄÇ

Over the whole populationÔºå which of course is around here„ÄÇ

 there just aren't very many people with an IQ of just 100„ÄÇÂóØ„ÄÇAnywayÔºå let's ask this extreme question„ÄÇ

 YesÔºå there's around the elite universities„ÄÇ There are a lot of people with IQ is much higher than 100„ÄÇ

 but what fraction of the population could possibly have an IQ as high as 300„ÄÇ

 Now I'm not sure that an IQ of as high as 300 has ever been recorded„ÄÇ

 but we're talking logically hereÔºå is it possible for a lot of people to have an IQ of greater or equal to 100„ÄÇ

 And the answer is noÔºå you can't possibly have more than a third of the population have have an IQ of 300„ÄÇ

 because if more than a third had an IQ of 300Ôºå then that third alone would contribute one third of 300 to the average„ÄÇ

 which would be greater than 100Ôºå so you can't have more than a third of the population have an IQ of triple„ÄÇ

 the expected value of the IQ so„ÄÇBut that's the basic bound„ÄÇ

So we can restate it this way that the probability that a randomly chosen person has an IQ greater or equal to 100„ÄÇ

 we can say is absolutely less than or equal to the expected value of IQÔºå namely 100 divided by 300„ÄÇ

And just parameterizing itÔºå if we askÔºå what's the probability that the IQ is greater or equal to some amount x„ÄÇ

 it's less than or equal to 100 over x by exactly that reasoning„ÄÇ

And this is basically Markov's bound„ÄÇ except there's one implicit fact that we're using in deriving the previous identity or inequality that IQ is bounded by 100„ÄÇ

 Our logic was that you can't have more than population X with an IQ of more than 100 x„ÄÇ

 because that would contribute x times 100 over x or more than 100 to the average and the average is only 100„ÄÇ

 That's only a problem if there areÔºå if there are no negative terms„ÄÇ

 negative IQ is to offset the excess contribution of the fraction of the population that has this high IQ„ÄÇ

 But we're implicitly using the fact that IQ is never negative„ÄÇ IQ runs from 0„ÄÇTo unlimited amount„ÄÇ

 But it's never negative„ÄÇ And that means that that contribution from the one thirdhird of the population that has an IQ of over 300 can't be offset by negative values„ÄÇ

 It's thereÔºå and it messes up the average„ÄÇ It forces the average up„ÄÇ

 So we were using the fact that IQ is always non negative„ÄÇ And by this very same reasoning„ÄÇ

 I'm not going to belabor you with a more formal proof„ÄÇ there's a trivial one in the textÔºå It's easy„ÄÇ

 the theorem's Markovs bound says that if R is non negative„ÄÇ

 Then the probability that R is greater or equal to x is less than or equal to the expectation of R divided by X„ÄÇ

And this holds for any x greater than 0„ÄÇ Of course„ÄÇ

 it's silly to state if this bound is is greater equal to1„ÄÇ It's not an interesting bound„ÄÇ

 since probability is never greater or equal to 1„ÄÇ So we might as well just restrict ourelve to x's that are greater than the expectation of R„ÄÇ

 because those are the only x's that are going to give us a non trivialal bound„ÄÇ

 That's less than one„ÄÇAgainÔºå if R is non negativeÔºå then the probability that r exceeds an amount x is less than or equal to the expectation of R over x„ÄÇ

And that's the Markov bound„ÄÇ If you restate it in terms of deviation from the mean and you could formulate it this way„ÄÇ

 The probability that R is greater than equal to a constant times its mean U is an abbreviation for the expectation of R is less than or equal to one over C„ÄÇ

 So now we can understand that as a bound on the deviation from the mean above the mean in this case„ÄÇ

 that R as as the factor of the expectation increasesÔºå the probability decreases proportionally„ÄÇ

So of the probability that R is greater or equal to three times the expected amount is less than or equal to a third„ÄÇ

 which was what we saw with thee„ÄÇIQ example„ÄÇSo look this Markov bound in general is very weak„ÄÇ

 As I saidÔºå I don't think there's ever been an IQ recorded that was as high as 300 and in almost all the examples that you come across there' will be other information that allows you to deduce tighter bounds on the probability that a random variable is significantly bigger than its expectation„ÄÇ

 but if you don't have any information about the random variable other than that it's non negativegative„ÄÇ

 that as a matter of factÔºå Markov bound is tightÔºå you can't possibly reach a stronger contribution because there are non-neg random variables where the probability that they are greater than or equal to a given amount x is in fact„ÄÇ

 equal to their expectation divided by x„ÄÇ So the Markov bound is weak in application but it's the strongest condition you can make on the very limited hypotheses that it makes about properties of the random variable„ÄÇ

 and it's also pretty obviousÔºå I hope„ÄÇFrom this examples that we've talked about„ÄÇ

 But the amazing thing is how useful it is„ÄÇ we will get mileage out of it by using it in clever ways„ÄÇ

 So let's talk about the first clever way„ÄÇ and suppose that we're thinking about IQ is greater or equal to 100„ÄÇ

 but I bring into into the storyÔºå another fact that we haven't mentioned beforeÔºå which is„ÄÇ

 let's suppose that as a matter of factÔºå IQs of less than 50Ôºå just don't occur„ÄÇ

 I think they might actuallyÔºå but but there's a certain point where you just are not functioning at all„ÄÇ

 and it's not clear that it makes sense to ever talk about somebody who's in a coma as having an IQ„ÄÇ

 maybe they have an IQ of 0„ÄÇ But let's assume that pragmatically IQ is never less than or equal to 50 okay„ÄÇ

üòäÔºåNowÔºå ifÔºå if I tell you that I know that IQ is greater or equal to 50„ÄÇ

 then I can actually get a better bound out of MarkovÔºå because now„ÄÇ

 knowing that IQ is greater than equal to 50Ôºå Iq-50 is becomes a non negative random variable„ÄÇ

 which I couldn't be sure it was before because IQ might have gone below 50„ÄÇ

 Now that I know that it's always above 50Ôºå Iq-50 is non negative and Markov's bound will apply to Iq-50„ÄÇ

 And applying it to Iq-50 will give you a better bound because now looking at the probability that the Iq is greater or equal to 100„ÄÇ

 Of courseÔºå that's the same as saying that Iq-50 is greater or equal to 300-50„ÄÇüòäÔºåW whichch„ÄÇ50„ÄÇ

 the average expected value of I Q -50 is 100-50„ÄÇ So we're asking whether this non negative random variable is greater than or equal to 250„ÄÇ

 the answer isÔºå that's less than or equal to„ÄÇIts expectation over 2Ôºå50Ôºå which is„ÄÇ1 fifthth„ÄÇ

50 over 250„ÄÇ And that's a tighter bound than the one third we had previously„ÄÇ

 This is a general phenomenon that you get in that helps you get slightly stronger bounds out of Markov's bound„ÄÇ

 namelyÔºå if you have a non negative variableÔºå you get a better bound on it by shifting it so that its mean is 0„ÄÇ

 As a matter of factÔºå even if it goes negativeÔºå if you shift it up„ÄÇ

 you can if you can force it to above 0Ôºå as a minimumÔºå then you can apply Markov's bound to it„ÄÇ

Our topic is deviation from the meanÔºå meaning the probability that a random variable returns a value that differs significantly from its mean„ÄÇ

Now the Markov bound gave you a coarse bound on the probability that R was overly large using very little information about R„ÄÇ

 not surprisingly if you know a little bit more about the distribution of R than simply that it's non- negative„ÄÇ

 you can state tighter bounds„ÄÇAnd this was noticed by a mathematician named Chebychev„ÄÇ

 and he has a bound called the chbiachev B„ÄÇ Now it's interesting that the Markov bound„ÄÇ

 even though it's very weak and seems not very useful„ÄÇ the chbychev bound„ÄÇ

 which generally gives you a significantly stronger and valuableuably stronger bound on the probability that a random variable differs much from its mean is actually a trivial corollary of Markov's theorem„ÄÇ

 So it's just a very simple ingenious way to use Markov's bound to derive chbychev's bound„ÄÇ

And let's look at how„ÄÇSo we're interested in the probability that a random variable R differs from its mean by an amount x„ÄÇ

 the distance between R and its meanÔºå the absolute value of r minus mu is greater than equal to x„ÄÇ

 we're trying to get a grip on that probability„ÄÇAs a function of x„ÄÇ

 now the point is that the event that the distance between R and its random and its mean is greater or equal to x„ÄÇ

Another way to say that is to square both sides of this inequality„ÄÇ

 It says that the event that R minus mu squared is greater or equal to x squared happens„ÄÇ

 These two events are just different ways of saying the same thing„ÄÇ So therefore„ÄÇ

 their probabilities are equal trivially„ÄÇNow what's nice about this is of course„ÄÇ

 that r minus mu squared is a non negative random variable to which Markovs theorem applies„ÄÇ

The square of a real number is always going to be non negative„ÄÇ

So let's just apply Markov's theorem to this new random variable r minus mu squared„ÄÇ

And what does Markov's bound tell us about this probability that the square variable is greater than equal to n amount x squared„ÄÇ

 WellÔºå just plug in MarkovÔºå and it tells you that this probability that the square variable is bigger than by x squared„ÄÇ

Then it's as big as x squared is simply the expectation of that squared variable divided by x squared„ÄÇ

 This is just applyinging„ÄÇMarkovs bound to this variable R minus mu squared„ÄÇNow this numerator„ÄÇ

Is a weird thing to stare at expectation of R minus mu squared and may not seem very memorable but you should remember it because it's so important it has a name all its own it's called the variance of R and this is an extra bit of information about the shape of the distribution of R that turns out to allow you to state much more powerful theorems in general about the probability that R deviates from its mean by a given amount„ÄÇ

So we could just restate the chebychev boundÔºå just replacing that expectation formula in terms of its name variance of R„ÄÇ

 this is what the chbychev bound says„ÄÇ the probability that the different distance between R and its mean is greater equal to x is the variance of R divided by x squared where variance of R is the expectation of the square of R minus mu Now the very important aspect technical aspect of the chebychev bound is that we're getting an inverse square reduction in the probability„ÄÇ

 remember with Markov the denominator was behaving linearly and here it behaves quadraically„ÄÇ

 So these bounds gets smaller much more rapidly as we ask about the probability of differing by a larger amount„ÄÇ

The variance of R maybe in a way that will help you remember it is to remember another name that it has it's called the mean square error if you think of r minus mu as the error that r is making in how much it differs from what it ought to be then and we square it and then we take the average so we're taking the mean of the squared errors„ÄÇ

And here we're back to restating the Markov bound in terms of the variance„ÄÇ

The variance has one difficulty with itÔºå and that leads us to want to look at another object„ÄÇ

 which is just the square root of the variance called the standard deviation„ÄÇ So you wonder why„ÄÇ

 I meanÔºå if you understand varianceÔºå what's the point of taking the square root and working with that And the answer is simply that if you think of R as a random variable whose values have some dimension like it's seconds or dollars„ÄÇ

 then the variance of R is the expectation of a square variable of r minus mu squared„ÄÇ

 which means its units are second squared or dollar squared or whatever„ÄÇ

 and the variance of r itself is a squared valueÔºå which is not reflecting the magnitude of of the distance that you the kind of errors that you expect R to make„ÄÇ

 the distance that you expect R to be from its mean So we can get the units of this quantity back into matching the units of R„ÄÇ

RAnd also get a number that's closer to the kind of variance that you'd expect to observe by just taking the square root„ÄÇ

 And it's called the standard deviation of R„ÄÇ If it helps you„ÄÇ

 the standard deviation is also called the root means square error„ÄÇ

 And you might have heard that phrase„ÄÇ It comes up all the time in discussions of experimental error„ÄÇ

 So againÔºå we're taking the error means the distance between the random variable and its mean„ÄÇ

 We're squaring itÔºå we're taking it the expectation of that square error„ÄÇ

 And then we're taking the square root of it„ÄÇ It's the standard deviation„ÄÇ

So going back to understand what the standard deviation means intuitively in terms of a familiar shaped distribution function for a random variable R„ÄÇ

 suppose that R is a random variable that has this fairly standard kind of bell curved shape or Gaussian shape that it's got one hump„ÄÇ

 it's unimmodal and it kind of trails off with some moderate rate as you get further and further away from the mean well the mean of a distribution that's shaped like this it's symmetric around that that high point is that's going to be the mean by symmetry„ÄÇ

 it's equally likely to be well the value is average out to this middle value„ÄÇ

Stand deviationation for a curve like this is going to be an interval that you can interpret as an interval around the mean and the probability that you're within that interval is fairly high for standard distributions„ÄÇ

 NoÔºå we'll see that„ÄÇThe chmney chip band is not going to tell us much about for an arbitrary unknown distribution„ÄÇ

 but in generalÔºå for the typical distributionsÔºå you expect to find that the standard deviation tells you that that's where you're most likely to be when you take a random value with a variable„ÄÇ



![](img/285bb4909446867ddd658c0dc4bc1372_4.png)

![](img/285bb4909446867ddd658c0dc4bc1372_5.png)

So let's return to the chbychev bound as we've stated it„ÄÇ and I'm just replacing here„ÄÇ

 I'm restating the chebychev bound just replacing the variance of R in the numerator by the square of its square root by sigma squared R„ÄÇ

 It's a useful way to restate it because by restating it this way and motivates another reformulation of the chbychev bound as we reformulated the markov bound previously in terms of a multiple of something I'm going to replace x by a constant times the standard deviation So I'm going to see the probability that the error is greater than or equal to a constant times the standard deviation„ÄÇ

 and this term is going to simplify once x is is a constant times the standard deviation„ÄÇ

 the standard deviations are going to cancel out and I'm just going to wind up with one over x squared„ÄÇ

 So let's just do that„ÄÇAnd„ÄÇThere's the formulaÔºå the probability„ÄÇ

That the distance of R from its mean is greater than or equal to a multiple C of its standard deviation is less than or equal to one over C squared„ÄÇ

 So it's getting„ÄÇMore much more rapidly smaller as sea grows„ÄÇ„ÅÇ„ÄÇ

Let's look at what that means for just some numbers to make the thing a little bit more real„ÄÇ

 what this assertion is telling us is that R is probably not going to return a value that's a significant multiple of its standard deviation„ÄÇ

For exampleÔºå what does this formula tell us about the probability that R is going to be greater than or equal to one standard deviation away from its mean„ÄÇ

 WellÔºå it actually tells us nothing„ÄÇ that's the case in which it's no good because C is one is just telling us that the probability is at most one which we always know because probabilitybabilities are at most one„ÄÇ

 But if I ask what's the probability that the mean the error of R is greater than equal to twice the standard deviation„ÄÇ

 then this theorem is telling me something nontrivial„ÄÇ

 It's telling me that the probability that it's twice the deviation is one over two squared or one quarter„ÄÇ

 an arbitrary random variable with standard deviation sigma is going to exceed twice the error is going to exceed twice the standard deviation of most the quarter of the time„ÄÇ

 three times at most the9 of the timeÔºå four times at most the 16th of a time„ÄÇ

 So the qualitative message to take away is that for any random variable whatsoever„ÄÇ

 as long as it has a standard deviation sigma„ÄÇThen you can say some definite things about the probability that the random variable is going to take a value that differs by a large multiple of the standard deviation from its mean„ÄÇ

That probability is going to be smaller and gets smaller and rapidly smaller as the multiple of the standard deviation continues„ÄÇ

If we're going to make use of Chey Chev's Bo and other results that depend on the variance„ÄÇ

 we'll need some methods for calculating variants in various circumstances„ÄÇ

 so let's develop that here„ÄÇA basic place to begin is to ask about the indicator about indicator variables in their variance„ÄÇ

 rememberÔºå I as the indicator variable means that it's 0Ôºå1 valued„ÄÇ

 It's also called a Bnoulli variable„ÄÇ And if the probability that it equals1 is P„ÄÇ

 that's also its expectation„ÄÇ So we have an indicator variable with expectation of the indicator is P„ÄÇ

 And we're asking what's its varianceÔºå whichÔºå by definition is the expectation of I minus p squared„ÄÇ

 WellÔºå this is one of these sort of almost mechanical proofs that follows simply by algebra and linearity of expectation„ÄÇ

 but let's walk through its step by stepÔºå just„ÄÇTo reassure you that that's all that's involved„ÄÇ

 I would recommend againstÔºå really trying to memorize this because it's„ÄÇ

 I can never remember it anyway„ÄÇ I just reprove it every time I need it„ÄÇ

 And so let's see how the proof would go„ÄÇ So step 1 would be to expand this I minus p squared algebraically„ÄÇ

 So we're talking about the expectation of I squared 2 p I plus p squared„ÄÇ

 Now we can just apply linearity of expectation„ÄÇ And I get the expectation of I squared 2 p times the expectation of I plus p squared„ÄÇ

 Of courseÔºå the expectation of a constant is the constant„ÄÇ So when I take expectation of p squared„ÄÇ

 I get p squared„ÄÇ But nowÔºå look at this„ÄÇüòäÔºåI squared is 0Ôºå1 valued„ÄÇ SoÔºå in fact„ÄÇ

 I squared is equal to I„ÄÇ And the expectation of I has now appeared here„ÄÇ That's P„ÄÇ

 So this term simplifies to expectation of IÔºå And this term becomes 2 P times p plus P squared„ÄÇ Of„ÄÇ

 that expectation of I is a P„ÄÇ I got P minus 2 p squared plus P squared„ÄÇ the p squares cancel„ÄÇ

 And I get P minus p squared„ÄÇ If you factor out PÔºå that's p times 1 minus P or P Q„ÄÇ

 which is the standard way that you write the variance of an indicator variable„ÄÇ

 It's p times 1 minus P„ÄÇüòäÔºåOkayÔºå that was easy and againÔºå completely mechanical„ÄÇ

There is a couple of other rules for calculating variance of new variables from old ones that are basic„ÄÇ

 like addivity of expectationÔºå but it doesn't quite work so simply for variance„ÄÇ

 So the first rule is that if you ask about the variance of a constant times r plus B„ÄÇ

 that turns out to be the same as a squared times the variance of B of the R„ÄÇ

 the B doesn't additive B doesn't matter And because the variance is really the expectation of something squared when you get rid of that constant A„ÄÇ

 you're factoring out in a squared„ÄÇ and this is the rule that you get here„ÄÇ

Another basic rule that's often convenient instead of working with variances in the form of the expectation of r minus mu squared is to say that it's the expectation of r squared minus the square of the expectation of R„ÄÇ

NowÔºå this expressionÔºå the square of the expectation of R comes up so often that there's a shorthand for it where instead of writing parends„ÄÇ

 you write e squared of r just means the same as expectation of r squared„ÄÇ

 and so much for the second ruleÔºå which we'll use all the time because it's a convenient rule to have„ÄÇ

 I'm going to prove the second oneÔºå just againÔºå to show you nothing to worry about„ÄÇ

 you don't even have to remember how the proof goes because you can reconstruct it every time„ÄÇ

 So this it'sÔºå againÔºå simple proofs just by linearity of expectation and doing the algebra„ÄÇ

 So the variance of R is by definitionÔºå the expectation of r minus mu squared„ÄÇ

 Let's expand r minus mu squared„ÄÇ It's the expectation of r squared minus2 mu R plus mu squared„ÄÇ

 Now we apply linearity to that„ÄÇ I get the expectation of r squared minus2 mu expectation of R plus the expectation of mu squared„ÄÇ

 if I'm really being completely mechanical about linearity„ÄÇExpectation„ÄÇ

 Now expectation of a constant mu squared is simply mu squared„ÄÇ

 And here I've got the expectation of R„ÄÇ That's mu again„ÄÇ

 So I wind up with the expectation of r squared -2 mu mu plus R squared„ÄÇ The this is2 mu squared„ÄÇ

 minus-2 mu squared plus mu squared„ÄÇ it winds up with minus mu squared„ÄÇ AndÔºå of course„ÄÇ

 mu squared is the expectation squared of R„ÄÇ I prove the formula again„ÄÇ

 as claimed there's nothing interesting here„ÄÇ just algebra and linearity of expectation„ÄÇüòä„ÄÇ

And the first result about factoring out in A and squaring it follows from a similar proof„ÄÇ

 which I'm not going to include here„ÄÇSo let's look at the space station mirror again„ÄÇ

 which we used as an example of calculating mean time to failure„ÄÇ

 So the hypothesis that we're making is that with probability P„ÄÇ

 the mere space station will run into some huge space garbage that will collaborate and the probability of that happening at any given hour is probability P„ÄÇ

So we know that that means that the expected number of hours for the mere to fail is one overpay„ÄÇ

 That's the mean time to failure„ÄÇ And what we're asking isÔºå what's the variance of F„ÄÇ

 If F is the number of hours to failureÔºå what's variance of F„ÄÇWell„ÄÇ

 one way we can do is just plug in the definition of expectation and this will work„ÄÇ

 The probability that it takes k hours to fail is we know the geometric distribution„ÄÇ

 The probability of not failing for k minus1 hours and failing after that Q to the K minus-1 times p„ÄÇ

 So the variance of f using our previous formula about the expectation of f squared minus the expectation squared of f„ÄÇ

 This becomes a minus1 over p squared„ÄÇ and we can forget about that„ÄÇ

 We want to focus on calculating the expectation of f squared„ÄÇ So F is 1Ôºå2Ôºå3 and so on„ÄÇ

 that means f squared is 1Ôºå4Ôºå9 K squared„ÄÇ the point being that the only values that f squared can take r squared„ÄÇ

 So we don't have to worry about counting them in in the sum that defines the expectation„ÄÇ

 So let's go look at that„ÄÇ So the expectation of f squared is the sum over the possible values that f„ÄÇ

squared can takeÔºå namely the sum from k equals 1 to infinity of k squared times the probability that f squared is equal to k squared„ÄÇ

 WellÔºå of course the probability that f squared is equal to k squared is the same as the probability that f equals k so I and we know what the probability that f equals k as it's a geometric distribution„ÄÇ

 So the probability that f equals k isq to the k minus1 times p if I factor out a p overq this simplifies to the sum from k equals0 to infinity of k squared Q to the K and this is a kind of sum that we've seen before and that it has a closed form and we could perfectly well calculate then the expectation of f squared by appealing to our generating function information to get a closed form for this and then remember to subtract1 minus p squared because the variance is this term minus the squared of the expectation of f„ÄÇ

 but let's go another way and use the same technique of total„ÄÇExpectation that we used before„ÄÇ

 That is the expectation of F squared of the failure times squared is equal by the law of total probability to the expectation of F squared„ÄÇ

 given that F is one„ÄÇThat is we fail on the first step times the probability that we fail on the first step„ÄÇ

 PlusÔºå the expectation of F squaredÔºå given that we don't fail on the first step that F is greater than one times the probability that F is greater than one„ÄÇ

 NowÔºå what's going to make this manageable is that this expression„ÄÇ

 the expectation of F squared when F is greater than one will turn out to be something that we can easily convert into a nonconditional probability and find a value for„ÄÇ

 So the limit that we're using here is the following„ÄÇ when I'm thinking about mean time to failure„ÄÇ

 If I think of any function whatsoeverÔºå G of the mean time to failure„ÄÇ

 And I'm interested in the expectation of G of F„ÄÇAnd I'm interested in the expectation of G of F„ÄÇ

 given that F is greater than N„ÄÇ that isÔºå it's already taken n steps to get where I am„ÄÇ

 Then the thing about the meantime to failure is that at any moment that you haven't failed„ÄÇ

 You're starting off in essentially the same situation you were at the beginning in waiting for the next failure to occur and the probability of failing in one more step is the same probability is the same P and the probability of your failing in two more steps is Q P and three more steps is Q Q P„ÄÇ

 The only difference is that the value of F has been shifted by N„ÄÇ

 It was in the ordinary case we start off with F equals 0„ÄÇ

 And look at the probability that we fail in one more stepÔºå two more steps„ÄÇ

 Now we're starting off with F having the value F plus N and asking about the probability it fails in the next step or the next step„ÄÇ

 when the next step„ÄÇ So the punch line is that the expectation of G of F„ÄÇ

GivenThat F is greater than n is simply the expectation of G of F plus n„ÄÇ

And I'm going to let you meditate that and not say any more about it„ÄÇ

 But the punch line is the corollaryÔºå that the expectation of F squared„ÄÇ

 given that F is greater than one is simply the expectation of F plus one squared„ÄÇ

 And that lets us go back and simplify this expression„ÄÇ

 that we had from total expectation we now haveÔºå here's the expectation of F squared„ÄÇ

 given that F is greater than one„ÄÇ And let's look at these other terms„ÄÇ

 this is the expectation of F squaredÔºå given that F equals1„ÄÇ WellÔºå the expectation of F squared„ÄÇ

 given that F equals1 is one squaredÔºå because we know what F is and that's the end of the story„ÄÇ

 times the probability that F equals1Ôºå that's PÔºå the probability of failure on a given step„ÄÇ

 This is the probability that F is greater than one„ÄÇ

 which is Q that we didn't fail on the first step„ÄÇ And we just figured out that this term is the expectation of the square of F of„ÄÇ

üòä„ÄÇ

![](img/285bb4909446867ddd658c0dc4bc1372_7.png)

![](img/285bb4909446867ddd658c0dc4bc1372_8.png)

F plus 1„ÄÇ So there's the one in the P„ÄÇ And that becomes a Q„ÄÇ

 And this is the expectation of F plus one squared„ÄÇ NowÔºå againÔºå I apply linearity„ÄÇ

 I'm going to expand F plus1 squared into F squared„ÄÇPlus to f plus1„ÄÇ

 and then apply linearity of expectation„ÄÇ and I'm going to wind up with the expectation of f squared plus twice the expectation of F„ÄÇ

 which remember is twice over2 over p plus1 times the Q„ÄÇ

 And now what I've got is a simple arithmetic equation between the expectation of f squared„ÄÇ

 and some other arithmetic and the expectation of f squared„ÄÇ

 It's easy to solve for the expectation of f squared„ÄÇ

 and I'll spare you that elementary simplification„ÄÇ

 but the punch line is when we also remember to subtract1 over p squared because that was the expectation of the square of F of the expectation of F„ÄÇ

 we come up with this punch line formula„ÄÇ The variance of mean time to failure is1 over the probability of failure on a given step times1 minus1 over the times the probability„ÄÇ

1 over the probability of the failure in the first step minus-1„ÄÇAll right„ÄÇ

 let's just for practice and fun„ÄÇ Let's look at the space station mirror again„ÄÇ

 Suppose that I tell you that there's a one in 10000 chance that in any given hour„ÄÇ

 the mirror is going to crash into some debris that's out there in orbit„ÄÇ Now„ÄÇ

 So the expectation of F is 10 to the  fourthÔºå about 10000 hours„ÄÇ

And the sigma is going to be the variance of of FÔºå which is about one over„ÄÇ1Ôºå10000„ÄÇ

 That is 10000 times 10000-1Ôºå which is pretty close to 10000 squared for the variance„ÄÇ

 And then when I take the square rootÔºå I get back to 10000„ÄÇ

 So Sigma is just a tad less than than 10000 It's 10 to the fourth„ÄÇ So with those numbers„ÄÇ

 I can apply to the chbychev theorem„ÄÇ and conclude that the probability that the mere lasts more than 4 times 10 to the fourth hours is less than  one chance in  four„ÄÇ

 And we translate that into years„ÄÇ If it was really the case that there was a  one in 10000 chance of the mere being destroyed in any given hour„ÄÇ

 then the probability that it lasts more than 4„ÄÇ6 years before destructing is less than one quarter„ÄÇ

So another rule for calculating variance and maybe the most important general one is that variance is additive„ÄÇ

 That is the variance of a sum is the sum of the variances„ÄÇ

 But unlike expectation where there's no other side condition„ÄÇ

 and it does not in any way depend on independence„ÄÇ

 it turns out that variance is additive only if the variance being addedÔºå are pairwise independent„ÄÇ

 Now you might wonder where the pairwise came from„ÄÇ

 and it's because variance is the square of an expectation„ÄÇ

 So when you wind up multiplying out and doing the algebra„ÄÇ

 you're just getting quadratic terms for variances for expectations of R times Rj„ÄÇ

 And so you need to factor those into expectation of R times R times expectation of Rj„ÄÇ

 which only need pairwise independence for„ÄÇ So that's a fast talking through the algebra that I'm going to leave to you„ÄÇ

 it's in the text and it's againÔºå one of these easy„ÄÇ



![](img/285bb4909446867ddd658c0dc4bc1372_10.png)