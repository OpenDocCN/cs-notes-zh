# P22：讲座22 数据复制和分布式事务 I - ___main___ - BV1cL411t7Fz

因为我们接下来要讨论的话题是当我们尝试在并行数据库上运行事务时会发生什么。这将是另一个有趣的话题。

![](img/36def6d6b9866c9c33a169a2bce65f2c_1.png)

但在此之前，稍微复习一下。我们开始谈论并行处理的原因是因为现在有大量不同的机器。而且，你知道的，我们如何才能有效利用它们，以免浪费。

通过这样做，我们实际上可以提高性能。所以基本上，这就是我们上节课试图讨论的内容。顺便说一下，如果你们不介意打开视频的话，那就太好了。我现在看到的是Warren和Jack，感谢你们的忙碌与参与。

我觉得我就像是在对着一面非常漂亮的墙说话，真是个专业的话题。太好了。是的，正如我上次说的，我们差不多已经掌握了。然后我们已经做了一些不同的方法。然后我们已经做了一些不同的方法。

然后我们已经做了一些不同的方法。然后我们已经做了一些不同的方法。然后我们已经做了一些不同的方法。现在我们已经掌握了单节点计算处理。

所以现在我们正在过渡到课堂的下一个模块，这个模块讨论的是当我们有多台机器时会发生什么。所以上次我也谈到过这张图，它展示了我们在谈论并行计算处理时遇到的不同类型的架构。这里的共享内存方面，我们有一堆CPU或多个核心，它们共享同一块RAM。

对，这里指的是主内存，这就是共享内存。还有共享磁盘架构，每台机器都有自己的RAM，但它们基本上共享同一块磁盘。还有共享无架构（shared nothing architecture），在这种架构下，任何东西都不共享。

因此，我们现在需要通过网络进行通信，并且访问存储在另一台机器上的数据，或者访问已经加载到另一台机器主内存中的数据。这是人们在谈论并行数据处理时会涉及的三种架构。

然后在这节课中，我们将重点讨论共享无架构（shared nothing architecture）。

![](img/36def6d6b9866c9c33a169a2bce65f2c_3.png)

然后在今年的课程中，我也讲到过上节课讨论的如何在这个环境中进行数据分区。所以现在我们有了很多磁盘，我们仍然需要考虑整个关系，只是现在可能整个关系不能装入单台机器中。

那么，我们到底怎么在所有不同的机器之间分配数据呢？我们也讨论了三种可能的方案来实现这一点。对于范围分配，我们基本上只是设定一个任意的范围，每台机器应该持有这个范围内的记录。例如，在上次讨论的案例中，我们是学生数据。

所以我们可以想象，需要将学生记录分配到多台机器上。我们可以通过简单地分配一个范围，比如按姓氏的首字母，把它们分配到不同的磁盘上，这就是范围分配。第二种方案，称为哈希分配，基本上依赖于一个独立的哈希函数来为我们进行分配。我们将每条记录通过哈希函数处理，然后无论哈希函数决定将记录放到哪里。

然后，将该记录发送到特定的机器上。这是第二种方案。第三种方案称为轮询（round Robin），基本上意味着我们会在所有不同的磁盘之间轮流，发送新的记录到它们到达的地方。

我上次讲到的重点基本上是这两个方面：一是访问记录的局部性，二是如何平衡所有机器之间的负载。那么，有人能提醒我们，为什么例如轮询调度在平衡负载方面是最有效的吗？这里的“最有效”意味着它是最公平的，意味着每台机器都会大致获得相等数量的元组。

所以这就是为什么我并没有说它们是按照特定的键排序的。比如说，我们并没有为每个机器分配特定的范围，因此我们不必担心，像你知道的，万一，假设我看到一个非常受欢迎的名字，那么在这种情况下，那个机器就会负责。

我可能最终会正确处理大部分的元组，但在这种情况下，由于我们采用的是轮询调度（round Robin）。这就是我们这么做的原因。在这种情况下，负载将会均衡，因为我们其实并不关心每个集合中的单个元素的内容。

这样讲清楚了吗？另一方面，为什么轮询调度在访问局部性方面是最差的呢？意味着它是最难访问单个元素的。它本质上是一个“添加排序”的方法，正如它能使负载均衡一样，也正是它让我们很难预测到底是哪台机器持有特定的记录。

在这种情况下，我们假设没有全局索引可供查找特定记录。所以我们能做的唯一方法就是查询每台机器，询问是否拥有某个名字的记录。

相比于如果我们有哈希分区方案或范围分区方案。我们已经知道哪个机器上会有特定的记录，因为你知道只需要查看范围，或者运行哈希函数，它就会告诉你哪个机器上当前存有该记录。如果该记录确实存在。好，有关这一点有问题吗？

顺便说一下，这并不是一个详尽的列表，列出了你可以如何将数据分区到多个机器上的所有方式。这基本上只是给你一个示例，展示你可以做什么。所以也有其他方案，比如为什么我们不直接复制数据，如果你能承担得起的话。

对，所以你不必像这样进行分区，我是说如果我们有足够的内存，我们可以直接复制数据，让每台机器都拥有整个关系的副本。对，如果是这样，确实会让并行处理变得更容易，因为每台机器已经拥有了整个关系。但显然，这意味着我们在存储和内存上需要付出更多的代价。

所以这是其中一种方式。在分区之后，我们开始讨论运行我们现在熟悉的操作集的不同方法。比如哈希操作。所以我上次提到的是这个两阶段方案，我们希望能够并行哈希。

这个想法是首先使用特定的哈希函数将数据在不同机器之间进行“洗牌”。这意味着我们基本上会决定将特定的数据片段分配给哪台机器来保存。

然后我们就把它交给那个机器，我们使用哈希函数。接着我们就像你知道的那样构建一个内部哈希表。然后你知道，现在如果你愿意的话，可以使用我们最喜欢的算法来构建那个哈希表。所以它全部都适合放进内存，然后我们就像你知道的那样，完全使用哈希函数构建，之后就完成了。

然后像你知道的那样，如果某个桶没有占用比需要的更多空间，那么我们就执行我们之前提到的多阶段分区方案，就像我们在期中前讨论过的内存外算法一样。

所以大家记得为什么我们需要不同的哈希函数来做这件事吗？就像你知道的，我们现在有不同的哈希函数，而不是最初的哈希函数H of N。大家记得为什么需要这样做吗？为什么不直接用同一个哈希函数？

这部分也是我们讨论课程的原因之一。是的，正是因为如果我们只是使用相同的哈希函数（比如 H），那么所有最终出现在同一个蓝色磁盘上的数据将被哈希到同一个桶里。这就不好了，意味着它们并没有被分配到不同的桶中。所以在并行处理的意义上，这样做其实没有多大帮助。

所以我们实际上希望将两个目标，如果它们的值不同，分到不同的桶中，这样我们就可以轻松地进行后续的连接操作。所以出于这个原因，以及我们使用不同哈希函数或核心算法的同样原因，我们需要确保在第一步中使用的哈希函数和这一步使用的哈希函数是不同的。

当然，这仍然存在同样的问题，那就是如果两个目标完全相同会发生什么？记得我们在讲核心算法时使用的例子是性别。在这种情况下，我们基本上需要一个单独的机制来检查是否所有的值都是相同的。情况是一样的。好，现在我们了解了如何进行哈希，接下来的部分，我将讲解如何使用这些不同的操作。

如何使用这些基本操作来实现我们熟悉的关系操作符。特别是这里列出的一些操作。首先是连接。如何进行并行连接？这里的场景和我们讨论核心算法时是一样的。我们有两个关系 R 和 S，想要基于某个值或者属性进行连接。

所以这里是如何在多台机器之间执行哈希连接的一种方式。首先，我们将其中一个关系（假设是跨越我们拥有的不同机器的关系）进行洗牌。然后，在每台机器接收到自己的数据后，我们会构建一个哈希表。所以这里右侧发生的就是这样，我使用 H of N 来将数据洗牌到我们这三台机器上。

然后我们将使用我们构建哈希表的机制，基本上构建出一个对应于我们收到的工具的哈希表。我们等待哈希表的构建完成。接着，我们使用相同的机制将另一方关系中的两个数据项通过网络洗牌。

所以这里我们使用相同的哈希函数，H of N，将所有的操作洗牌到网络中。与连接不同，我们基本上会把数据放到同一台机器上。对吧？那么，当我们收到数据时，会发生什么呢？你知道，在我们已经构建了哈希表之后，我们只需要检查我们已经构建好的哈希表，看看是否有任何两个元素在哈希表中匹配。

如果有匹配项，我们就将结果写入磁盘。如果没有匹配项，我们就基本上不生成任何输出。好吧，就这样。所以在这个过程中，大部分步骤实际上都是并行的，除了第一个步骤，我们需要构建哈希表。例如，写入磁盘的部分，你看到最右边的部分，写入磁盘是完全并行的。

所以每个磁盘基本上都会做自己的事情，当它收到数据时，对吧？它只需要在已经加载到内存中的哈希表中进行查找。如果有匹配项，就将其存入磁盘，所有这些操作完全可以并行完成，因此我们在不同机器之间不需要做任何协调。

但请注意，我们确实需要等待哈希表首先在我们这里构建起来。所以，这是需要时间的步骤，可能会花时间对吧？然后如果有一些需要等待的情况，我们就需要等到最后的哈希表构建完成，才能进行流式处理。所以有一种变种方案，实际上不需要等待哈希表构建完成，我们稍后几张幻灯片中会讨论这个。

但我可以告诉你，我已经有时间可以说没有什么是免费的，没这回事什么免费午餐。因此，你知道，那个特定的算法实际上会使用更多的空间和内存来实现这个目标。但在此之前，你知道，关于这个方案有任何问题吗？在我们继续之前。这基本上是你能想象到的最简单的并行连接方式，就是把其中一个关系洗牌到不同的机器上，并进行分区。

我们让每台机器自己构建哈希表并存入内存，然后我们就像你知道的那样，把要连接的其他数据从其他关系发送到不同的机器上，然后让它去完成任务。

完全独立地进行。明白了吗？所以现在我们问一个常见的问题，对吧？那就是如果没有足够的内存来构建这个哈希表呢？你看右边那个，假设我们无法把整个哈希表放进内存里，那我们该怎么办？

你认为我们应该怎么做？递归？其实你已经学过这个算法了，叫做Greatest Hash Join，记得吗？或者你会在我们讨论期中考试时记得。当时有一种算法可以让我们像这样操作，比如说在这个哈希表上做Hash Join，而不需要强制要求整个哈希表都能够适应内存。

所以我们这里做的其实是相同的事情。首先在这种情况下，我们只是使用Grace Hash Join来尝试跨多个步骤填充哈希表。最后一步基本上就是把我们已经有的桶写入磁盘，正如你可能还记得的那样。

只是，在这种情况下，我们需要多次进行操作。基本上我们想做的事情是对两个数据集都进行相同的操作。正如我们所知道的，整个哈希表不能完全放入内存中。如果数据集A也放不下，那么我们基本上就需要通过多次传递来实现，首先将单个桶写入磁盘。

然后，在那之后，我们可以检索与数据集A对应的桶，以及数据集B中匹配的桶，然后进行内存中的Hash Join，就像我们执行Grace Hash Join算法时一样。所以希望这对大多数人来说只是复习，唯一不同的地方其实是这个初步的洗牌步骤，或者说是使用我们称之为Band的新哈希函数来进行分配。

但实际上，这只是一种方式，用来将两个数据集分配到我们所有的机器上。所以这里是Grace Hash Join算法并行版本的文本形式，首先我们通过在机器间洗牌两个数据集来进行分配。

然后，在此之后，我们基本上就可以在各个节点上执行本地的Grace Hash Join。也就是说，我们将单个桶写入磁盘，直到它们填满。然后，根据需要进行多次传递，就像你记得的Grace Hash Join一样。之后，我们就像之前一样，从相应的关系中读取桶并进行连接。

然后我们在本地执行连接。再次强调，这几乎是完美的加速，因为你知道，一旦两步都能够在相互独立的情况下执行和完成，除了第一步涉及将目标通过网络洗牌的步骤之外，其他都可以独立进行。

后续的步骤基本上就像你知道的你最喜欢的Grace Hash Join算法一样。你可以运行每台机器，让它们并行运行。还有一个变种是不需要等待的，它叫做Extra，我们实际上没有时间在这门课上讲解，所以如果你感兴趣，我邀请你查阅这个关键词或相关的论文。

但基本上，我已经介绍了两种不同的方式来执行并行连接。即所谓的天真连接，通过对两个不同关系的所有数据进行打乱。或者在内存不足以容纳所有数据的情况下，我们执行并行的Grace哈希连接。实际上，我们稍后会讨论对称哈希连接，只是在这种并行Grace哈希连接的特定变体中，我们没有时间在这节课上覆盖。

到目前为止一切顺利。好的。那么现在我们来谈谈排序，它是我们一直在处理的关系操作之一。现在我希望能够跨多个机器运行这个操作。我们该怎么做呢？其实很简单，我们基本上做类似的操作，将数据打乱并分配到所有机器上。然后我们就知道每台机器应该处理哪些范围的记录。

然后，你可以在之后运行你喜欢的排序算法。所以例如，在这种情况下，我已经根据某个属性随机分配了数据范围，结果发现它是数值型的。所以我随意地为三个机器分配了三个不同的范围。第一个，蓝色的机器会处理从负无穷到10之间的所有数据。

然后第二台机器，即绿色的机器，将处理从11到100之间的所有内容。其余的则会被分配到橙色的机器上。所以我们首先通过基于范围的分配进行这种分配或打乱步骤。然后，接下来，我们知道如何在运行局部排序时进行处理。

这取决于我们。一旦所有数据被分配到更大的机器上，我们可以运行任何你喜欢的排序算法。对吧？是的，这就是为什么我称之为其他的n-1次排序，因为它可能依赖于我们使用的排序算法，这可能最终会导致多次排序。是的，但就是这样。所以这给我们带来了正确的结果，因为每个数据范围都会得到处理。

每个范围的两组数据我们会在相应的机器上进行排序。但是有一个问题，那就是性能。所以这其实和我们之前谈论如何将工具分布到不同机器上的问题类似。那么我们如何避免数据倾斜呢？因为例如，可能所有的工具大部分值都在11到100之间。

所以在这种情况下，绿色机器会获得大部分的工具，从而承担大部分的工作。如果我们不想让这种情况发生，我们该如何调整这个方案呢？这样可以避免数据倾斜。有人有想法吗？而且你不允许在随后的讲座中选择切片。好吧，这个直方图怎么样？我们的目标是什么？你可以使用轮询方法，但问题在于第二步和后续步骤就不再是独立的了，因为如果我们进行轮询。

然后蓝色的机器就可以像它能够从所有可能的值中获取一样去执行。所以对于其他两台机器也是如此。如果是这种情况，假设我们在每台不同的机器上都有这些部分排序的内容。

我们需要一种方法来合并它们。那么如果我们使用归并排序呢？这就是我们要探讨的方向。首先，我们需要确保我们理解问题，如果我们只是进行轮询，然后尝试通过那种方式来排序。如果我们这么做的话。

然后我们需要再进行一轮合并，实际上可能需要多轮合并，才能得到完整的排序结果。所以一项一项地进行，我们可以先试着用直方图，就像Charlie提到的那样。

那这意味着什么呢？如果我们 somehow 有一个所有工具的直方图以及它们对应的频率（像我在右边展示的那样），我们现在就可以把x轴的范围或域划分开，在这种情况下，并不是均匀地分配到各台机器上。

我们可以根据我们将要获得的工具总数来划分这些范围，然后根据这些范围将其分配到不同的机器上，以确保每台机器最终拥有相同数量的内容进行排序。那么如何选择呢？其实应该比较容易，因为如果我们知道每个值的频率，那么我们只需要查看分布情况。

然后我们确保每台机器一开始都有相同数量的两个值。这在元组数量较少时容易做到，比如说我们只有10个元组，完成这个直方图就很简单。

但是当数据量不小的时候就不再是这种情况了。其实这也是通常的情况，因为我们使用并行机器的原因是我们有大量的元组需要处理。如果我们只有10个元组，而我只是运行在本地的笔记本电脑上，那么就不需要并行处理了。即使我们进行并行化，也不会加速很多。

只有在我们有像千万级或者甚至亿级的元组时，这种情况才会发生。如果是这种情况，那么构建这样的直方图就不会容易，除非我们已经以某种方式在数据最初被导入系统时，便在跟踪该直方图。所以，你可能有一个想法：与其构建整个直方图，不如我们直接进行采样。

好吧，假设我们就像这样进行采样：从输入关系中随机挑选一些元组，或者你想挑选多少个元组，然后基于这些样本构建直方图。然后，你知道，我们基本上假设样本直方图会反映出整体直方图的样子。

所以，如果你相信这个假设：我们可以利用从样本构建的直方图，然后用它来将元组分配到不同的机器上，然后继续进行我的工作。现在，当然我们基本上就是在把问题推迟到以后了，因为在这种情况下，你可能会问，好吧，我们怎么才能做到采样是公平的，事情是均匀的呢？因为如果我们在采样的过程中有偏差。

那么我们可能最终会创建一个不能反映所有元组的直方图，因此你知道，我们创建的直方图将会是偏斜的。所以这实际上是一个比我们在这门课上能讲解的要复杂得多的问题。

事实上，有很多不同的统计技术可以学习，比如在统计学课程中，你可以学习如何进行不同类型的采样，以确保采样是无偏的。如果你对这方面有兴趣，我也很乐意在课外和你们讨论。

但是这里的一般思路是，如果我们有一个无偏的样本，那么我们基本上可以基于这个无偏的样本构建直方图，然后用它来将元组分配到不同的机器上，因此可以按这种方式进行排序。明白了吗？好的，是的。

所以，假设我们做了基于创建所有元组的直方图或者基于采样技术的采样。现在，让我们谈谈在我们接收到每台机器上的元组后，我们可以做什么。

我们可以做到的一种方式是基本上使用我们最喜欢的排序合并连接，假设我们想要做一个连接操作。那么我们现在已经知道如何做排序了。现在让我们讨论一下如何利用排序来执行连接，就像我们在排序合并的案例中学到的那样。

这确实是可能的。所以实际上这变得和我们之前讨论的类似。只是现在我们又多了一个额外的过程，就是进行分区，然后正确地对其进行排序。只是现在我们需要对两个关系进行排序，就像我们在谈论排序合并连接（sort merge join）时所说的那样。对吧？所以记住，排序合并连接的核心思想是将两个元组集排序，使得我们可以从这两个关系中一次处理一个元组进行连接。

然后我们就用这种方式执行连接。对吧？希望你还记得什么是排序合并连接。或者我相信你最终会理解的。好吧，所以在这种情况下，我们希望并行执行操作。如果我们能够将两个数据集分发到不同的机器上就可以实现这一点。

然后其余的过程就和我们之前讨论的完全一样。所以再次强调，排序合并排序可能实际上需要多次迭代，因为我们可能需要多轮排序来确保一切排序完成，因为所有元组可能无法一次性全部放入内存。

所以我们可能会经历多轮处理。我们在这里要做的唯一事情就是首先独立地处理每个元组，然后再做合并部分。而且，合并部分可以完全独立地在不同的机器上进行。因此，这样我们能够实现完美的并行处理，唯一的前提是我们首先需要将元组分发到不同的机器上。

所以这就是我们所说的阻塞。这样讲明白了吗？现在我们已经学习了三种不同的方式来进行并行连接：朴素方式、哈希连接方式和排序合并方式。除了第一种方式，第二种和第三种方式都可以使用。

这是我们之前在讨论核心算法时已经学到的内容。唯一的区别是，我们如何在一开始决定是否需要进行分区，或者何时需要将元组跨机器进行分发。所以实际上，之后你会看到更多的连接结果，正如我之前所说，连接是其中一个反复出现的问题，需要不断处理。

所以这就是数据处理中的核心问题之一，至今仍然是大家在不断研究的领域。所以你现在依然会听到人们讨论新的连接结果，正是因为这个原因。这也意味着没有一种通用的解决方案，不是说某个单一的连接方法就能一直表现最好。因此，我们才会讨论所有这些不同的选项。

好的，先暂停一下连接，过一会儿我们会回到这个话题。接下来的几张幻灯片中你还会听到更多关于不同连接结果的内容。但现在，我们也来讨论一下如何在并行中运行聚合操作。所谓聚合，就是像计算计数、求和等我们已经学过的那些操作。

对。所以这里的方法将采用分层的方式来进行。我们会为每个聚合函数分层执行操作，将操作分解为局部聚合和全局聚合两部分。是什么意思呢？举个例子，如果我们要计算一百万个元组的和，这就很适用。

我可以通过首先对元组进行洗牌或分区，分发到不同的机器上，然后计算局部和。接着，我会计算全局和，方法是要求每个机器把它们已经计算的和发送给我们。这个听起来很直观吧？我们有很多不同的机器，只需要让每台机器计算它们各自的局部和，在决定如何将数据分配到各台机器后。然后，在每台机器完成局部和计算后，我们只需把局部和加起来，就能得到全局和。

好的，计数其实是一样的操作，我们只需要让每台机器计算它们自己那一组元组的计数值，这里是小写的s。然后我们会把所有机器的局部计数加起来，最后得到的就是我们需要的全局计数。

所以你基本上可以看到一个模式，这就是我所说的将操作分解为局部和全局两个版本的意思。比如在计算平均值时，我们首先计算总和和计数，然后全局平均值就等于全局总和除以全局计数。

到目前为止，如果你能弄清楚我们如何运行操作，譬如说最大值或者最小值，类似的方式对吧？这张图只是一个示意，展示我们如何进行一些操作。例如，这就是我们如何以并行的方式进行求和。所以，再一次强调，我们需要对数据进行分区，使其能够分布到不同的机器上。

剩下的部分基本上是完全独立的，每台机器可以在本地独立运行聚合操作。之后我们将收集这些结果，然后计算全局结果。那么，关于分组呢？分组和聚合通常是一起进行的，就像我们在预处理阶段学到的一样。那么如何处理大量元组的分组计算呢？

有一个版本类似于空洞的方式，我们进行本地聚合。你知道，我们会追踪正在计算的组键，然后根据我们拥有的数据分区，在哈希表中保留本地聚合结果。接着，在每台机器上计算完本地的分组之后，我们就进行一次洗牌步骤，将本地聚合结果转发给一台应该接收的单一机器。

举个例子，假设在这种情况下，蓝色机器负责所有从负无穷到 10 的组，绿色机器负责从 11 到 100 之间的所有组，依此类推，橙色机器也负责相应的组。在这种情况下，我们首先要求本地机器计算本地的分组，无论它们最终应该计算的内容是什么。

然后，我们只是根据它们应该计算的范围，在机器之间洗牌部分计算的结果。因此，最上面的蓝色机器会收到所有对应于负无穷到 10 的组，这些组来自绿色机器和橙色机器，经过第一步之后。

然后，它会在接收到本地计算结果后，为该特定组计算最终聚合结果。到目前为止有任何问题吗？哦，对了，这是一个挑战，弄清楚如果我们有如此多的组，以至于它们无法在单一机器的内存中存放，会发生什么。

你可以做类似于 Grace 哈希空间哈希连接的操作，实际上你可以尝试弄清楚该怎么做。而且不是为了直接解决这个特定问题，你应该能够完成这个练习。但我会告诉你，这种情况在实际中是相对罕见的。因为通常情况下，我们有数百万个二元组。

但我们不太可能有数百万个组。对吧？因为除非你在像学生ID这样的唯一标识符上进行分组。如果你在学生ID上进行分组，我想你首先要回答的问题是，为什么要写这个查询？我的意思是，为什么你要按某个你知道不会形成任何组的内容进行分组，除了一个单独的工具。

好的，你知道了，但除此之外，如果你真的进入那种情况，即拥有如此多不同的组，以至于它们无法适配到单一机器上，那么你知道，我们可以。你可以考虑如何使用类似 Grace 哈希连接的方式来解决这个问题。

到目前为止我们学到的基本上是如何并行运行这些不同的关系操作符。重点放在连接操作上，因为我们知道有不同的算法可以实现。

所以，这就让我们通过讨论不同类型的连接算法来总结这个图景。除了他们目前讨论的那些算法，分别是：一是朴素版本，二是Grace哈希连接版本，三是智能合并版本，对吧？好的。

现在我们也学习了排序和其他不同的方法，那么让我们看看是否还有其他方法可以运行并行连接操作。第一个是有时你会称之为“单边洗牌”。这是一个特殊情况，如果我们知道其中一个关系已经被分区，那么我们就不需要再次对该关系进行分区了，对吧？

假设我们已经知道我们的关系跨越了三台不同的机器。那么我们需要进行数据洗牌，即对其他需要分区的关系进行洗牌。如果它还没有被分区的话。然后我们可以在本地运行这些连接操作。所以你可能会说，这有点像作弊的情况，对吧？为什么会发生这种情况呢？

但请记住，这些是“奇异”操作符，这些奇异操作符在树形结构中运行。所以，你知道，可能的情况是我们已经对其进行了排序。因此，某个关系可能已经排序好了。如果是这种情况，那为什么不直接利用这一点呢？

然后，你只需要对另一个关系进行洗牌，并运行连接操作，而不需要同时对两个关系进行洗牌。好吧，有时候确实会出现这种情况，如果是这样的话，我们就可以做单边洗牌。这个幻灯片正在讨论的就是这个情况。你可能遇到的另一个情况是，当其中一个关系与另一个关系相比非常小。

假设如果你非常小，以至于它实际上可以完全适应单台机器的主内存。那么在这种情况下我们能做什么呢？嗯，如果是这样的话，我们可以简单地将整个关系传送到不同的机器上。

忘记排序吧，忘记运行哈希函数来试图弄清楚如何进行分区。我们只是将整个关系复制到所有机器上，然后让它们运行各自喜爱的本地连接算法。为什么这样做有好处呢？因为我们节省了时间，实际上不需要去琢磨如何进行分区，是否要构建直方图之类的东西，这些基本都不再需要。我们只需将整个关系复制到多个机器上。

所以这种情况发生的频率比你想象的要高。主要是因为，当我们连接两个关系时，并不能保证这两个关系的大小相等，甚至相对相等。一个关系可能比另一个关系小得多。如果我们遇到这种情况，并且如果它恰好也能适配这些独立机器的主内存。

为什么不直接使用这种广播连接机制，而不是解决如何洗牌数据的问题。最后，让我们再谈谈你们知道的，有些连接操作是我们基本上需要等待的，直到某个步骤完成。

通常这是最初的步骤之一，要么意味着分区情况，要么意味着排序合并连接的情况，基本上是排序必须完成后，我们才能实际进行合并。而在哈希构建的情况下，当我们谈论到优雅哈希连接时，我们必须等待哈希表在其中一个关系上首先构建完成，才能对另一个进行洗牌。

所以在这种情况下我称它们为管道中断点，因为它们正是我们需要等待的地方。在完成那个阶段之前，我们不能继续执行其余的连接操作。无论这意味着开始，还是意味着创建哈希表并存储在内存中。是否存在一种连接方案可以完全实现流水线处理？

我们需要这样做是因为我们不需要等到第一次通过结束后再进行连接。正如我之前提到的，有一种叫做对称管道哈希连接的技术，基本上它避免了我们需要等待的情况。它的工作方式是，我们基本上会在每台独立的机器上构建两个哈希表。一个哈希表用于我们自己，另一个哈希表用于我们要连接的关系。

为什么呢，因为现在当一个工具从我们的端口到来时，我们基本上会像我们之前做的那样构建哈希表。好的，但当一个工具从s端口到来时，我们实际上会查询我们已经构建好的哈希表，就像我们进行哈希连接时一样。唯一的不同是，我们还需要为我们自己构建一个哈希表。为什么呢？

因为下次我们从我们的端口接收到一个工具时，我们不仅要构建哈希表并将该工具放入其中，还需要查询另一个哈希表，看看是否存在匹配项。这里是与此对应的动画。所以，当哈希表出现时，抱歉。

当我们的（our）数据出现时，我们会把它放入我们的哈希表中，然后检查是否在我们的（us）哈希表中有匹配项。如果有，我会进行所需的连接。类似地，当我们的（us）数据出现时，我们会反向操作，进行同样的操作。所以注意，这里有一个权衡——所以不是为我们的（our）数据创建一个单一的哈希表。

现在我们将创建两个不同的哈希表，一个是为我们的（our）哈希表，另一个是为我们（us）哈希表。它们都应该驻留在内存中。你可能会问的第一个问题是，为什么这个方法在计算正确结果时有效？你可以看到为什么这个方法确实有效，因为所有需要生成的输出只会生成一次。那就是当它对应的部分已经生成时。所以下面如果是小写字母。

所以，如果要进行连接，则取决于 A 是否先出现或者 B 是否先出现。第二个出现的将与另一个连接，因为另一个已经放入了它对应的哈希表中。这样说有道理吗？对，所以如果它还没有出现，那只是因为你知道，应该与之连接的部分还没有被流入内存。这就是为什么这个算法完全是流式处理的原因——意味着我们不需要等待不同机器上任何阶段完成。

你知道，我们可以随时从 RNS 拉取这些不同的工具，带着它们如何在不同机器间划分，然后你知道，彼此独立地进行工作。所以这个特定的方案对于所谓的流处理引擎非常有用，即我们不在谈论一个有限大小的关系。所以，假设我想连接像你知道从 Twitter 等看到的帖子。

所以我们基本上每秒都会得到新的数据。因此，我们实际上是在谈论一个无限的关系。如果是这种情况，我们可以承受运行之前的任何算法，在这些算法中，我们基本上是在等待你知道，整个哈希表构建完成。例如，对于我们的（our）哈希表，因为在无限关系的情况下，

哈希表永远不会完全构建完成。所以我们唯一的希望是运行这些所谓的流式算法，在这些算法中，我们不需要等待某个特定阶段完成才能继续进行剩下的工作。

这是其中一个例子。所以你再次可以看到这里的权衡，对吧，我们基本上是在内存中构造两个哈希表，而不是一个。因此我们有更高的可能性，哈希表将无法装入内存。因此我们需要像你知道的那样，写出单独的桶，就像我们在进行Grace算法时一样，然后当处理完成后，再将对应的桶带回来。

到达，和我们可能需要担心的所有其他好事情。同样的，没有免费的午餐，对吧，总是会有权衡。在这里的好处是，我们不需要担心像你知道的那样。我们不需要等待一个阶段完成，以便所有的单独机器可以完全独立地进行处理和进度。

缺点是我们需要更多的内存。因此，是的，这就是我们想要说的关于并行预处理的所有内容。所以在这两节课中，我们讨论了像如何在母机之间划分两个帖子的话题。我们讨论了运行低级预处理时可能采用的不同架构。在这门课中，我们专注于“无共享”方法，因为它既便宜又稀缺。

我们基本上只是添加了商品硬件。那样的情况下，对吧。然后我们讨论了不同类型的并行性，包括操作符之间、不同查询之间的并行性，等等。接着我们讨论了如何利用上次讨论的基本构建块（例如哈希）在并行中实现关系运算符。然后这次是排序。接着你可以想象我们如何通过构建类似的预处理计划来进行并行预处理。

就像我们之前讨论的那样。所以你知道，你可能会考虑如何通过那样的方式进行职业优化。所以基本上我可以告诉你，这是一个类似的过程，我们基本上会做一个基于成本的估算，找出不同的职业计划的成本，然后选择最优的一个。

就像我们之前讨论过的，或者说是职业优化。那么这里有一个有趣的部分，对吧，事务呢？这也是我们现在在这门课中讨论的内容。现在我们知道如何在不同机器之间并行执行查询，那么如何在不同数据库之间并行执行事务呢？

所以我们以某种方式需要分布式块，对吧？我们之前提到过。那这是否意味着我们开始在模型机器之间获取大量数据，然后才能执行事务？我们该如何处理运气不好？现在当有多台机器参与时。并且我可以告诉你，我们需要新的协议，因为我们之前学到的关于两阶段锁定（2PL）以及其他不同方式执行事务的知识，在我们有并行机器的情况下无法直接应用。

事实上，这就是我们在休息后要讨论的内容。但是在休息之前，关于这部分讲座有什么问题吗？如果没有，或许我们可以直接进入休息时间。

![](img/36def6d6b9866c9c33a169a2bce65f2c_5.png)

我来共享屏幕。好的，这个是给错过视频的人看的。如果你们想看视频，请来上课。我们想念你们。教室里只有40个学生，我们只能看到五个人的面孔。所以请来上课，我们想念你们。

我们很乐意见到你们。好的，不管怎样，你们错过了所有的视频。所以，Tegan问是否是1115。你可能是对的。我想我还没有更新这个幻灯片。所以，这可能会削弱这个管理通知的目的，但确实有一个考试准备会，即将在1113或1115举行。因为我原来定的是1113，后来我被纠正了，所以Gabe有最新的信息。

好的，1115就是这样。那么我还想宣布的另一件事是，我们已经开始为那些在中期考试中有录音问题的同学进行口头考试了。而且，这完全是为了公平起见，对吧？这样做是为了确保那些参加了考试且没有问题的同学不会受到影响。

他们并不处于劣势。那些有录音问题的人，根本就没有录音。例如，他们也不处于劣势。我是说，他们只要进来就行。我们不会，咱们不会，咱们不是要为难他们。对吧？你只需要解释几个答案，基本上就是这些。就这样。

这些口头考试并不是为了让你们感到有压力。它只是一个简单的检查，目的是确保你们认真参加了考试，差不多就是这样。我知道Alvin今天进行了几场口头考试，还有一些已经安排好的。所以，Alvin，有什么要补充的吗？没有，对吧？好的。好的。那么期中考试已经评分了，成绩很快就会公布。我们正在等待这些口头考试结束，然后才能发布期中成绩。

请开始做第五个项目。我知道你们中的一些人还在处理第四个项目，但希望你们已经完成了，或者快完成了，并且开始着手做第五个项目。所以，如果有任何问题，请告诉我们，并充分利用办公时间。我在办公室很孤单，你们应该来。

好的。好吧，其他通知还有吗，Alvin，没有吧？好的，那我继续讲分布式事务。

![](img/36def6d6b9866c9c33a169a2bce65f2c_7.png)

好的，既然我已经提到过了，我们谈论的是如何将传统的关系数据处理操作符进行并行处理。现在，这只是处理过程。从事务的角度来看会发生什么呢？

如果你的数据分布在多台机器上，怎么处理事务呢？

如何在这种情况下保证ACID是一个挑战。这就是我们将要处理的问题。所以首先，关于术语的说明。请注意，上一讲讲了并行查询处理，而这一讲或这一组幻灯片讨论的是分布式查询处理，实际上有什么不同吗？至少从并行数据库的角度来看，我们讨论了三种。

三种机制：共享内存、共享磁盘和无共享分布式。我们说，分布式数据库基本上不过是无共享并行数据库。但在许多情况下，这也意味着网络较慢。好吧，所以并行和分布式之间的区别并不显著。

只是分布式通常有较慢的网络。所以这个分布式实际上可能意味着地理上的分布，可能甚至跨越多个大陆。好吧，这确实会对我们需要处理事务的方式产生影响。所以，如果东西分布在一个大陆的两端，比如西海岸和东海岸，或者美国和中国。

对吧，我的意思是，网络延迟可能是一个巨大的因素。所以考虑到分布式计算的最坏情况是很重要的。那么分布式计算有什么特别之处呢？它继承了我们已经知道的无共享并行计算，你有并行计算，以及不依赖共享内存，基本上我们有这些商品化的机器。

它们有自己的内存和磁盘，并通过共享网络进行通信，对吧。所以网络是所有这些机器共享的东西，这些机器通过该网络进行通信。因此，网络，尤其是在分布式环境下，设备彼此相距甚远，这些网络可能非常不可靠。所以你可能会遇到延迟。数据包传输需要很长时间。

然后有很多不同的方式进入网络。所以你可以有很多不同的方式进入网络。然后你可以有很多不同的方式进入网络。然后你可以有不同的方式进入网络。

然后你可以有不同的方式进入网络。然后你可以有不同的方式进入网络。然后你可以有不同的方式进入网络。然后你可以有不同的方式进入网络。

然后你可以有不同的方式进入网络。然后你可以有不同的方式进入网络。然后你可以有不同的方式进入网络。然后你可以有不同的方式进入网络。

然后你可以有不同的方式进入网络。然后你可以有不同的方式进入网络。然后你可以有不同的方式进入网络。然后你可以有不同的方式进入网络。

然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。

然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。

然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。

然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。

然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。

然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。

然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。

然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。

然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。

然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。 然后你可以通过不同的方式进入网络。 所以我们将重点关注事务并发控制和恢复，正如算法在上一个幻灯片中提到的那样，还有很多内容。

所以你最终会弄明白很多关于分布式的课程。 对不起，你已经知道了很多分布式处理的课程。 现在你将会知道分布式事务处理的课程。 好的，我们从分布式锁开始。 好的，像之前一样，我们讨论的是共享无分布式数据库系统。

今天我们将假设是分区而没有复制，我之前提到过这点，他说为了让查询运行更快，你可能会复制数据的多个副本。但这不是我们要担心的问题，所以我们只会关心分区，这是我和Alvin在并行计算部分谈到的。

所以每个事务你可以想象它会到达某个节点，而这个节点可能被指定为该事务的协调员，负责协调事务并将结果反馈给。例如，反馈给应用服务器，说：“嘿，这个事务已经提交，这个事务出了问题。”

好的，所以这个协调员可以被指定，因此可以为所有事务指定一个集中式协调员，或者可以根据某些方案动态分配，比如“环绕分配”。比如丢弃它。好的。那么问题是，如果你想进行分布式并发控制，锁表应该放在哪里？

通常，锁是与数据一起分区的，所以你将锁与数据保留在一起。因此，基本上每个节点都会管理自己的锁表，并包含所有相关对象的条目，基本上像页面和元组这样的东西会有与该节点相关的锁表条目。

现在，关于那些训练过的对象会发生什么情况呢？在这种情况下，假设是表、关系或整个数据库。那么通常你最终会为这些对象分配一个主节点。而且，这些主节点将包含训练锁。因此，被锁定的对象，无论是表还是数据库。

这些可能存在于不同的节点之间，但它们有一个主节点。例如，单独的元组（比如船只、预定和水手）以及可以驻留在不同节点中的页面，它们可能为每个状态或每个页面在这些节点中拥有锁表条目。但是，我们在哪里为关系本身拥有训练过的锁表条目呢？

因此有几种选择。一个是这些训练锁可以在节点之间分区。例如，这个节点被指定为持有水手关系的锁表条目，这个节点持有船只的条目，另一个节点持有其他的条目。所以这种分区可以发生。

通过哈希分区或轮询分区，所以基本上这些训练锁会被分配到其他节点，另一种选择是它是集中式的。所以你会有一个主节点，负责处理所有这些训练锁。好的。因此，没有一种方案是赢家，意思是两者各有优缺点。好的，那么让我们暂时忽略这些全局训练锁，来讨论剩下的部分。

所以每个节点都进行自己的锁定。它会锁定你可能在事务中引用的元素。所以这些元组，页面。而且这是不错的，对吧。这种方式既干净又高效，并且将单个节点的设置进行了泛化。所以每个单独的节点只需要关心自己，而不需要关心其他节点，因为每个节点都有一组元组和一组页面是它负责的。

它拥有所有这些对象的锁表条目。然而，依然存在一些全局性的问题，对吧，例如处理死锁的问题。那么，如何检测死锁呢？特别是当有多个节点参与同一个事务时，如何处理提交和回滚的问题呢？如果有多个节点参与这个事务，该如何处理？

对。所以这些就是我们要重点讨论的内容。到目前为止有什么问题吗？好的。那么我们就从分布式死锁检测开始。好的，我们讨论过单节点情况下的死锁检测，这基本上就是构建这个等待图。所以你可能会想，尝试将这种方法应用到分布式环境中，看会发生什么。

所以你可以想象，每个节点根据其正在执行的事务维护一个等待图，事务可能会对相应的节点上的锁进行修改。对，比如说，这个节点可能有这三个事务 T1、T3 和 T2，并且它们之间存在等待关系。

所以 T2 正在等待 T3，T1 正在等待 T2。在这种情况下，放大可能会有帮助。好的，T1 在等待 T3，T2 在节点 3 中没有等待任何东西，T3 正在等待 T2，依此类推。好的，所以你基本上在这些节点上都有这些等待图。现在。

显然，存在一个挑战，那就是如果你只在一个节点上而不查看其他节点上的等待图时，可能无法检测到循环。对吧。所以这里基本上每台机器没有循环，但存在一个全局循环。为了看清这一点，如果你从这个特定的图开始

然后将来自其他图的边加入进去。所以我加入了其他的边，然后你就有一个循环，对吧。所以在这里至少有一个循环。还有其他的局部循环。所以，在这个图和全局图中有多个循环。但在任何单独的节点或任何单独的机器上，都没有循环，对吧。

那么你如何处理这个问题呢？所以看起来每个单独的节点似乎不能意识到存在死锁，然后通过可能中止某个事务来处理死锁。所以看起来每个节点无法仅凭它自己本地的信息做到这一点。

所以解决方案很简单。实际上，就是定期将你的图形权重发送给指定的协调者。这个指定的协调者收到所有这些图形权重后，查看是否存在死锁。如果有死锁，它可能会说，嘿，你，这个特定的事务需要中止。然后它会与所有机器进行通信，所有机器就会意识到。

嘿，也许那个事务真的很糟糕。好吧，确实有一个指定的协调者，负责处理死锁检测。因为它有所有这些节点发送的信息。所以，有些人可能会想，嘿，假如在发送给协调者的信息传送期间发生了什么呢？

也许某个事务在某个节点决定提交，因此图形在某个节点发生了变化。这种情况当然有可能发生，在这种情况下，你可能会不必要地强制中止某个事务。这可能没问题，这不是一个坏的情况。如果在此期间出现新的死锁，发生死锁的时间恰好是在某个节点将其图形的权重发送给协调者之后。

如果由于这个间隔导致出现新的死锁，它们会在稍后被检测出来。所以我们现在不用担心这些。我的意思是，反正我们稍后会处理它们。所以总体来说，尽管这可能有点激进，可能有点过时，但整体上还是做得不错。所以，这就是死锁检测。

我们有一个协议，叫做两阶段提交，但我们先一步步来。好吧。如我之前提到的，每个事务都有一个协调节点。好吧，假设我的强人策略是，这个协调者将决定这个事务是提交还是继续执行。然后，一旦事务。

一旦协调者做出决定，它会通知所有其他节点。对吧？所以，它基本上会通知所有其他节点，这个事务将会提交或中止。对于这个方案可能存在的问题，你有什么看法吗？协调者崩溃无疑是一个挑战。

但假设现在协调者没有崩溃，但这也是一个挑战。如果两个节点得出不同的结论怎么办？正如Nicholas所说，可能有一个节点想提交，另一个节点想中止。在哪种情况下，一个节点可能想提交，而另一个节点想中止呢？

如果某个节点崩溃，那又是另一个挑战，虽然这与中止事务无关，但它也是一个问题。比如，可能有一个事务涉及到某个未崩溃的节点上的数据。因此你就无法进行变更了。

本地死锁可能是尝试在特定节点中止事务的一个很好的理由，这可能是一个节点希望中止事务的原因。但是否还有其他原因呢？是的，比如尼古拉斯，你可能会遇到这样一种情况，在某个节点的本地等待图中，你发现了一个循环，然后决定中止事务。

一个事务，可能用户只是想中止，用户并没有直接参与其中，实际上这些机器是用户请求了事务的提交。所以至少这是一个用户说，“我希望这个事务被提交”的情况，对吧？延迟，正如之前所说的，确实可能成为问题，可能会有一个节点延迟很长时间。

它想中止，但无法告诉协调者。总之，另外一个问题是，所有这些观点都很好，但我在总结之前还想提一下一个问题，可能有一个节点会说：“我想中止，因为你正在对我进行的插入会违反某些一致性。”所以它会违反一些完整性约束，比如主键约束。这个节点有本地信息，它可以做出这个决定并且说：“我不希望这个事务继续。”

但如果协调者单方面做出决定，那就像是独裁统治，你不希望那样，对吧？你是希望节点能够传达它们对事务提交的不满。所以说“不行”。这个强人方案的问题就在于，它没有考虑到所有可能参与该事务的节点的输入。

好的，除了这些情况，如我所说，其中一个节点可能希望中止，即使协调者希望提交，某些节点实际上可能已经宕机。因此，任何涉及数据的节点事务都不应继续。好了，那么让我们总结一下这些内容，并谈谈通常会发生什么问题。首先可能出问题的类型基本上就是来自节点的故障和延迟。

举个例子，如果我们没有收到某个节点的消息，我们就不知道该节点是活着还是死了。这个决定可能会影响到该节点，因此假设在那个节点上有可能发生外键评估。我们确实需要听到该节点的消息，然后才能决定是否提交这个事务。不幸的是，我们甚至可能不知道一个节点是否还活着。

如果节点已经崩溃，节点是如何恢复的呢？在一个可能有多个事务已经提交的世界里，恢复又该如何支持呢？所以当一个节点失败后需要恢复时，这就形成了一个挑战。好了，那么接下来我们将讨论本协议数据中的恢复方案。

所以这是在节点出现故障和延迟时可能出错的情况。那么在消息出现故障和延迟时，可能出错的情况是什么呢？首先，可能会在每个通道的基础上发生非确定性的重排序，并且在通道之间发生交错。好的，另外一个问题是，可能会有一些非常。

非常延迟的消息，挑战在于我们应该等多久才能处理一个延迟的消息。这条消息仅仅是延迟了吗，还是节点宕机了？很难将两者区分开来。所以，关于每个通道的非确定性重排序、通道间的删除以及丢失的消息，下面有一个动画，展示了这三种情况。

这里我有四条消息，分别是绿色、红色、黄色和紫色。正如你所看到的，红色和黄色的消息会被重新排序，绿色的消息会在两者之间交错，而紫色的消息将被丢弃。好的，这确实可能发生。这是一个可能发生的场景。

所以考虑到这一切，要弄清楚所有这些节点如何决定同意提交还是中止，确实是一个难题。好的，我想我将在此停下来，因为现在是六点二十九分。接下来，我将在下一堂课中讲解分布式投票协议和二阶段提交。到目前为止有任何问题吗？如果有问题，请留下来，否则我想我们将在这里停止报告，祝大家周末愉快。

再见，大家。再见。[BLANK_AUDIO]

![](img/36def6d6b9866c9c33a169a2bce65f2c_9.png)
