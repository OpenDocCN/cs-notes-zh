# Âç°ËÄêÂü∫Ê¢ÖÈöÜÂ§ßÂ≠¶ 14-740 ËÆ°ÁÆóÊú∫ÁΩëÁªú Fundamentals of Computer NetworksÔºàFall 2020Ôºâ - P20ÔºöLecture 20 Congestion Control The Router's View - ___main___ - BV1wT4y1A7cd

This is 147Ôºå40„ÄÇI'm saying that in a very low voiceÔºå because it turns out„ÄÇ

One of the reasons I enjoyed being in economycon auditorium is that my wife actually teaches also from our home and on Tuesday mornings at this time„ÄÇ

 she actually has a class too„ÄÇ

![](img/f7ad1b05502545c34f0eae43c163cc5a_1.png)

So I'm trying to keep my voice down because she's asked me to and I know that won't last very long and then she'll get annoyed„ÄÇ

 but that's okay„ÄÇWe'll deal with it„ÄÇAnyway„ÄÇThose of you„ÄÇ

 hopefully nobody's in economy auditudorium todayÔºå I appreciate you all helping me out with this actually if you're in economy I hope it's by your own choice rather than the oh I forgot all about this„ÄÇ

Kind of choice„ÄÇBut yeahÔºå we'll be basically for the rest of the year we'll be completely remote for our lectures„ÄÇ

So let's go ahead and get started„ÄÇToday we're going to be still in the network today's our last„ÄÇ

Lecture in the network layer we're going to be looking at at things the router can do to manage congestion so I promised you way back when when we're talking about congestion control in tCP I said there's more coming and this is the more that is coming we have turns out some things the router can do to try to handle the congestion that's going on„ÄÇ

You did a paper review for todayÔºå only one more that's it for the rest of the class and that's late in the class that's the software defined networking paper so you'll be able to take a bit of a breather for this„ÄÇ

Got homework twoÔºå also that hopefully you are working on and lab3 will be posted„ÄÇPretty soon here„ÄÇ

But other than that„ÄÇA little bit of a pauseÔºå a little bit of a chance to do some relaxing and take a breath for a little bit„ÄÇ

OkayÔºå today we're going to talk about two things one is we're going to talk about the different„ÄÇWs„ÄÇ

or can handle the packets that are incoming and we call these queuing disciplines turns out there are different choices the router can make different algorithms that they can run when they're figuring out how to make choices about the packets that are coming in and then we'll talk about the content of the Floyd paper which was the randomly drop gateways„ÄÇ

OkayÔºå so first off„ÄÇQuuing right we've talked about queuing theory„ÄÇ

 we know queuing means to stand in a lineÔºå we know that queuing is not the favorite activity for us as humans and if packets had any animation to them any any thought processes I'm sure they wouldn't enjoy standing in line either on their behalf we humans who are sending the packets want things to get through the network as fast as possible we don't want our packets standing in line either„ÄÇ

But unfortunately they do and that's because of this queuing delay we talked we actually talked about this way back when like lecture two when we talked about the different delays that you would see in a network and queuing delay is one of the big ones and it's one of the„ÄÇ

Most variable of the the delays and so we'd like to find ways to keep it from getting too far out of control the„ÄÇ

Remember what's happening in the router is the packets are coming in from all of the different incoming wires and the routers making decisions based on the forwarding table to figure out where they should go and anytime you get a couple of packets aimed down the same outlet wire so here's link 42 is the wire leaving the router and you have a couple of packets that are headed that way„ÄÇ

When one is in the middle of transmission„ÄÇYou can't be sending another packet„ÄÇ

 you can only send one at a timeÔºå and so the others have to get put in a queue„ÄÇ

 they have to start waiting around for that one to head out„ÄÇ

Now it turns out that there are several choices the router can make about how that happens and those choices we call packet scheduling this is a choice of the packets that are available„ÄÇ

 so let's imagine you have five packets that are supposed to go out that wire 42„ÄÇ

Which of them do you decide to send nextÔºå which choice do you makeÔºå that's a scheduling choice„ÄÇ

There's also a drop policy choice that's a choice that happens when you don't have any space for another packet and so when all your memory full„ÄÇ

 if you have an incoming packet and no place to put it what do you do we call that the drop policy of the router that's another choice the router has to make„ÄÇ

And„ÄÇBoth of these choices are going to affect the they're not really going to affect the performance of the router too much the router is still going to be sending at top speed it's just sending different packets at top speed and so the effect will be seen on the particular packets that are being sent even though the router„ÄÇ

wonon't see a big difference one way or the other„ÄÇSo I have a couple different queuing algorithms to talk with you about today„ÄÇ

 five of them in factÔºå in various levels of detail„ÄÇThe first of these is PIFOÔºå and this is the one„ÄÇ

You all know this is kind of the the normal progression of human life when we get into lines„ÄÇ

 those lines are oftentimes fiIO lines„ÄÇAnd it's likely you were imagining that this is what happens in the routers all the time and that would not be entirely wrong„ÄÇ

It's what happens most of the time this is a very common scheme„ÄÇ

 this is basically packets coming in you see it in this picture I'm trying to show this idea that there's a cu in the router and a particular packet„ÄÇ

At the front of that queue will be the one that's being transmitted and anything else waiting for it just gets put in that queue„ÄÇ

 put in line one after the other as they go„ÄÇAnd so that's kind of what I'm trying to show with this this blue box here being a packet that has arrived and this kind of area depicting how much memory there is for this cu with in this case three packets already in it and so this incoming packet just„ÄÇ

Joins the back of the queue„ÄÇKnow nothing about the the packet can make it special the routers choice so it's packet scheduling choice which of these four packets will get sent next its choice is„ÄÇ

BasicallyÔºå just whichever one is the front of the queue will be the one that gets sent next„ÄÇOkay„ÄÇ

 a couple things to note one of them is I'm showing this„ÄÇWith a single output„ÄÇ

 but we should recognize that the router is going to have many of these outputs and so there's going to be a queue at each of the outgoing space lines wires on the router okay„ÄÇ

 so if it has I've mentioned beforeÔºå they often have many 48 wires going into this router„ÄÇ

48 wires that I can transmit packets on or that the router can transmit packets on„ÄÇ

 so 48 different cues where you would be putting packets in because it doesn't make sense to put one in line except for that particular link that you're actually sending it out„ÄÇ

Okay„ÄÇAnd the router is able to do some like memory optimizations and things like that globally across all of the queuees„ÄÇ

 although we won't get into that too much„ÄÇAll rightÔºå so drop tailÔºå this is a drop policy„ÄÇ

I said earlier that a drop policy is a choice that the router is making when it doesn't have memory„ÄÇ

So I'm showing in this case or'm trying to show that the space for our queue is full„ÄÇ

And this should really mean that the router has no additional memory to allocate here„ÄÇ

Maybe you there's a certain amount of space for each of the outgoing length or maybe there's just none left on the entire router„ÄÇ

So there's a queueÔºå and in this caseÔºå it's full„ÄÇDroptail is the common that's the one you would kind of imagine if a packet shows up to the queue and the queue is already full„ÄÇ

 you just throw that packet awayÔºå you drop it okay you„ÄÇRemove it from memory„ÄÇOkay„ÄÇ

 and that means that that packet will never get sent if it was important TCP would have been used and it'll be detected and all that kind of thing„ÄÇ

 but that's higher level decisions controlling this at the network layer at this particular router„ÄÇ

Nothing you can doÔºå rightÔºå it's got to get dropped„ÄÇWell„ÄÇ

 technically there is something you can do you could drop a different packet instead droptail doesn't so droptail is the choice that says when I have to drop a packet„ÄÇ

The packet that will be dropped is the incoming packetÔºå the most recent packet that will happen„ÄÇ

As I saidÔºå it's kind of normalÔºå this makes sense„ÄÇTo us as humans who stand in line a lot„ÄÇ

 but it's not the only choice it is however the most common choice In fact„ÄÇ

 fiIO combined with this droptail is by far the most common choice you'll see in actual internet routers and„ÄÇ

Yeah„ÄÇespecially among the kind of routers you and I have in our house like those $99 boxes we buy or that the cable company gives us„ÄÇ

 those things don't do anything special and so this is the easiest to code up„ÄÇ

 this is what's going on inside of them„ÄÇIt's also the easiest to control„ÄÇ

There's some issues with this thoughÔºå maybe depends on how you look at it„ÄÇ

 one of them is the fact that there's no discrimination right you're just choosing whatever packet came in last„ÄÇ

And„ÄÇAnd there's nothing to say thatÔºå you knowÔºå some packets should be chosen because they're more important or they're required more timely something or anything like thats just„ÄÇ

Whatever packet happens to come inÔºå okay„ÄÇWe'll talk a little bit about how we may change this idea„ÄÇ

 but so far in the courseÔºå we really haven't talked about the idea that some packets might be more important„ÄÇ

Than other packets and how you figure out that importance and how that's decided as a tough choice„ÄÇÂóØ„ÄÇ

Of courseÔºå using PIO and droptail means that we're kind of implicitly saying that our end hosts are using TCP probably or some other reliable transport why do I say that well first off if you're dropping packets„ÄÇ

You know that you„ÄÇyou're not providing any reliable service now the network is not intended to be reliable right the network is a best effort network it's doing its best and when a packet comes in and a queue is full it can't do any better okay„ÄÇ

But you knowÔºå we're basically saying if that packet had to be important„ÄÇ

 then somebody else has got to deal with the importance level and so that kind of means TCP is running„ÄÇ

OkayÔºå it also means that all the packets are getting mixed together in this flowÔºå in this queue„ÄÇ

 and so if somebody is being a little greedy and not paying attention to congestion control algorithms or something like that„ÄÇ

 there's no policing going on here„ÄÇRightIt's that„ÄÇknowThat flow will be overflowing the queue and yes„ÄÇ

 some of their packets may be droppedÔºå but some of my packets will be too„ÄÇ

 and so I can be affected by somebody who's not behaving well„ÄÇLet's seeÔºå so I'm sorry Albu Hadi„ÄÇ

 I didn't notice here„ÄÇHere we go„ÄÇChaack question when it came inÔºå so let me take a look„ÄÇYeah„ÄÇ

 so you were asking about when I was talking about the drop policies right and in this case drop tail would drop this one„ÄÇ

 but another policy might decide that there was a different packet to get dropped„ÄÇOkay„ÄÇ

 so if I were to chooseÔºå you knowÔºå heyÔºå here's a packet to throw away„ÄÇThen that's a drop„ÄÇ

That's a drop policy that i'm choosing that one now what happens to the rest right do the rest slide forward well if my scheduling algorithm is a first in first out„ÄÇ

 then yesÔºå they would all have to slide forward because the incoming one was not the first in„ÄÇ

Compared to any of theseÔºå so it's got to go at the endÔºå okay„ÄÇ

But we'll see other policies that might drive a different decision there„ÄÇOkay„ÄÇSo I'm not sure„ÄÇ

What you mean by fairness policy here in this next questionÔºü

If we start distinguishing which package to drop based on burstinessÔºå et cetera„ÄÇ

 will that contradict the fairness policyÔºå I'm not sure I understand that there is a fairness policy right it might contradict our„ÄÇ

Oh„ÄÇKind of standing our idea of what fair is we kind of wantÔºå hey everybody's packets should„ÄÇ

You should have a fair shot at router resources and that I guess does mean that if we're making choices we're going to decide one packet or another„ÄÇ

 but there's no like oh I can choose this fairness policy or that fairness policy right have to deal yeah right fairness is something we„ÄÇ

We look at a lot of times when we're looking at these algorithms and the choices they're making„ÄÇ

 we want them to be fair„ÄÇBut it's not like there's a„ÄÇ

Policy driving that or something we have an intention to be fair„ÄÇOkay„ÄÇJeremy„ÄÇ

 will'll talk more about thisÔºå TCP does make that assumption„ÄÇRightÔºå TCP„ÄÇ

You using the receiver feedback to know if there is some congestion and so anytime you know if this packet gets dropped that TCP flow will do a couple things„ÄÇ

 one it' going totrain it's going to detect the loss because it has a timer going„ÄÇOkay„ÄÇ

 and so it will retransmit to to take care of that one that was droppedÔºå but you're right„ÄÇ

 it will also respond to that as if there was congestion„ÄÇ

Wwhich is probably the right thing to do look look at a queue right that thing right there it's full right so that's congestion so yes we would like TCP to notice that this thing dropped and to stop sending or to slow down the sending but let this router clear out this queue so it is reasonable to assume that and that's in fact the intention„ÄÇ

OkayÔºå a different„ÄÇQuuing strategyÔºå a different choice you would make about or a choice that a router might make about how to choose which packet to send and's„ÄÇ

One of the it seems like this is the fundamental thing as soon as people get mad at PIFO and want something else„ÄÇ

 they say well some things are more important than others„ÄÇ

 we should have a priority right what shouldn't the important stuff go before the unimportant stuff and priority queuing is a reaction to that it's a way to to allow for some priority to make the important stuff not be waiting behind the less important stuff„ÄÇ

And so the idea here is I've broken my single queue now into several cues„ÄÇ In this case„ÄÇ

 I've got three„ÄÇWith three different priorities on them right I have a high medium and low priority„ÄÇ

And you'll notice I have three packets in the high„ÄÇ

 I have a lot in the medium and five packets in the low priority cues„ÄÇ

And the idea is that when a packet comes inÔºå we will put it in the queue„ÄÇ

That corresponds to its priorityÔºå and so I'm going to have to have some label or something on a packet to know what the priority is„ÄÇ

And then its priority will act in a PIO fashionÔºå so if this if I have a packet coming in that is low priority„ÄÇ

 it will go at the back of the low Q and the low Q acts in PIO fashion„ÄÇ

 so of all the low priority packets the first one in is the first one to be sent„ÄÇ

The difference here of courseÔºå is I have these priorities and so the router as a whole is not acting as PIO right only each of the priority levels is acting PIFO„ÄÇ

The the router as a whole is working through all the high priority ones Every time it has to make a choice of which packet to send it's going to send the one with the highest priority of those available So in this case I have three high priority packets I'm going to send those one to three when those are done if the high priority queue is empty then we'll go to the medium queue and we'll send the first one at the front of the medium queue and work our way through all those and when we're done with the medium queue then we'll go to the low queue and make that happen so that's kind of the intention of this loopy thing I put in this picture right I'm looping here on a priority on a single queue I'm going to send everything out of this queue before I go on to the next queue„ÄÇ

Okay„ÄÇÂóØ„ÄÇSo youho„ÄÇIf you have a single priority cuÔºå that's the same as PFOÔºå right„ÄÇ

 that's kind of like this one„ÄÇEverything is the same priority right and so I'm tempted to use that line from the Incredibles about when everybody's special kind of think you look everybody's high priority right they all go in the same queue okay and so it only makes sense to do priority queuing if you have multiple priorities if you have something to make a decision based on„ÄÇ

Okay„ÄÇThere is a problem here„ÄÇIt dependss on how you look at it as still whether this is a problem„ÄÇ

 rightÔºå priority queuing oftentimes it's annoyed„ÄÇMe it's the noise my innate sense of fairness also because I was in the military for many years where theres a lot of priority queuing going on and you know when you're the lieutenant„ÄÇ

 you get annoyed at the generals who always go first right and the problem is that the general's always going first means that you can get starved as a lieutenant right there's a it is possible that these low priority packets never get handled„ÄÇ

OkayÔºå you can imagine I've got three high priority packets here that does not mean that this„ÄÇ

First low priority packet will be sent as soon as we send oneÔºå two„ÄÇ

 three high and then eight mediumsÔºå it's next up because during that time when you're sending these three highs and those eight mediums you can have other incoming packets that might go in the high queue and so they will step in front of you and go ahead and so it's possible that these low priority packets never get sent or takes a long time to send them we call that starvation„ÄÇ

EdtricÔºå you get your hand up„ÄÇYeahÔºå it was a question about like having three different priority cues so„ÄÇ

If so when we say that like we drop a packet when the cues are full„ÄÇ

 do each of the three cues have the same capacity in terms of like a memory allocation or is it one allocation for all of threeÔºü

No it's a great question and it's going show up in multiple places here because my picture kind of indicates that there is some memory set aside for high and medium and low and you have like the same amount of space for them that's typically not how you're going to manage this the router has a big pool of memory that it is using and there's no reason it can't use some of that memory for something else right so maybe maybe there's another entire link somewhere that has its own high medium and low„ÄÇ

Priority cues and nobody is sending anything out that link„ÄÇ

 I can steal some of that memory and bring it over and use it for these these particular packets„ÄÇ

OkayÔºå and so usually it's not going to be as static as these pictures seem to indicate and so if you know like right now„ÄÇ

 if another medium comes in it looks from my picture like there's no memory space for it in the queue„ÄÇ

OkayÔºå but that's an artifact of how I've drawn the pictures„ÄÇ

You would merely still take some of the memory that's not being used elsewhere and use it to store that packet„ÄÇ

Does it makes sense„ÄÇYeahÔºå okay yeahÔºå I was wondering because if if we did have static allocations then running out of memory for the medium one and dropping a low priority packet doesn't seem like it would solve that issue So that's right that's right you'd end up like dropping it It doesn't make right so first a couple things to remember one of them is that this is a scheduling algorithm Okay this is not a drop policy and so we can choose different drop policies„ÄÇ

Based on what's going on here„ÄÇTypically that any policy we're going to choose is going to want to drop lower priority packets than higher priority right you'd never want to throw away a high priority packet if you have a low priority packet taking up space and you can recover that space from throwing that away„ÄÇ

So yeah„ÄÇGurishs got„ÄÇOh you've got the key point to all this„ÄÇIf we were in the classroom right now„ÄÇ

 I would ask anybody who would volunteer to let your packets get sent at low priority to raise your hand okay„ÄÇ

 and you know anybody out there who'd like to have all of your packets sent at low priorityÔºåYeah„ÄÇ

 nobody rightÔºå we all want our thing to go high priority and so the question whenever you have a priority cu is how do you decideÔºü

OkayÔºå and„ÄÇI point out on the slide that you could solve this through economics right this is how we solve many Qing problems through economics right anybody've been to Disneyland recently right you can stand in the long line or if you're willing to shell out some bucks they have a lot of different mechanisms for you to have I don't forget what they are you know fast passes or whatever that let you have some sort of priority right or you know at the airport you can„ÄÇ

Pay extra in terms of your loyalty and pass purchase decisions to become a frequent flyer and to be able to you know stand in get in the first boarding group queue instead of the second boarding group Q right that's how we normally solve these problems„ÄÇ

The issue hereÔºå howeverÔºå is that the network is decentralized„ÄÇAnd so there's no way„ÄÇ

No billing mechanism that is available that would let some network somewhere else„ÄÇIn the world„ÄÇ

 charge me for my packets„ÄÇIf I'm sending a packet and happens to go through a server into Tajikistan„ÄÇ

 I don't get a bill from a network in Tajikistan once a month sayingÔºå hey„ÄÇ

 we sent 14 of your packetsÔºå U owe 12 cents„ÄÇI could imagine you paying money to your local ISP right and saying„ÄÇ

 oh yeahÔºå sureÔºå you knowÔºå Verizon I will I„ÄÇAt the„ÄÇInstead of the slow plan or something and all my packets will be high priority instead of low priority„ÄÇ

The problem is that only holds through Verizon's network right as soon as Verizon passes my packet off to some other network it loses any special glamour that Verizon would have put on it or that I would have paid Verizon to put on it okay so there's no really good way and people have tried to figure this out for decades people love to make a business where we could find ways to get all the users to sign up for the gold plan but it just hasn't happened so we don't have a good way to do this we do have a couple of heuristics that sometimes run into priority queuing for instance there are some ways to look at particular packets and say oh this one is actually probably more important than another„ÄÇ

It tends to be things likeÔºå ohÔºå BGP packets are important„ÄÇ

 right those are routing packets that are fundamental to the workings of the internet„ÄÇ

 those should go first„ÄÇüò°ÔºåOkayÔºå and so that that sort of choice sometimes get made in terms of putting stuff into different cues and we'll talk a little bit more about network engineering when we get to software defined networking„ÄÇ

Within a networkÔºå it might be valuable for a particular network to make some decisions about how they'd like to run„ÄÇ

Flows through their particular routers and in that case then as Ro Ha has pointed out„ÄÇ

 the network administrator would have to be able to say„ÄÇ

 oh the packets coming from here and going to there are a higher priority then packets that intersect through there and so they somehow get set with a higher higher priority„ÄÇ

OkayÔºå and Jeremy„ÄÇIt's the sort of thing that makes net neutrality people go crazy right because if and so part of me is glad that there is no chargeable„ÄÇ

 good chargeable mechanism in the internet to make this happen otherwise there will be backpart deals and you know Netflix would be able to pay Verizon to make sure Netflix traffic went into the high queue and you know we'd never have any other other company able to start streaming because you know they would wouldn't have the money to talk to Verizon„ÄÇ

To work withÔºå you knowÔºå to be to negotiate as well as as Netflix would or something like that„ÄÇ

Abdul HadiÔºå you have your panÔºüYeahÔºå I was thinking that for example„ÄÇ

 if there's if I'm a server and I'm an application and I'm serving packets to my client„ÄÇ

 can't I like assign them priorities based on what I want to get done with first and what I want to get done with laterÔºü

And so we could maybe use the options field in the IP header or something like that to implement that„ÄÇ

 but won't that work„ÄÇWellÔºå so actually we have so let me answer the last part first„ÄÇ

 therere actually our label fieldsÔºå we haven't talked about them much„ÄÇ

 but there's that TOS field in the IP header and that was the intention of that is to provide some differentiated service mechanism„ÄÇ

 the reason it hasn't taken off though is because of this decentralized idea„ÄÇAnd that kind of„ÄÇ

 you're right that if„ÄÇIf the application was something that was intended to operate on a particular network„ÄÇ

ÂóØ„ÄÇAnd you had a way to communicate„ÄÇBetween the application and the network layer on the end hosts„ÄÇ

 right somehow you'd have to have your application able to set that value on packets on my computer admitted„ÄÇ

but even thenÔºå that meaning only has meaning in the networks that are willing to support it„ÄÇRight„ÄÇ

 so a Verizon app on my„ÄÇMaybe on my Verizon phone working on a Verizon network could set these values„ÄÇ

 but if those packets ever then headed off to some other network„ÄÇ

 the meaning of them would be lost entirely„ÄÇAnd reply packetsÔºå of course„ÄÇ

 would not have that set unless they also happen to come from a Verizon phone with a Verizon app„ÄÇ



![](img/f7ad1b05502545c34f0eae43c163cc5a_3.png)

SoÔºå yeah„ÄÇSo another scheduling algorithm that is has a few less issues about choosing priorities is one that„ÄÇ

That basically says every flow ought to be the same„ÄÇ OkayÔºå so we'll have some you knowÔºå some„ÄÇ

Some fairness among flowsÔºå but we're not going to choose one flow as being more important than another and the idea here called round Robinqueuwing„ÄÇ

Is that each flow gets its own cue and now how do we define a flowÔºü

Remember back to the net flow discussion we have a bunch of packets and we're trying to figure out does this packet in this packet have something to do with each other and we would do it based upon some of the fields in the headers right so we would be looking at things like destination IP address and port numbers and things like that to try to figure out if this packet and this packet belong in the same flow„ÄÇ

And so somehow we would have determined that that these are part of the same flow„ÄÇ

 and then we just would put them into their own queue„ÄÇ

Okay so in this case you see I have four active flows here and they have differing numbers of packets and the round Robinqueuing algorithm says let's choose one packet from the first flow and then go choose one packet from the next flow and then a packet from the third flow and then a packet from the fourth flow and then let's come back to the first that's what I'm trying to show with this arrow right we're not working through a single queue at a time„ÄÇ

 we're doing one thing from each and then going back around one thing from each queue„ÄÇOkay„ÄÇÂóØ„ÄÇAgain„ÄÇ

 this is a scheduling algorithmÔºå so the questions I have here are drop policy related right flow two has a lot of packets in it„ÄÇ

OkayÔºå so what happensÔºüDo IÔºå as we talked about earlier with the kind of static picture here„ÄÇ

 do I sayÔºå ohÔºå you knowÔºå actually flow4 has plenty of memory„ÄÇ

 let's go ahead and take flow twos packets„ÄÇAnd use memory that is shown in this picture for flow4 okay I could do that okay or I could just cap I should say„ÄÇ

 oh maybe I should have enough memory set aside so if flow four comes in with another seven packets it would only be fair for flow4 to be able to store those seven packets and so if I've taken that memory and given it to flow two that wouldn't be bad those are all drop policy questions„ÄÇ

OkayÔºå and„ÄÇI could argue either way right there are good things about about having the static picture and saying flow to is sending too many packets let me go ahead and not accept anymore okay and there are good things about oh we might as well accept them because we have memory for them I don't know what we're basically doing is trying to predict the future what activity will happen in flow for and maybe that memory would be needed and maybe„ÄÇ

When„ÄÇOkay and this this quote here this last bullet multiple cues doesn't necessarily mean more wasted space that's trying to get to this point that Edric brought up about the the nature of the memory and whether it is allocated specifically to particular flows or not there's no reason that you you can't manage basically you know these values in the cues can be pointers basically to the actual packet sitting in some big„ÄÇ

You know big pool of memory across the entire router„ÄÇOkay„ÄÇThere's an issueÔºå thoughÔºå rightÔºü

Flow2 looked like this„ÄÇBeing greedyÔºå you'd sent more flowes than the other„ÄÇ

 more packets than the other„ÄÇBut it turns out all our packets don't have to be the same size„ÄÇ

And so it's here I've changed the picture slightly right and flow2 now has sent only a single packet„ÄÇ

But that packet is eight times the size of those packets that were previously there and under round Robbinqueuing„ÄÇ

Flow one will send a single packet and then flow two will send a single packet„ÄÇ

 which could be this huge thing„ÄÇAnd then flow three and then flow four„ÄÇ

And that doesn't feel fair to us right because if that was sent as these eight different„ÄÇPackets„ÄÇ

 we would have only sent one of themÔºå okayÔºå but if flow2 is sending bigger packets than anybody else„ÄÇ

 they get more bandwidth„ÄÇBecause the bandwidth is a per bit thingÔºå not a per packet thing„ÄÇOkay„ÄÇKyle„ÄÇ

 hang on a few slidesÔºå we'll get thereÔºå we will put priority and Rob Robin together in a minute„ÄÇÂØπ„ÄÇ

Okay so how do we make this more fair how do we fact that not all packets are the same size that's the next algorithm the next scheduling mechanism that we would use to choose packets and it's called fairqueuing the idea is we want to deal with this problem whereby„ÄÇ



![](img/f7ad1b05502545c34f0eae43c163cc5a_5.png)

We're sending„ÄÇPackets as a wholeÔºå but we want to allocate the amount of bandwidth to the bits in the packets„ÄÇ

 not to the packet themselvesÔºå and we want to make it so that you know here I've kind of rewritten the picture to show that we have in this case packets of different size„ÄÇ

If we did round RobinÔºå I just chooseÔºå you knowÔºå packet of size fiveÔºå packet of size 10„ÄÇ

 packet of size threeÔºå packet of size five packet and all of a sudden at this point in the flow„ÄÇ

 purple has gotten to send„ÄÇ15 units worth of„ÄÇOf packets„ÄÇBefore green was it„ÄÇAndend on both„ÄÇ

And that doesn't seem fair to us„ÄÇPurple is using more bandwidth than green is just because purple is sending bigger packets„ÄÇ

OkayÔºå and we don't want„ÄÇYou knowÔºå if the whole network worked that way„ÄÇ

 you might actually see applications being written to sendÔºå you know„ÄÇ

 let me collect and make sure my packets are as big as possible so that I can send the bits„ÄÇ

 what you want is applications to be sending amounts of data that make sense for what it's doing not to try to still you know get their fair share of bandwidth„ÄÇ

And so fair queuing is a slight change to„ÄÇA round Robin algorithm where instead of just going one packet from each queue in order„ÄÇ

We're going to go as if we were sending a bit at a time„ÄÇSo you knowÔºå I can't do that„ÄÇ

 I can't send a bit of a packet and then a bit of a packet knowÔºå back and forth a bit of each„ÄÇ

 but I can kind of pretend that was„ÄÇOkayÔºå so the idea is if I were sending bid at a timeÔºå then„ÄÇ

I can calculate at what point in time I would be done sending this packet of size five„ÄÇ

It would take me five time units to get to that and it would take me then an additional three time units to get the second packet sent„ÄÇ

And so that second packet kind of finishes at time eight„ÄÇ

As opposed to this first packet that the purple one hasÔºå which would finish only at time 10„ÄÇ

And then time 15„ÄÇ and so I'm going to use these finishing times„ÄÇ

To go ahead and make the choice about which packet it gets sent„ÄÇ

And that's going to allocate my bandwidth as if we were sending kind of bit by bit by bit and collect that up until we have enough„ÄÇ

Bit sent to go ahead and send the whole packet as a whole and so if my order is different you'll notice in the previous slide we went green then purple green purple right now we're doing green green purple„ÄÇ

And this means thatÔºå you knowÔºå the green was able to send eight and purple 10„ÄÇ

 and that's closer to being fair than five for green and 10 for purple„ÄÇOkay„ÄÇ

Does that make sense to everybodyÔºüFrom a„ÄÇAcademic perspective„ÄÇ

 if i'm talking about algorithms and you know writing out conference papers and talking about you know who chooses which I can there are some kind of„ÄÇ

Academic names we put on on some characteristics of this so we measure the algorithms and it turns out actually a lot of our algorithms are what's known as work conserving„ÄÇ

Right so what you you don't want to waste work time that the router could be sending packets so almost because all of them we've talked about are work conserving in that they will send a packet if there's one anywhere to send„ÄÇ

Even I guess I should point outÔºå I didn't when I was talking about Ron Robin„ÄÇ



![](img/f7ad1b05502545c34f0eae43c163cc5a_7.png)

I send once I've sent one packet from each of these flowsÔºå the second time around flow4 is empty„ÄÇ

OkayÔºå I don't sit around twiddling my thumbs„ÄÇYou know pretendending like well this is the time when I would be sending from F4 if they had one right instead we want the router to be work conserving so if there's nothing in a flow you just skip it and go back you know and go on so as long as there is a queuee to a packet to send from any queue we're going to send it okay that's what we mean by work conserving and all these algorithms are work conserving„ÄÇ



![](img/f7ad1b05502545c34f0eae43c163cc5a_9.png)

Okay„ÄÇFairqueuing goes beyond thatÔºå and it does a good job of sharing the bandwidth in the link„ÄÇ

As long as if there are multiple flowsÔºå then each flow is going to get its fair share of the bandwidth regardless of the packet size it's using„ÄÇ

Okay so it's going to get one in of the bandwidth„ÄÇWhen measured over many packets„ÄÇOkay„ÄÇ

And that's because the calculations of how to do fair queuing are based upon how many bits you're sending„ÄÇ

And so if you're sending more bitsÔºå you getÔºå or you knowÔºå if everybody's sending bits„ÄÇ

 then everybody gets their fair share of the actual bandwidth„ÄÇA„ÄÇ

Another word we use to describe these is max Min fairness„ÄÇ

Which fairqueuing achieves and it's a little bit weirder concept and I'm not sure my words here„ÄÇ

Describe it in a non confusing wayÔºå it maximizes the minimum data flow of any flow„ÄÇ

And the minimum data rate of any flowÔºå what that means is if you look at an algorithm„ÄÇ

Let's take priority queuingÔºå rightÔºüWhat is the minimum possible„ÄÇData rate of any of the flows„ÄÇ Well„ÄÇ

 ifÔºå if we look at the low priority flowÔºå low priority QÔºå the minimum data rate„ÄÇ

 it is possible to have a data rate as low„ÄÇ WellÔºå as0„ÄÇ right you can have starvation„ÄÇ

 And so as long as„ÄÇHigh priority packets are coming in„ÄÇ

 the low priority queue could get zero bandwidth„ÄÇOkay so that's the minimum data rate it has„ÄÇ

And what you'd like is an algorithm that maximizes that minimum value„ÄÇ

 that's why we call this max min right I'd like that minimum value to be higher„ÄÇ

 I don't like zero as my potential data flow if things just happen to go badly„ÄÇ

Okay I'd like to get that number up a little bit more„ÄÇOr how do I do thatÔºüOkayÔºå well„ÄÇRight„ÄÇ

Other algorithms like fairqueuingÔºå fairqueuing is going to make sure that there's no way to starve you„ÄÇ

 even if some other flow is sending big packets„ÄÇRight and so that you know you can do the proof and prove that there is no way to get the minimum level of service any higher you've maximized„ÄÇ

The worst case„ÄÇSo the worst case isn't as bad as it could be„ÄÇBasicallyÔºå in fact„ÄÇ

 the worst case is the best„ÄÇIt could be of all algorithms„ÄÇ

That's what we're saying with maximum fairness„ÄÇThere is another touch to thisÔºå my fifth„ÄÇ

Quuing algorithm that's when Kyle was mentioning earlier of let's add priority and fair queuing and and this is„ÄÇ

If you're going to do fairqueuingÔºå weighted faire queuing is not that much of a difference„ÄÇ

The idea is that each of the cues is going to have its own weight assigned to it„ÄÇ

 its own priority and again„ÄÇWhere priority comes from is an interesting question„ÄÇOkay„ÄÇ

 I'm saying here somebody manually configured it right or somehow you have some other signaling mechanism that says these packets are in this particular flow happen to be high priority„ÄÇ

Or maybe thered be GP packetsÔºå so therefore that flow is high priority„ÄÇü§ßAnd then„ÄÇ

What you do is when you're trying to do the calculations of finishing time for each packet„ÄÇ

 the finishing timeÔºå you can divide the finishing time by the priority„ÄÇ

 or you can multiply the number of bits you're allowed to send by your priority„ÄÇ

 either one would be the same„ÄÇAnd that means that the flows that happen to have a higher weight end up sending more bits than the flows that have a lower weight„ÄÇ

And„ÄÇYeahÔºå so this is a„ÄÇA slight tweak to the scheduling algorithm„ÄÇ

 it is a useful tool when you do some things like you can some of the more complicated„ÄÇ

Reservation based schemes or things are trying to build what are called differentiated services architectures that that is systems that would allow for different priorities different quality of service is another term we use QOS that lets you assign different priorities to different flows and and actually be able to base„ÄÇ

Your reservation so in a reservation schemeÔºå you would have you would be sending„ÄÇ

Signals to all the routers along a path ahead of time basically saying„ÄÇ

 I'd like to reserve some bandwidth and that would be giving flows with particular weights in each of those particular routers you're going to send through„ÄÇ

More complicated we don't talk about that much in this class„ÄÇAdvanced topic„ÄÇOkay„ÄÇ

 any questions about any of those queuing disciplinesÔºå drop policiesÔºå anything like thatÔºüAlrighty„ÄÇ

 let's move on to the paper you read„ÄÇÂóØ„ÄÇThis isÔºå as mentioned our„ÄÇ

Because it's our last shot at thinking about congestion control„ÄÇ

 which we've spent a lot of time thinking about in terms of the endOSs and how TCP manages congestion control and how it can run different algorithms„ÄÇ

Those were easy to change and easy to instantiate it's a little bit harder when you're getting routers to do the same thing you if you have any widespread congestion control algorithm you want to incorporate„ÄÇ

 you need to change a lot of routersÔºå which can be difficult C IPV6„ÄÇ

The other problem is that congestion control in like all the algorithms in TCP„ÄÇWell„ÄÇ

 they're controlling their sending rates based upon what they measure or what they think is happening„ÄÇ

OkayÔºå and so they do things like here's one of the strategies for all the load based„ÄÇ

Or loss based congestion control algorithmsÔºå you know when we saw some collisions happening or when we saw some loss happening„ÄÇ

 when we saw some three do backs or whatever signals that TCP was collecting„ÄÇ

That meant that the congestion was already starting to have an effect and so the TCP implementation would then pull back the congestion window„ÄÇ

 you'd slow down the rate at which you're sending„ÄÇThe packets and„ÄÇ

And hope to control congestion because of that and that means you kind of have to step your foot into the realm where there's actually congestion happening and you're actually seeing some loss occur before you can do anything about it„ÄÇ

And we had delay algorithmsÔºå by the wayÔºå that triedÔºå did a good job actually of predicting it„ÄÇ

 they would measure the roundship times and see that the delays were increasing and sayÔºåooh„ÄÇ

 looks like there's some congestion coming because of that„ÄÇ

The idea that we're going to talk about today is more of a„ÄÇ

 let's see if we can avoid the congestion a whole lot„ÄÇIn the first placeÔºå right„ÄÇ

 let's not get into that realm where we're dropping packets„ÄÇ

And we're going to do that based upon knowledge that the router has„ÄÇ

The individual router knows what's going on on a particularÔºå well„ÄÇ

 remember also this is on a particular output link„ÄÇSo the router is going to look„ÄÇ

At an output link and say ohÔºå that the cues are really long there there's a lot of packets being sent„ÄÇ

 that's congestion and it can do that based upon the state it sees at that instant going on in the network„ÄÇ

And so the idea is was your way to go ahead and use thisÔºüTo tell the senders to slow down and this„ÄÇ

Its a very fundamental idea it's been tried over and over again„ÄÇ

 in fact there are other network technologies like deck bit you guys have probably never even heard of the digital equipment Corporation company that's sold many„ÄÇ

 many many mini computers ATM is different networking technology TCP actually has a thing called explicit congestion notification it's an option field in the TCP headers you that lets a router specify oh look that„ÄÇ

This particular segment saw some congestion and that would be mirrored by a TCP receiver when it sends the ax so that the sender would look at this and say„ÄÇ

 ohÔºå there was congestionÔºå let me slow down„ÄÇSo those ideas are all there have been tried„ÄÇIn general„ÄÇ

 it turns out the feedback based mechanisms that TCP uses tend to be good enough„ÄÇ

Rand Mer drop is another take on let's have the router actually make some changes and so it's a kind of cool algorithm and has seen applicability in the world„ÄÇ

 so let's go ahead and talk about it„ÄÇAll rightÔºå so red is random random early drop„ÄÇ

 it's a algorithm that's run at the router so that helps because the router actually knows its particular notification knows what's going on„ÄÇ

 knows what its state is„ÄÇWe're still going to kind of assumeÔºå in fact„ÄÇ

 we're going to for much of the discussionÔºå we're going to assume that the packets are carrying TCP segments and so that TCP will be able to react to things properly„ÄÇ

And we call this an active queuee management scheme„ÄÇ

 it's because the router itself is actually trying to manage what's going on with the congestion at particular cues as opposed to if you don't do something like this„ÄÇ

 the router is more passive right the router is just kind of like well I'll take packets you know fill up my cue when my queuee gets full I'll drop a few„ÄÇ

 you knowÔºå it's not actively making decisions to try to avoid that situation„ÄÇOkay„ÄÇ

 so it's going to randomly choose a packetÔºå thus the R part of it its acronym„ÄÇ

 it's going to try to do this„ÄÇBefore the congestion really gets bad„ÄÇ

 that's actually the key point here is that the router because of its knowledge is trying to make predictions before the end hosts have time to make the predictions before they have the data they need or have actually seen the congestion so we're trying to be early on it and when we see the congestion we're going to inform the center by dropping random early drop I should also technically every time today when I say drop a packet„ÄÇ

Or markup packet the two„ÄÇüò°ÔºåAs far as this algorithm or concerned are the same„ÄÇ

 the effects won't be the same if I drop a packet I'm actually throwing that data away„ÄÇ

 if I mark a packetÔºå then I am doing the kind of early congestion notification techniques right I'm setting a bit in the TCP options field and expecting the end hosts the receiving host to send that back to the sender„ÄÇ

As far as this algorithm is concernedÔºå either one would be fine„ÄÇ

And so we will use the term drop or mark independently through the rest of this conversation„ÄÇ

All rightÔºå so what are we trying to do here Well we're trying to„ÄÇWell„ÄÇ

 it turns out there's this thing called persistent queuing delay„ÄÇ

What will happen as you have a bunch of routers sending packets around is that you end up with„ÄÇ

 you got a couple packets„ÄÇIn the queue and the queue isn't necessarily getting into congetion realms„ÄÇ

 but maybe it's just kind of hanging out holding a bunch of stuff we call that a persistent delay yeah I always have you know eight packets in the queue kind of thing„ÄÇ

And in those casesÔºå it's nice to kind of shock the system a little bit get you know if you are if you have a persistent„ÄÇ

Delay„ÄÇThat means if you've already got always got eight packets in the queue„ÄÇ

 that means your incoming and outing averages are about the sameÔºå okayÔºå so„ÄÇ

Woulddn't it be nice to have those averages but with zero packets in the queue and so oftentimes we kind of shock the system to make that happen so that we can clear out our queue„ÄÇ

And get back to about the same situation and some casesÔºå that will mean that we're going to„ÄÇ

By being earlyÔºå we're going toÔºå yesÔºå we'll drop a few packets„ÄÇ

 but that's better than what would happen if we didn't if we don'tt„ÄÇ

Do anything early and we run into congestion that means a lot of packets are going to get dropped so let's go ahead and keep that from happening„ÄÇ

What we're actually doing hereÔºå if you think back many lectures I showed a picture of a curve and we said we all agreed that there was some knee point in our network that we wanted to control around we wanted to make sure the network load was you know at some point where we got a lot of throughput but had very little delay and so what we're trying to do is do the control anytime we get beyond that knee let's bring it back below that knee that knee point„ÄÇ

We're also avoiding a couple things that we've talked about one of these briefly„ÄÇ

 but we haven't talked about the idea of global synchronization„ÄÇ

Think about a whole bunch of TCP flows that are running effectively along the same path„ÄÇOkayÔºå well„ÄÇ

 what happens with TCP TCP increases congestion windowÔºå increases congestion window keeps you know„ÄÇ

 increases the flowÔºå increases the flow until there's a loss and when that loss happens„ÄÇ

 then you back off dramatically„ÄÇWellÔºå it turns out if you have a lot of flows going the same way„ÄÇ

 then you have a bunch of TCP senders that are all increasing their congestion window„ÄÇ

 all increasing their congestion window until some congestion happens and when that congestion happens„ÄÇ

 then basically everybody loses a packet„ÄÇAnd thereforeÔºå all of the TCP„ÄÇ

Senders will detect that there was some loss they will all drop their congestion windows in half or back to slow start or whatever they'll all react similarly„ÄÇ

They'll back offÔºå which is goodÔºå and that particular router will get out of the congestion situation„ÄÇ

But they're all kind of synchronizedÔºå so all of them are now backed off„ÄÇ

 so there's a bunch of bandwidth going to waste and then they all increase„ÄÇ

 all increase all increase together until they all cause a loss again„ÄÇ

We call this global synchronization okay and it leads to a lot of bandwidth being lost because everybody's backing off at the same time„ÄÇ

 it'd be nice to break those up so that you knowÔºå one was backing off„ÄÇ

And lowering the bandwidth and others were still able to make use of the bandwidth they had„ÄÇ

The other issue that a lot of times happens is a bias against bursty traffic„ÄÇ

 a lot of what we do tends to be very burstyÔºå that is we have periods where we don't send many packets and then we suddenly do something and that sends a lot of packets„ÄÇ

And then we go back into a scenario where we're not sending very few of our applications actually are steadily sending X number of packets every second„ÄÇ

If that's their average„ÄÇX packets per second very few applications will actually send x packets and then x packets and then x packets right a lot of them instead will send very low numbers very low numbers high number low number low number number right this is kind of how humans work with computers„ÄÇ

A lot of times the computer is waiting for us and when we do something„ÄÇ

Then all of a sudden it has to respond and it needs to respond by sending stuff to DNS servers and then to web servers and a bunch of requests to web servers and oh„ÄÇ

 I need a favorite con to go with that and I need the JavaScript file to go with that and we collect all that stuff„ÄÇ

And then the computer sits around rendering for the user waiting for the user to click another button„ÄÇ

And so it says bursty trafficÔºå we don't want the bursts to be seen as the congestion and then a lot of packets to be dropped from the same flow because the same flow„ÄÇ

 even though it's average amount of bandwidth is lowÔºå it's„ÄÇ

Banddth usage for a small period of time is very high„ÄÇ

 we don't want lots and lots of its packets to get dropped because of that„ÄÇ

So here's the algorithm for red„ÄÇEvery time a packet comes in„ÄÇ

 you're going to basically run this code All right first thing you're going to do is calculate the average Q size„ÄÇ

 I guess I should point out this is done after we've done the forwarding so we know which link the packet is going to get sent out on and so we're all this everything I talk about in terms of Q sizes and which what to do with each Q that's on a per output link basis„ÄÇ

And so every one of those 48 wires connected to my router„ÄÇ

Would have an instance of this algorithm basically running on that output„ÄÇ

So I have a packet that is supposed to be sent out a particular link„ÄÇ

 let's go ahead and look at what the Q sizes are for that link and calculate some average number„ÄÇ

And then we're going to decide is this average number small medium or largeÔºå basically„ÄÇ

 is it small enough that we don't care if the Q size is small„ÄÇ

 that means there's no congestion happeningÔºå then adding this extra packet to the Q is okay„ÄÇ

 it's not going to cause a problem„ÄÇIf the„ÄÇAverage Q size is big„ÄÇOkay„ÄÇ

 that means there is a problem and so what I wanted to do in that case is I want to drop that packet right I don't want to accept that packet into the queue and so I'm going to drop it or I'm going to mark depending on which way we're running this„ÄÇ

If it's in the middle though i'm going to well not too small and not too large what am I going to do I don't know we're going to basically roll some dice we're going to randomly decide should we drop this packet or not and so we're going to calculate some probability„ÄÇ

And we're going to choose a random number and if the random number is less than that probability then that packet lost and so we're going to drop it or market„ÄÇ

If its if the random number is above that probability„ÄÇ

 then we go ahead and add the packet to the queue„ÄÇ

![](img/f7ad1b05502545c34f0eae43c163cc5a_11.png)

OkayÔºå here's a kind of a graphical way of looking at it„ÄÇ

The packets coming in we choose we do the calculationÔºå what's the Q length„ÄÇ

 is it small medium or largeÔºå if it's small we're going to go ahead and put the queue in the packet put the packet into the queue„ÄÇ

If the average is bigÔºå there's not enough spaceÔºå we're going to drop the packet or mark the packet„ÄÇ

 but if it's in the middleÔºå we might go one way we might go the other way and to decide that we're going to choose a probability and do a random number and decide which way we go„ÄÇ

And so JakeÔºå you're rightÔºå every outgoing link has this picture on it„ÄÇ

ca it has its own separate queue right so the size of a Q you know I don't imagine something graphically imagine you're in the middle of of Wan Hall at some router right and you could send you have outgoing links that are going to Hammerlaag deporter to Baker you know to fine arts to UC it doesn't help if you know if you've got a bunch of packets going to the UC„ÄÇ

Then I don't care what the Q length is to the other directions„ÄÇ

So this is all happening per output out link„ÄÇOn our router„ÄÇ

And so that means that the Q lengths and computing are for the Q on that particular link and packet the Q I'm going to put the packet in„ÄÇ

Or drop the packet from that queue is on that particular link„ÄÇYeahÔºå very important point„ÄÇ

All right so in this picture there are a couple of fuzzy ideas we haven't talked about right there's i'm computing something i'm calculating something i'm deciding you know is my average too big or too small how do we handle all those well those are the details„ÄÇ



![](img/f7ad1b05502545c34f0eae43c163cc5a_13.png)

That come up in the paper„ÄÇFirst offÔºå average Q lengthÔºå right you can look at the Q or the router„ÄÇ

 can look at the queue right now and sayÔºå ohÔºå you know„ÄÇ

 there's 27 kilobytes worth of packets in the queue waiting to be sent„ÄÇ

Or whatever or there are 79 packets right it can it has the status„ÄÇ

 it can look at that cue and know what's the situation right now„ÄÇNow it turns out„ÄÇThat data is very„ÄÇ

 very burstyÔºå OkayÔºå and so„ÄÇWe want to kind of smooth that out and get an average queue length„ÄÇ

 not just an instantaneous queue length„ÄÇAnd so turns out Floyd uses this same technique„ÄÇ

 we've used it before„ÄÇ Do you remember where we used the exponential weighted moving averageÔºü

When did we talk about thatÔºüYeahÔºå veryÔºå very trueÔºå Luke„ÄÇ

Back when we were estimating round trip times„ÄÇAllÔºå we had values„ÄÇ

 we had these sample round trip times that were very burstyÔºå very all over the place„ÄÇ

 and so we wanted to smooth them out„ÄÇAnd so we went ahead and„ÄÇAnd took the value coming in Okay„ÄÇ

 so the Q length is the instantaneous valueÔºå I multiply it by some parameter„ÄÇ

 I then add it to the old average„ÄÇMultiply by  one minus that parameter„ÄÇ

 and that becomes our new average„ÄÇAnd so I'll do this calculationÔºå get an average Q length„ÄÇOkayÔºå and„ÄÇ

That is nice and smoothed out so that I'm not reacting to just kind of what you know oh look I just had four packets come in that happen to go out the same way that's a maybe a very short term event I may not want to react to that so quickly I want to react on if there is a steady increase in the number of packets on this particular outgoing link„ÄÇ



![](img/f7ad1b05502545c34f0eae43c163cc5a_15.png)

Here's the picture from the paper„ÄÇOkayÔºå and it's hard to see a little bit because there's a couple lines on here that are all drawn and black and very similar„ÄÇ

 Remember this is an 1993 paper we didn't have„ÄÇYou know color PDFs at the time so this is the Q length over some amount of time and you'll see this kind of up down bursty value„ÄÇ

 this one that's all over the place is the number of packets in the queue„ÄÇ

That's the instantaneous Q sizeÔºå though„ÄÇOkayÔºå and so it's all over the place„ÄÇ

There is another line buried here that is this one„ÄÇ This is the smoothed value„ÄÇ

 So if you take this bursty up downÔºå up downÔºå up down kind of thing and do the exponential weighted moving average„ÄÇ

 this is the moving average you seenÔºå you'll notice it takes a while after it's been a lot of low traffic„ÄÇ

 it takes a while for that that exponential value to get anywhere„ÄÇ

Okay even though the instantaneous value was you know was way highÔºå was over 30 packets at one point„ÄÇ

Exponential value hasn't grown„ÄÇSo fastÔºå so it's nice and it is a very nicely smooth value„ÄÇ

And then there's also these two dotted lines hereÔºå the horizontal lines those tell us what the thresholds are we'll talk about those in a minute that lets us know whether this average size is low or medium in this middle area or if it were up here it's a high„ÄÇ



![](img/f7ad1b05502545c34f0eae43c163cc5a_17.png)

High threshold„ÄÇOkay or high value above the threshold„ÄÇAll right„ÄÇ

I mentioned a minute ago thatre going to we're going to use the exponential weighted moving average because we'd like to smooth out these very bursty values„ÄÇ

Á≥ªËØ∂„ÄÇThat might seem weird right I'm actually talking about the advantages of having a router do congestion control because it knows the situation right now„ÄÇ

 so shouldn't we react to that congestion right nowÔºå shouldn't we be doing things„ÄÇ



![](img/f7ad1b05502545c34f0eae43c163cc5a_19.png)

LookÔºå I have a long queue length„ÄÇBut the exponential valueÔºå the moving average value is still low„ÄÇ

 stillÔºå it's like four packets in sizeÔºå whereas my instantaneous Q size is above 30„ÄÇ

That sounds like congestion to meÔºå shouldn't we react to that right nowÔºü



![](img/f7ad1b05502545c34f0eae43c163cc5a_21.png)

What do you guys thinkÔºüWhy do we smooth stuff outÔºüWhy did the paper say we smooth stuff outÔºüSure„ÄÇ

So againness you writeÔºå there's a burst thereÔºå rightÔºå that's the whole thing we're trying to do„ÄÇHis„ÄÇ

YeahÔºå is to manage this burstÔºå rightÔºüTo ignore a short term burst„ÄÇÂóØ„ÄÇYeahÔºå you got not very's precise„ÄÇ

 but we obviously were trying to avoid these outliersÔºå I guess why is my questionÔºü

Dion says the short term burst isn't going to„ÄÇResult may not result in a long term value„ÄÇ

 and that's true„ÄÇWe'd like it not to well why not react to that and right now kill some packets to make sure it doesn't„ÄÇ

The issue is one of the reaction time„ÄÇÂóØ„ÄÇIf I do anything at the router„ÄÇÂóØ„ÄÇ

It takes a hall for any sender to discover that„ÄÇIf I mark a packet„ÄÇ

 that marked packet has to go to a receiver and then travel back to the senderÔºå if I drop a packet„ÄÇ

 then I have to wait for a timer to go off before anybody discovers this„ÄÇ

And so nobody can react as fast as the router could anyway„ÄÇ

So if I was signaling right away that there's some congestion„ÄÇ

Nobody can do anything about it right now and it might be that that would be an overreaction right it might be that this is some transient thing which we actually saw in that graph right we see here yes„ÄÇ

 there is a lot of congestion it's bursty„ÄÇ

![](img/f7ad1b05502545c34f0eae43c163cc5a_23.png)

So it's transient and you in a very short period of time it's gone„ÄÇ

And so if we had done something to kill off a bunch of thisÔºå we would have created a bunch of loss„ÄÇ

And maybe it would have fixed itself anyway„ÄÇ

![](img/f7ad1b05502545c34f0eae43c163cc5a_25.png)

OkayÔºå so' we can't send the signals quick enough„ÄÇTo get anybody to instantaneously respond anyway„ÄÇ

 so let's kind of hang back and allow ourselves to get a feel that this actually is long term congestion before we do too much reaction„ÄÇ

All rightÔºå next question in our from our picture graph a minute ago was how do we set these thresholds„ÄÇ

 we have to make this decision that there's some small number and some large number and Floyd in that graph showed these two horizontal lines„ÄÇ

Those were these thresholds that split the entire average queue length into small medium and large chunks„ÄÇ

OkayAnd so there is a minimum thresholdÔºå a maximum threshold„ÄÇ

That specify kind of I don't care below this I don't I do care and we'll always react above this and in between then there's some you know groupsop things are starting to get bad kind of situation„ÄÇ

How do you set thoseÔºüYeahÔºå those you just choose right those are let's once again„ÄÇ

 we're going to kick it back into the network administrators world and say yeah„ÄÇ

 network administrators should somehow set these values„ÄÇOkay„ÄÇ

 I'm sure somebody at some point in the last two years has saidÔºå oh„ÄÇ

 let's go ahead and run a machine learning algorithm on the router to find out what those thresholds should be„ÄÇ

 but you knowÔºå okayÔºå you set some numbers„ÄÇAll rightÔºå next questionÔºå we have some probabilities„ÄÇ

 how do we calculate the probability of dropping the packetÔºå should it just be a coin flip 50Ôºå50„ÄÇ

It could be„ÄÇthat's a random decisionÔºå but if you're doing that„ÄÇ

 then you're basically saying that anythingÔºå any situation where you're just barely into the middle territory is as bad as when you're just next to the the max threshold„ÄÇ

 you're basically saying everything in that middle range should be handled the same way„ÄÇ

And so the paper wants to be a little bit more informed and come up with a better idea„ÄÇ

The first step is to kind of look at what your average is compared to your thresholds and so this fraction here that we're calculating is is basically that it's determining what portion of the distance across the threshold so if you think of the the distance„ÄÇ

The number of packets from the minimum to the maximum„ÄÇ

I mean am I doing this the right way for you guys my pictures back from the minimum to the maximum value there's some distance okay and what we're going to do is we're going to say how far is the average away from the minimum compared to the entire distance that's what that fraction is„ÄÇ

OkayÔºå am I close to the minimum or am I close to the maximumÔºü

And we'll get a value between zero and one„ÄÇYou know in that range from minimum to maximum„ÄÇ

I'm going to then multiply that by well another parameter something called the max probability that we said okay just imagine oh we want the maximum probability to drop a packet to be 20% or something like that and so this probability sub B will be some value between zero and 20„ÄÇ

Depending upon how close the average is to either of those two thresholds„ÄÇ

The other thing Floyd didn't want to do was didn't want to drop a bunch in a row„ÄÇ

He's thinking that if„ÄÇThere's a bunch of packets in that were you know coming in one after another„ÄÇ

 there's a good possibility they're all from the same flow and so let's not drop„ÄÇ

Let's not choose to drop a bunch of those in a row„ÄÇ

 and so we're going to go ahead and take this probability we just calculated„ÄÇ

And and use this fraction to figure out if there are how long it's been since since we've job packets and so as count count is a number that is the„ÄÇ

Basically keeping track of how long it's been since you drop a packet and so count will you know be zero than one then two then three right as you go as you don't drop don't drop don't drop a packet and so that means PA will start low and get a little bit higher and then that's the actual drop probability you're going to roll some dice against you're going to decide oh you know this is 12%„ÄÇ

Let's come up with a random numberÔºå if it's less than 0„ÄÇ12Ôºå then we'll drop the packet„ÄÇ



![](img/f7ad1b05502545c34f0eae43c163cc5a_27.png)

I've tried to draw this graph of the drop probability against the Q length here„ÄÇ

And so the way this worksÔºå if the Q lengthÔºå if I'm sorry„ÄÇ

 if the average queue length is less than your minimum threshold„ÄÇ

 you're not going to drop the packet and so that means that there's a zero% chance of dropping the packet„ÄÇ

If the„ÄÇQ length is greater than the maximum than you're always going to drop the packet„ÄÇ

 so that has a probability of one„ÄÇYou're going to drop the packet„ÄÇ



![](img/f7ad1b05502545c34f0eae43c163cc5a_29.png)

The if you look at these two piecesÔºå this PB portion right„ÄÇ

 that's the portion that goes from zero at min threshold up to some probability set parameter called max P at the max threshold„ÄÇ

 and so that's this line„ÄÇ

![](img/f7ad1b05502545c34f0eae43c163cc5a_31.png)

hi will effectively just increase if you haven't dropped anything in a while„ÄÇOkay„ÄÇ

 so maybe this is a little dramatic„ÄÇBut„ÄÇBut this shows that if you know„ÄÇIfIf the Q length is bigger„ÄÇ

 there's a higher probabilityÔºå you're going to drop it and if it's been a while since you dropped one„ÄÇ

 there's a higher probabilityÔºå you're going to drop this particular packet„ÄÇ



![](img/f7ad1b05502545c34f0eae43c163cc5a_33.png)

And then you roll some dice and decide whether it goÔºå whether you drop that packet or not„ÄÇOkay„ÄÇ

 so it makes sense that everybody kind of understand the process that's going to happen here what's what's going on with each packet coming in„ÄÇ



![](img/f7ad1b05502545c34f0eae43c163cc5a_35.png)

![](img/f7ad1b05502545c34f0eae43c163cc5a_36.png)

All rightÔºå so what's this do Certainly this is different from droptail right you're not dropping the packets when they come in and you don't have any space for them instead you're early dropping them you're dropping them because your Q lengths„ÄÇ

Are bad„ÄÇOkayÔºå it is also red is not biased against the bursty connection you're randomly dropping some stuff and so just because you are a bursty flow and you happened to throw a bunch of packets out it's„ÄÇ

Unlikely that all of your packets are going to get dropped or that many of your packets in your burst are going to drop okay„ÄÇ

 it also drops early to try to avoid this global synchronization problem„ÄÇYour„ÄÇNow if there are many„ÄÇ

 many TCP flows going through this router and out this particular link„ÄÇ

 you're just choosing some random packets and so they will distribute themselves across the different flows of packets going through that particular link and so the senders will get the signal that they need to back off but they will back off at different periods of time and so you don't have this everybody sees the congestion everybody goes back into slow start„ÄÇ

Nobody's sending in packets and the router then is wasting bandwidth„ÄÇIs it fairÔºüWell„ÄÇ

 if you're sending more packetsÔºå that means there's more dice being rolled for you and that means there's more chance you're going to be the one losing the packet„ÄÇ

 if you're only sending an occasional packetÔºå then your chance of that being lost is well it's the same chance effectively as any packet but because the decision is being made on a packet by packet basis„ÄÇ

 those who send more packets will seem more loss„ÄÇNow certainly we're not doing any kind of TCP police or flow management to you know„ÄÇ

To see if somebody's doing something badÔºå it's more of aÔºå well„ÄÇ

 if you're sending a lot more if you're if you're not backing off if you're sending„ÄÇÂëÉ„ÄÇ

Too many packetsÔºå then you're probably going to have more of them dropped„ÄÇ

 but there's nothing here that's going to be stopping you or choosing you as the bad flow„ÄÇ

 although you could implement some mechanisms to detect that„ÄÇIt does„ÄÇ

Require that the end hosts are going to react to this right if you're going to mark a packet or if you're going to drop a packet„ÄÇ

That's you know we're trying to send a signal to a sender and the sender could be ignoring that Okay now part of that„ÄÇ

I'm not sure they would ignore it just because it was red„ÄÇ

 but you might ignore it because you're sending a bunch of UDP packets and not TCP for instance„ÄÇ

 and in '93 it was pretty rare to send a whole bunch of UDP packets one after another and nowadays that might not be so rare in fact„ÄÇ

 we're sending a bunch of UDP packets back and forth to each other right now over Zoom„ÄÇ

There is some scaling issues that were„ÄÇDetected when Floyd did the work he did„ÄÇ

 he did it on a fairly small networkÔºå actually did on a simulation of a small network and some people discovered when they actually implemented red at scale„ÄÇ

 the updates to the average Q length just weren't going fast enough and so I forget what SP is but there's another variant that is more oftenly implemented in large„ÄÇ

Large networksÔºå' same idea as readÔºå it's just changed some of the details about how the probabilities are chosen„ÄÇ

It is pretty common in most mono routersÔºå if you buy a Cisco router„ÄÇ

 it is a flip of the switch to use red if you wanted toÔºå and there are other versions of it out„ÄÇ

 of course anything that's been working since 1993 has had plenty of other research done on it since 1993 and so there are weighted reds or adaptive red versions as well„ÄÇ

 that you can find out there„ÄÇAll rightÔºå so that brings us„ÄÇ

Well does a couple things first off it brings us to the end of this look at how a router can make some decisions and be involved in the congestion control process„ÄÇ

Even though it's and it also showed you some of the complications there because you have to somehow communicate that there is congestion to the end hosts who can actually be the ones doing the real control that can actually stop sending stuff„ÄÇ

This lesson also brings us to the end of our understanding of the network layer on Thursday„ÄÇ

 we're going to dive down into the next layerÔºå we're going to look at the link layer and start looking at how Ethernet„ÄÇ

OkayÔºå so if there are no further questionsÔºå bid you all adoÔºå have a great day„ÄÇByeÊãú„ÄÇ



![](img/f7ad1b05502545c34f0eae43c163cc5a_38.png)

Oh Ganesh did not mention that today but I should have I will not be holding office hours today because I'm going to be grading something for a different class so if you want to talk to me send me email we can work that out„ÄÇ

StephaosÔºå I will get La3 out in the next couple of days„ÄÇIt's a bunch of stuff about the link layer„ÄÇ

 so in theory haven haven't taught you enough to do lab three anyway„ÄÇ

 but it will' get out there anyway„ÄÇÂóØ„ÄÇOhÔºå Dan waiteded fairqueuing„ÄÇDrawbacks„ÄÇWellI mean„ÄÇ

 it's an algorithm that does„ÄÇIt does what it's supposed to do„ÄÇ

 and so the issue is whether it fits the need you want„ÄÇ

And it does have that same problem of kind of how do you set the priorities„ÄÇ

 how do you decide you knowÔºå which flow is the one that should have a different weight than other flows„ÄÇ

But there's no„ÄÇThere's no huge performance gain performance problem or anything like that it's a fairly simple thing that's being done on packet by packet„ÄÇ

 so there isÔºå as you mentionedÔºå some cost to implementation in terms of know a little bit extra code making these choices„ÄÇ

 but I don't think there's any hugeÔºå huge problem with huge cost„ÄÇÂóØ„ÄÇYeah„ÄÇGanessh„ÄÇ

 I don't think I agree that the computation is pretty heavy„ÄÇ

You have basically what you're doing is you you are doing one„ÄÇOn a per packet basis„ÄÇ

 one update to the Q average length so that's a one multiply by beta add multiply by one minus beta now that can be pretty simple if you've chosen beta to be a quarter or an eighth some nice power two so you're not actually multiplying things„ÄÇ

And then you're you know a couple lines of code to calculate a probability and choose a random number„ÄÇ

I wouldn't say it's heavyÔºå I don't think it's an overwhelming amount of code to run„ÄÇSo„ÄÇ

 I don't think„ÄÇYeah„ÄÇI wouldn't see it as a bad thing from that perspective„ÄÇ

Certainly not on this on the„ÄÇScale of some of the other mechanisms„ÄÇ

 so we talked about sending quench messagesÔºå for instance early on you know routers could send a message back to the source saying please stop that's a heavy mechanism right because you actually have to create a new message and go ahead and send it back„ÄÇ

ÂóØ„ÄÇSo I'm not sure I agree that it is a„ÄÇA heavy mechanism„ÄÇSure„ÄÇI offhand„ÄÇ

 I don't recall the comments from the paper because it's been a year since I read it„ÄÇ

 sorry didn't read it last night„ÄÇAnd so I don't know the implementation status of those„ÄÇ‰Ω†Â•Ω„ÄÇ

Hey hey rightÔºüI've gone through„ÄÇ

![](img/f7ad1b05502545c34f0eae43c163cc5a_40.png)