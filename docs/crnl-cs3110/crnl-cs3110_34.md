# 讲座 25：摊销分析

哈希表具有*O*(1)的期望性能（用于查找和插入）的断言基于这样一个假设：存储在表中的元素数量与桶的数量相当。如果哈希表的元素比桶多得多，则存储在每个桶中的元素数量将变大。例如，如果桶的数量是常数，并且有*O*(*n*)个元素，则查找时间是*O*(*n*)而不是*O*(1)。

这个问题的解决方法是当表中元素的数量与表的大小相比变得太大时，增加表的大小。如果我们将<def>负载因子</def>定义为元素数与桶数的比率，当负载因子超过某个小常数（通常对于链接为 2，对于线性探查为 0.75）时，我们会分配一个新表，通常是旧表大小的两倍，并将所有元素重新哈希到新表中。这个操作不是恒定时间的，而是与表增长时的元素数量成线性关系。

调整大小操作的线性运行时间并不像听起来的那么严重（尽管对于某些实时计算系统可能会有问题）。如果表每次需要时都增加两倍大小，那么调整大小操作的频率将指数级下降。因此，将*n*个元素插入空数组仅需要*O*(*n*)的时间，包括调整大小的成本。我们说插入操作具有*O*(1) <def>摊销运行时间</def>，因为插入元素所需的时间*平均*为*O*(1)，即使有些元素会触发对哈希表的所有元素进行长时间的重新哈希。

数组大小以几何级增长（倍增）至关重要。固定增量增长数组（例如，每次增加 100 个元素）可能很诱人，但这会导致渐近线性而不是恒定的摊销运行时间。

现在我们转向对摊销分析的更详细描述。

## 摊销分析

摊销分析是对一系列操作的最坏情况分析，旨在得到比单独分析序列中的每个操作更紧密的整体或平均每个操作的成本界限。例如，当我们考虑前面学期对不相交集数据抽象的并集和查找操作时，我们能够将单个操作的运行时间限制在*O*(log *n*)。然而，对于一个包含*n*个操作的序列，可以获得比*O*(*n* log *n*)更紧密的界限（尽管该分析更适用于 4820 而不是这门课程）。在这里，我们将考虑上述哈希表问题的简化版本，并展示*n*个插入操作的序列具有总时间*O*(*n*)。

摊销分析使用了三种主要技术：

+   <def>聚合方法</def>，即分析一系列操作的总运行时间。

+   <def>记账</def>（或<def>银行家</def>）方法，我们对廉价操作施加额外费用，然后用它来支付后续昂贵的操作。

+   <def>潜力</def>（或<def>物理学家</def>）方法，我们通过导出描述我们每一步可以额外完成的工作量的*潜力函数*来进行。该潜力函数随着每一次操作的进行而增加或减少，但不能为负。

考虑一个可以存储任意数量整数的可扩展数组，如 Java 中的`ArrayList`或`Vector`。这些是以普通（非可扩展）数组来实现的。每次`add`操作都在之前插入的所有元素之后插入一个新元素。如果没有剩余的空单元，就会分配一个大小加倍的新数组，并且所有旧数组中的数据都复制到新数组的相应条目中。例如，考虑以下一系列插入，从长度为 1 的数组开始：

```
           +--+
Insert 11  |11|
           +--+
           +--+--+
Insert 12  |11|12|
           +--+--+
           +--+--+--+--+
Insert 13  |11|12|13|  |
           +--+--+--+--+
           +--+--+--+--+
Insert 14  |11|12|13|14|
           +--+--+--+--+
           +--+--+--+--+--+--+--+--+
Insert 15  |11|12|13|14|15|  |  |  |
           +--+--+--+--+--+--+--+--+

```

表在第二、第三和第五步中加倍。由于在最坏情况下每次插入都需要*O*(*n*)的时间，所以简单的分析会得出*n*次插入的*O*(*n*²)时间界限。但情况并非如此糟糕。让我们使用三种方法分析一个*n*次操作的序列。

### 累积法

让*c[i]*表示第*i*次插入的成本：

```
*c[i]* = *i* if *i−1* is a power of 2
     1 otherwise

```

让我们考虑序列中前几次插入的表大小*s[i]*和成本*c[i]*：

```
*i*   1  2  3  4  5  6  7  8  9 10
*s[i]*  1  2  4  4  8  8  8  8 16 16
*c[i]*  1  2  3  1  5  1  1  1  9  1

```

或者我们可以看到*c[i]*=1+*d[i]*，其中*d[i]*是表大小加倍的成本。也就是说

```
*d[i]* = *i−1* if *i−1* is a power of 2
     0 otherwise

```

然后在整个序列上求和，所有的 1 都加起来得到*O*(*n*)，而所有的*d[i]*也加起来得到*O*(*n*)。也就是说，

```
Σ*[1≤i≤n] c[i]*   ≤   n + Σ*[0≤j≤m] 2^(j−1)* 

```

其中*m* = log(*n* − 1)。不等式右边的两个项都是*O*(*n*)，因此*n*次插入的总运行时间为*O*(*n*)。

### 记账（银行家）法

累积法直接寻求对一系列操作的总体运行时间的界限。相比之下，记账法试图找到对每个个别操作收取的额外时间单位的*支付*，使得这些支付的总和是总实际成本的上界。直观地说，我们可以想象维护一个银行账户。低成本操作的收费略高于其真实成本，并且剩余部分存入银行账户以备将来使用。然后，可以将高成本操作的收费低于其真实成本，并且透支是由银行账户中的储蓄支付的。通过这种方式，我们将高成本操作的成本分摊到整个序列中。对每个操作的收费必须设置得足够大，以确保银行账户的余额始终保持正数，但又不能太大，以至于没有一个操作的收费明显高于其实际成本。

我们强调，对操作额外计费并不意味着该操作实际花费了那么多时间。这只是一种使分析更容易的记账方法。

如果我们让*c'[i]*是第*i*个操作的费用，*c[i]*是真实的费用，那么我们希望

Σ*[1≤i≤n] c[i]* ≤ Σ*[1≤i≤n] c'[i]*

对于所有的*n*，这意味着*n*个操作序列的<def>分摊时间</def> Σ*[1≤i≤n] c'[i]* 是对真实时间 Σ*[1≤i≤n] c[i]* 的一个上界。

回到可扩展数组的示例。假设插入一个元素花费 1 个单位，当表扩大时移动它花费 1 个单位。显然，每次插入 1 个单位的费用不够，因为没有剩余的费用来支付移动费用。每次插入 2 个单位的费用还不够，但是每次插入 3 个单位似乎足够：

```
*i*   1  2  3  4  5  6  7  8  9 10
*s[i]*  1  2  4  4  8  8  8  8 16 16
*c[i]*  1  2  3  1  5  1  1  1  9  1
*c'[i]* 3  3  3  3  3  3  3  3  3  3
*b[i]*  2  3  3  5  3  5  7  9  3  5

```

其中*b[i]*是第*i*次插入后的余额。

实际上，这在一般情况下就足够了。让*m*指代第*m*个插入的元素。对*m*收取的三个单位被用于以下目的：

+   一个单位用于立即将*m*插入表中。

+   一个单位用于在插入*m*后第一次扩大表时移动*m*。

+   一个单位捐赠给元素*m − 2^k*，其中*2^k*是不超过*m*的最大 2 的幂，并且用于在插入*m*后第一次扩大表时移动该元素。

现在，每当一个元素被移动时，移动费用已经付清。第一次移动元素时，它是由插入时向其收取的一个自己的时间单位付费的；所有后续的移动都是由后来插入的元素的捐赠支付的。

实际上，我们可以做得更好一点，只收取第一次插入的费用为 1，然后对每次插入后的插入收取 3 个单位的费用，因为对于第一次插入，没有要复制的元素。这将在第一次插入后产生零余额，然后在此后产生正余额。

### 势（物理学家）方法

在上面，我们看到了处理可扩展数组的聚合方法和银行家方法。现在让我们来看看物理学家的方法。

假设我们可以在数据结构的状态上定义一个<def>势函数</def>Φ（读作“Phi”），具有以下特性：

+   Φ(*h*[0]) = 0，其中*h*[0]是数据结构的初始状态。

+   对于计算过程中出现的数据结构的所有状态*h*[*t*]，都有Φ(*h*[*t*]) ≥ 0。

直观地，势函数将跟踪计算中任意点的预充值时间。它衡量了有多少积累的时间可用于支付昂贵的操作。这类似于银行家法中的银行余额。但有趣的是，它仅取决于数据结构的当前状态，而不考虑使其处于该状态的计算历史。

我们将一个操作的*分摊时间*定义为

> *c* + Φ(*h*') − Φ(*h*),

其中*c*是操作的实际成本，*h*和*h*'是操作之前和之后的数据结构状态。因此，摊销时间是实际时间加上潜力变化。理想情况下，Φ应该被定义为每个操作的摊销时间很小。因此，潜力变化对于低成本操作应该是正的，对于高成本操作应该是负的。

现在考虑一系列*n*个操作，实际花费时间为*c*[0]，*c*[1]，*c*[2]，...，*c*[*n*−1]，并产生数据结构*h*[1]，*h*[2]，...，*h*[*n*]，从*h*[0]开始。总摊销时间是单个摊销时间的总和：

> (*c*[0] + Φ(*h*[1]) − Φ(*h*[0])) + (*c*[1] + Φ(*h*[2]) − Φ(*h*[1])) + ... + (*c*[*n*−1] + Φ(*h*[*n*]) − Φ(*h*[*n*−1]))
> 
> = *c*[0] + *c*[1] + ... + *c*[*n*−1] + Φ(*h*[*n*]) − Φ(*h*[0])
> 
> = *c*[0] + *c*[1] + ... + *c*[*n*−1] + Φ(*h*[*n*])。

因此，一系列操作的摊销时间高估了Φ(*h*[*n*])，假设Φ始终为正。因此，总摊销时间始终是实际时间的上界。

对于动态可调整大小的数组，通过加倍调整大小，我们可以使用潜力函数

> Φ(*h*) = 2*n* − *m*，

其中*n*是当前元素的数量，*m*是数组的当前长度。如果我们从长度为 0 的数组开始，并在添加第一个元素时分配长度为 1 的数组，然后每当需要更多空间时都将数组大小加倍，我们有Φ(*h*[0]) = 0，并且Φ(*h*[*t*]) ≥ 0 对于所有*t*都成立。后者不等式成立，因为元素的数量始终至少是数组大小的一半。

现在我们想要证明添加元素的摊销时间是常数时间。有两种情况。

+   如果*n* < *m*，那么实际成本是 1，*n*增加 1，*m*不变。然后，潜力增加 2，因此摊销时间为 1 + 2 = 3。

+   如果*n* = *m*，那么数组加倍，因此实际时间是*n* + 1。但是潜力从*n*下降到 2，所以摊销时间是*n* + 1 + (2 − *n*) = 3。

在两种情况下，摊销时间都是 O(1)。

用物理学家的方法进行摊销分析的关键是定义正确的潜力函数。潜力函数需要保存足够的时间，在需要时使用。但是它不能保存太多的时间，以致导致当前操作的摊销时间过高。
