# 亚马逊的 Dynamo 键值存储

# 6.824 2015 年第 18 讲：亚马逊的 Dynamo 键值存储

**注意：** 这些讲座笔记是从 2015 年春季 6.824 [课程网站](http://nil.csail.mit.edu/6.824/2015/schedule.html) 上发布的笔记中稍作修改的。

## Dynamo

+   最终一致性

+   比 PNUTS 或 Spanner 要不一致得多

+   像 Cassandra 这样的成功开源项目是基于 Dynamo 的想法构建的

## 设计

+   非常担心他们的服务水平协议（SLA）

    +   内部 SLA，比如 web 服务器和存储系统之间

+   担心*最坏情况*的性能，而不是平均性能

+   他们希望延迟的 99.9th 百分位数 `< 300ms`

    +   这个要求如何在设计中体现还不是很清楚

    +   为了满足这一点做了什么选择？

+   应该处理不断发生的故障

    +   整个数据中心离线

+   他们需要系统始终可写

    +   `=>` 没有单一的主节点

图表：

```
Datacenter

    Frontend            Dynamo server
    server
                        Dynamo server
    Frontend
    server              Dynamo server

                        Dynamo server 
```

+   猜测：亚马逊有相当多的数据中心，没有一个是主要的或备份的，那么即使一个数据中心宕机，你的系统只会有一小部分宕机

    +   相比于在每个地方都复制每个记录，只在 2 或 3 个数据中心复制它要自然得多

+   Dynamo 的设计实际上并不是以数据中心为��向的

+   与 PNUTS 的不同之处在于他们的设计中没有关于局部性的内容

    +   他们不担心这个问题：不会为了靠近每个客户端而复制数据

+   他们需要广域网的运行非常良好

## 详情

+   始终可写 `=>` 没有单一的主节点 `=>` 不同的服务器上有不同的 puts `=>` 冲突的更新

+   puts 应该去哪里，gets 应该去哪里，以便它们可能看到 puts 写入的数据

### 一致性哈希

+   你对键进行哈希，它告诉你应该从哪个服务器获取/存储它

+   哈希输出空间是一个环/圆

+   每个键的哈希值都是圆环上的一个点

+   每个节点的哈希值也是一个点

+   `=>` 一个键将在节点之间或在圆环上的一个节点之上

+   在圆环上最接近键的节点（顺时针）是键的*协调者*

+   如果键被复制 `N` 次，那么键后面的 `N` 个后继节点（顺时针）存储该键

+   即使节点 ID 随机选择，一致性哈希并不能均匀地将键分布在节点之间

    +   一个节点上的键的数量与该节点与其前驱节点之间的间隙成比例

    +   间隙的分布相当宽

+   为了弥补这一点，使用虚拟节点

    +   每个物理节点由一定数量的虚拟节点组成，与物理节点的性能/容量成比例

优先级列表：

+   假设你有节点 A、B、C、D、E、F 和键 `k`，它的哈希值在节点 A 之前

+   这个键 `k` 应该在 A、B 和 C 存储 3 个副本，如果 `N = 3`

+   请求可能发送到第一个节点 A，该节点可能已经宕机

    +   或者请求可能发送到第一个节点 A，该节点会尝试在节点 B 和 C 上复制它，这两个节点可能已经宕机

    +   `=>` 这个第一个节点会在节点 D 和 E 上复制

    +   `=>` 可能有超过 `N` 个节点拥有数据

    +   `=>` 记住所有这些节点在一个*优先级列表*中

+   请求 `k` 发送到优先级列表中的第一个节点

+   该节点作为请求的协调员，并在所有其他节点上读取/写入密钥

+   懒惰的法定人数，

    +   `N`协调员发送请求的节点数

    +   `R`协调员等待获取数据返回的节点数

    +   `W`协调员等待写入数据的节点数

+   如果没有故障，协调员会像主节点一样运作

+   如果出现故障，那么懒惰的法定人数确保数据被持久化，但可能会产生不一致性

+   问题：因为没有真正的法定人数，获取可能会错过最近的放置

+   你可以让节点 A、B、C 存储一些放置在陈旧数据上的放置，节点 D、E、F 存储另一些放置在数据上

    +   D、E、F 中的协调员知道数据位置不对，并存储一个标志以指示应该将其传递给 A、B、C（*hinted hand-off*）

## 冲突

+   论文中的图 3

+   当存在 2 个冲突版本时，客户端代码必须能够调和它们

+   dynamo 使用版本向量，就像 Ficus 一样

    +   `[a: 1] -> [a: 1, b: 3]`

    +   `[a: 1] -> [a: 1, c: 3]`

    +   `[a:1, b:3, c: 0]`和`[a:1, b:0, c:3]`冲突

+   Dynamo 比 Bayou 弱

+   两者都有解决冲突版本的方法

    +   在 dynamo 中，我们只有两个冲突的数据片段，但我们没有应用于状态的操作（比如从购物车中移除/添加某物）

    +   Bayou 有操作日志

+   PNUTS 支持原子操作，比如`test-and-set-write`操作

    +   Dynamo 中没有这样的东西

    +   Dynamo 中唯一的方法是能够合并两个冲突的版本

## 性能

+   *关于版本向量始终要问的问题：*当版本向量变得太大时会发生什么？

    +   删除了长时间修改过的节点的条目

        +   `v1 = [a:1, b:7] -> v1' = [b:7]`

        +   会出什么问题？如果`[b:7]`更新为`v2 = [b:8]`，那么`v2`将与`v1`冲突，即使它是直接从`v1`派生出来的，所以应用程序会获得一些*错误*合并

+   他们喜欢能够调整`N、R、W`以获得不同的权衡

    +   标准的`3,2,2`

    +   `3, 3, 1 ->`快速写入但持久性不强，读取很少

    +   `3, 1, 3 ->`写入速度慢，但读取速度相当快

+   平均延迟为 5-10ms，远小于 PNUTS 或 memcached

    +   相对于数据中心之间的光速来说太小了

    +   但数据中心在哪里以及工作负载是什么并不清楚

# 6.824 2015 原始笔记

```
Dynamo: Amazon's Highly Available Key-value Store
DeCandia et al, SOSP 2007

Why are we reading this paper?
  Database, eventually consistent, write any replica
    Like Ficus -- but a database! A surprising design.
  A real system: used for e.g. shopping cart at Amazon
  More available than PNUTS, Spanner, &c
  Less consistent than PNUTS, Spanner, &c
  Influential design; inspired e.g. Cassandra
  2007: before PNUTS, before Spanner

Their Obsessions
  SLA, e.g. 99.9th percentile of delay < 300 ms
  constant failures
  "data centers being destroyed by tornadoes"
  "always writeable"

Big picture
  [lots of data centers, Dynamo nodes]
  each item replicated at a few random nodes, by key hash

Why replicas at just a few sites? Why not replica at every site?
  with two data centers, site failure takes down 1/2 of nodes
    so need to be careful that *everything* replicated at *both* sites
  with 10 data centers, site failure affects small fraction of nodes
    so just need copies at a few sites

Consequences of mostly remote access (since no guaranteed local copy)
  most puts/gets may involve WAN traffic -- high delays
    maybe distinct Dynamo instances with limited geographical scope?
    paper quotes low average delays in graphs but does not explain
  more vulnerable to network failure than PNUTS
    again since no local copy

Consequences of "always writeable"
  always writeable => no master! must be able to write locally.
  always writeable + failures = conflicting versions

Idea #1: eventual consistency
  accept writes at any replica
  allow divergent replicas
  allow reads to see stale or conflicting data
  resolve multiple versions when failures go away
    latest version if no conflicting updates
    if conflicts, reader must merge and then write
  like Bayou and Ficus -- but in a DB

Unhappy consequences of eventual consistency
  May be no unique "latest version"
  Read can yield multiple conflicting versions
  Application must merge and resolve conflicts
  No atomic operations (e.g. no PNUTS test-and-set-write)

Idea #2: sloppy quorum
  try to get consistency benefits of single master if no failures
    but allows progress even if coordinator fails, which PNUTS does not
  when no failures, send reads/writes through single node
    the coordinator
    causes reads to see writes in the usual case
  but don't insist! allow reads/writes to any replica if failures

Where to place data -- consistent hashing
  [ring, and physical view of servers]
  node ID = random
  key ID = hash(key)
  coordinator: successor of key
    clients send puts/gets to coordinator
  replicas at successors -- "preference list"
  coordinator forwards puts (and gets...) to nodes on preference list

Why consistent hashing?
  Pro
    naturally somewhat balanced
    decentralized -- both lookup and join/leave
  Con (section 6.2)
    not really balanced (why not?), need virtual nodes
    hard to control placement (balancing popular keys, spread over sites)
    join/leave changes partition, requires data to shift

Failures
  Tension: temporary or permanent failure?
    node unreachable -- what to do?
    if temporary, store new puts elsewhere until node is available
    if permanent, need to make new replica of all content
  Dynamo itself treats all failures as temporary

Temporary failure handling: quorum
  goal: do not block waiting for unreachable nodes
  goal: put should always succeed
  goal: get should have high prob of seeing most recent put(s)
  quorum: R + W > N
    never wait for all N
    but R and W will overlap
    cuts tail off delay distribution and tolerates some failures
  N is first N *reachable* nodes in preference list
    each node pings successors to keep rough estimate of up/down
    "sloppy" quorum, since nodes may disagree on reachable
  sloppy quorum means R/W overlap *not guaranteed*

coordinator handling of put/get:
  sends put/get to first N reachable nodes, in parallel
  put: waits for W replies
  get: waits for R replies
  if failures aren't too crazy, get will see all recent put versions

When might this quorum scheme *not* provide R/W intersection?

What if a put() leaves data far down the ring?
  after failures repaired, new data is beyond N?
  that server remembers a "hint" about where data really belongs
  forwards once real home is reachable
  also -- periodic "merkle tree" sync of key range

How can multiple versions arise?
  Maybe a node missed the latest write due to network problem
  So it has old data, should be superseded by newer put()s
  get() consults R, will likely see newer version as well as old

How can *conflicting* versions arise?
  N=3 R=2 W=2
  shopping cart, starts out empty ""
  preference list n1, n2, n3, n4
  client 1 wants to add item X
    get(cart) from n1, n2, yields ""
    n1 and n2 fail
    put(cart, "X") goes to n3, n4
  client 2 wants to delete X
    get(cart) from n3, n4, yields "X"
    put(cart, "") to n3, n4
  n1, n2 revive
  client 3 wants to add Y
    get(cart) from n1, n2 yields ""
    put(cart, "Y") to n1, n2
  client 3 wants to display cart
    get(cart) from n1, n3 yields two values!
      "X" and "Y"
      neither supersedes the other -- the put()s conflicted

How should clients resolve conflicts on read?
  Depends on the application
  Shopping basket: merge by taking union?
    Would un-delete item X
    Weaker than Bayou (which gets deletion right), but simpler
  Some apps probably can use latest wall-clock time
    E.g. if I'm updating my password
    Simpler for apps than merging
  Write the merged result back to Dynamo

How to detect whether two versions conflict?
  As opposed to a newer version superseding an older one
  If they are not bit-wise identical, must client always merge+write?
  We have seen this problem before...

Version vectors
  Example tree of versions:
    [a:1]
           [a:1,b:2]
    VVs indicate v1 supersedes v2
    Dynamo nodes automatically drop [a:1] in favor of [a:1,b:2]
  Example:
    [a:1]
           [a:1,b:2]
    [a:2]
    Client must merge

get(k) may return multiple versions, along with "context"
  and put(k, v, context)
  put context tells coordinator which versions this put supersedes/merges

Won't the VVs get big?
  Yes, but slowly, since key mostly served from same N nodes
  Dynamo deletes least-recently-updated entry if VV has > 10 elements

Impact of deleting a VV entry?
  won't realize one version subsumes another, will merge when not needed:
    put@b: [b:4]
    put@a: [a:3, b:4]
    forget b:4: [a:3]
    now, if you sync w/ [b:4], looks like a merge is required
  forgetting the oldest is clever
    since that's the element most likely to be present in other branches
    so if it's missing, forces a merge
    forgetting *newest* would erase evidence of recent difference

Is client merge of conflicting versions always possible?
  Suppose we're keeping a counter, x
  x starts out 0
  incremented twice
  but failures prevent clients from seeing each others' writes
  After heal, client sees two versions, both x=1
  What's the correct merge result?
  Can the client figure it out?

What if two clients concurrently write w/o failure?
  e.g. two clients add diff items to same cart at same time
  Each does get-modify-put
  They both see the same initial version
  And they both send put() to same coordinator
  Will coordinator create two versions with conflicting VVs?
    We want that outcome, otherwise one was thrown away
    Paper doesn't say, but coordinator could detect problem via put() context

Permanent server failures / additions?
  Admin manually modifies the list of servers
  System shuffles data around -- this takes a long time!

The Question:
  It takes a while for notice of added/deleted server to become known
    to all other servers. Does this cause trouble?
  Deleted server might get put()s meant for its replacement.
  Deleted server might receive get()s after missing some put()s.
  Added server might miss some put()s b/c not known to coordinator.
  Added server might serve get()s before fully initialized.
  Dynamo probably will do the right thing:
    Quorum likely causes get() to see fresh data as well as stale.
    Replica sync (4.7) will fix missed get()s.

Is the design inherently low delay?
  No: client may be forced to contact distant coordinator
  No: some of the R/W nodes may be distant, coordinator must wait

What parts of design are likely to help limit 99.9th pctile delay?
  This is a question about variance, not mean
  Bad news: waiting for multiple servers takes *max* of delays, not e.g. avg
  Good news: Dynamo only waits for W or R out of N
    cuts off tail of delay distribution
    e.g. if nodes have 1% chance of being busy with something else
    or if a few nodes are broken, network overloaded, &c

No real Eval section, only Experience

How does Amazon use Dynamo?
  shopping cart (merge)
  session info (maybe Recently Visited &c?) (most recent TS)
  product list (mostly r/o, replication for high read throughput)

They claim main advantage of Dynamo is flexible N, R, W
  What do you get by varying them?
  N-R-W
  3-2-2 : default, reasonable fast R/W, reasonable durability
  3-3-1 : fast W, slow R, not very durable, not useful?
  3-1-3 : fast R, slow W, durable
  3-3-3 : ??? reduce chance of R missing W?
  3-1-1 : not useful?

They had to fiddle with the partitioning / placement / load balance (6.2)
  Old scheme:
    Random choice of node ID meant new node had to split old nodes' ranges
    Which required expensive scans of on-disk DBs
  New scheme:
    Pre-determined set of Q evenly divided ranges
    Each node is coordinator for a few of them
    New node takes over a few entire ranges
    Store each range in a file, can xfer whole file

How useful is ability to have multiple versions? (6.3)
  I.e. how useful is eventual consistency
  This is a Big Question for them
  6.3 claims 0.001% of reads see divergent versions
    I believe they mean conflicting versions (not benign multiple versions)
    Is that a lot, or a little?
  So perhaps 0.001% of writes benefitted from always-writeable?
    I.e. would have blocked in primary/backup scheme?
  Very hard to guess:
    They hint that the problem was concurrent writers, for which
      better solution is single master
    But also maybe their measurement doesn't count situations where
      availability would have been worse if single master

Performance / throughput (Figure 4, 6.1)
  Figure 4 says average 10ms read, 20 ms writes
    the 20 ms must include a disk write
    10 ms probably includes waiting for R/W of N
  Figure 4 says 99.9th pctil is about 100 or 200 ms
    Why?
    "request load, object sizes, locality patterns"
    does this mean sometimes they had to wait for coast-coast msg? 

Puzzle: why are the average delays in Figure 4 and Table 2 so low?
  Implies they rarely wait for WAN delays
  But Section 6 says "multiple datacenters"
    you'd expect *most* coordinators and most nodes to be remote!
    Maybe all datacenters are near Seattle?

Wrap-up
  Big ideas:
    eventual consistency
    always writeable despite failures
    allow conflicting writes, client merges
  Awkward model for some applications (stale reads, merges)
    this is hard for us to tell from paper
  Maybe a good way to get high availability + no blocking on WAN
    but PNUTS master scheme implies Yahoo thinks not a problem
  No agreement on whether eventual consistency is good for storage systems 
```
