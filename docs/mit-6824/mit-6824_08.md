# Harp

# 6.824 2015 年第 8 讲：Harp

**注意：**这些讲座笔记是从 2015 年春季 6.824 [课程网站](http://nil.csail.mit.edu/6.824/2015/schedule.html)上发布的内容稍作修改。

论文：Harp 文件系统中的复制

+   利斯科夫，盖马瓦特，格鲁伯，约翰逊，施里拉，威廉姆斯

+   SOSP 1991

#### 为什么我们在读这篇论文？

+   Harp 是第一个完整的主/备份系统，处理了分区问题。

+   它是一个完整的复制服务（文件服务器）的案例研究

+   它使用类似 Raft 的复制技术

#### 1991 年的论文如何仍然值得阅读？

+   Harp 引入的技术仍然被广泛使用

+   很少有论文描述完整的复制系统

#### 这篇论文混合了基本原理和次要内容。

+   我们非常关心复制

+   我们可能对 NFS 本身不太在乎

    +   但是我们非常关心将实际应用程序与复制协议集成时面临的挑战。

+   我们关心优化的可能性所在。

#### 我将专注于 Harp 中尚不存在的部分。

+   但请注意，Harp 比 Raft 早了 20 多年。

+   Raft 在很大程度上是 Harp 开创性思想的教程。

    +   尽管它们在许多细节上有所不同。

#### Harp 论文解释了 Raft 论文没有解释的内容？

+   将复杂服务适应状态机抽象

    +   例如应用操作两次的可能性

+   大量优化

    +   请求流水线到备份

    +   见证者，以减少复本的数量

    +   使用租约只在主服务器上执行只读操作

+   重新启动的服务器与大状态的高效重新集成

    +   不想做像复制整个磁盘这样的事情

    +   "赶上"重新加入复制品

+   电源故障，包括所有服务器同时失败

+   磁盘上的高效持久性

+   日志释放

#### Harp 的作者尚未实现恢复

+   较早的论文（1988 年）描述了[视图戳复制](http://www.pmg.csail.mit.edu/papers/vr.pdf)

+   [稍后（2006 年）的论文](http://pmg.csail.mit.edu/papers/vr-revisited.pdf)描述更清晰，虽然有些不同：

#### 基本设置是熟悉的

+   客户端，主服务器，备份（S），见证者（S）。

+   客户端 -> 主服务器

+   主服务器 -> 备份

+   备份（S）-> 主服务器

    +   主服务器等待当前视图中的所有备份/晋升的见证者

    +   提交点

+   主服务器 -> 执行并回复客户端

+   主服务器 -> 告诉备份要提交

+   `2n+1`个服务器，`n`个备份，`n`个见证者，1 个主服务器。

    +   需要`n+1`个服务器的大多数`=>`

    +   容忍多达`n`个故障

+   客户端将 NFS 请求发送到主服务器

    +   主服务器将每个请求转发给所有备份

    +   在所有备份回复之后，主服务器可以执行操作并将其应用于其文件系统

    +   在后续操作中，主服务器通过 ACK 告诉备份操作已提交

#### 为什么需要`2b+1`个服务器来容忍`b`个故障？

+   （这是复习...）

+   假设我们有`N`个服务器，并执行写操作。

+   不能等待超过`N-b`，因为`b`可能已经死了。

+   所以让我们要求每个操作等待`N-b`。

+   我们没有等待的`b`可能是活动的，并且在另一个分区中。

+   如果`N-b > b`，我们可以阻止它们继续进行。

+   即`N > 2b => N = 2b + 1`就足够了。

#### Harp 的见证人是什么？

+   主要操作者和备份都有 FSs

+   见证人不会从主要操作者接收任何内容，也没有 FSs

+   假设我们有一个`P，B`和一个`W`

+   如果有一个分区`P | B, W`，见证人充当裁决者

    +   无论哪个（P 或 B）能与见证人交流，都可以继续执行客户端操作

    +   见证人充当裁决者：谁能与其交流就能获胜并成为主要操作者

+   见证人的第二个用途是记录操作

+   一旦见证人成为分区`B, W`的一部分，它记录操作，以便大多数节点具有最新操作。

+   见证人的最终功能是，当主要操作者复活时，自主操作者已经记录了自主消失以来发出的每一个操作，因此见证人可以重新执行每个操作给主要操作者，使主要操作者对执行的所有操作保持最新状态

    +   高效地使主要操作者跟上速度

    +   备份也可以做到，但 Harp 设计为备份将操作日志转储到磁盘，见证人保留日志本身，以便能够快速将其发送给主要操作者重新应用

        +   重新应用见证人日志比复制备份磁盘更快的假设

        +   见证人日志不会变得太大的假设

+   见证人是与 Raft 的一个重要区别。

+   这`b`个见证人通常不会听取操作或保留状态。

+   为什么这样做没问题？

    +   `2b+1`中的`b+1`确实有状态

    +   因此，任何`b`个故障都会留下至少一个活动状态的副本。

+   为什么需要`b`个见证人？

    +   如果带有状态的`b`个副本失败，见证人提供所需的`b+1`多数派支持。

    +   以确保只有一个分区在运行--没有分裂的情况。

+   因此，在一个 3 服务器系统中，见证人用于解决主要操作者和备份位于不同分区时允许哪个分区操作的冲突。

    +   带有见证人的分区获胜。

#### 主要操作者需要将操作发送给见证人吗？

+   主要操作者必须从`2b+1`中的大多数收集每个 r/w 操作的 ACK。

    +   以确保它仍然是主要操作者--仍然处于多数派分区。

    +   以确保操作在足够多的服务器上以与任何交集

        +   形成新视图的未来多数派。

+   如果所有备份都正常，主要操作者+备份足以形成多数派。

+   如果`m`个备份宕机：

    +   主要操作者必须与`m`个“晋升”的见证人交流，以获得每个操作的多数派支持。

    +   这些见证人必须记录该操作，以确保与任何重叠

        +   未来的多数派。

    +   因此，每个“晋升”的见证人都会保留一个日志。

+   因此，在一个`2b+1`系统中，每个视图始终有`b+1`个主要操作者必须为每个操作联系的服务器，并且存储每个操作。

注意：与 Raft 有些不同

+   Raft 继续将每个操作发送到所有服务器，当大多数回答时继续

    +   因此领导者必须保持完整的日志，直到失败的服务器重新加入

+   Harp 从视图中消除失败的服务器，不会将操作发送给它

    +   只有见证者必须保留一个大日志；有特殊计划（内存，磁盘，磁带）。

+   更大的问题是，将重新加入的副本更新到最新状态可能需要大量工作；需要仔细设计。

#### UPS 的故事是什么？

+   这是 Harp 设计中最有趣的方面之一

+   每台服务器的电源线都插在 UPS 上

+   UPS 有足够的电池可以运行服务器几分钟

+   UPS 通过串口告诉服务器主交流电断电了

+   服务器将脏文件系统块和 Harp 日志写入磁盘，然后关闭

#### Harp 购买 UPS 是为了什么？

+   有效防止所有服务器的交流电故障

+   对于最多 b 台服务器的故障，复制就足够了

+   如果*所有*服务器都失败并丢失状态，那就不止 b 次故障了，

    +   所以 Harp 没有保证（实际上没有状态！）

+   有了 UPS：

    +   每台服务器都可以在不写入磁盘的情况下回复！

    +   但仍然保证保留最新状态，尽管同时发生电源故障

+   但请注意：

    +   UPS 不能保护其他同时发生故障的原因

    +   例如错误，地震

    +   Harp 对在 UPS 保护的崩溃后重新启动的服务器进行了不同处理

        +   而不是那些重新启动时丢失内存状态的崩溃

    +   因为后者可能已经忘记了*已提交*的操作

+   对于独立的故障，Harp 有强大的保证，对于像软件错误这样会导致一系列崩溃的东西，它并没有真正的解决方案

**更大的���点**，每个容错系统都面临的问题

+   每个复制系统都倾向于需要一个提交点

+   副本*必须*保持持久状态以应对所有服务器的故障

    +   已提交的操作

    +   最新的视图编号，提案编号等

+   必须在回复之前持久化这个状态

+   每次写入都写入磁盘非常慢！

    +   每个磁盘写入需要 10 毫秒，所以每秒只有 100 个操作

+   所以有几种常见模式：

    1.  低吞吐量

    1.  批处理，高延迟

        +   批量写入很多数据并同时执行它们以分摊每次写入的成本

        +   但现在你需要让客户端等待他们的写入完成更多

            +   因为它们也在等待其他客户端的写入完成

    1.  从同时发生的故障中丢失或不一致的恢复

        +   崩溃后没有保证

    1.  电池，闪存，带电容的固态硬盘等

#### 让我们谈谈 Harp 的日志管理和操作执行

+   主服务器和备份必须将客户端操作应用到它们的状态中

+   这里的状态是一个文件系统--目录，文件，所有者，权限等

+   Harp 必须模拟普通的 NFS 服务器给客户端

    +   即不要忘记已发送回复的操作

#### 典型日志记录中有什么？

+   不仅仅是客户端发出的操作，比如`chmod`

日志记录存储：

+   客户端的 NFS 操作（写入，创建目录，修改权限等）

+   阴影状态：执行后修改的 i 节点和目录内容

    +   （即执行操作后的结果）

+   客户端 RPC 请求 ID，用于重复检测

    +   主服务器可能会重复一个 RPC，如果它认为备份服务器已经失败

+   回复发送给客户端，用于重复检测

#### 为什么 Harp 有这么多日志指针？

+   FP 最近的客户端请求

+   CP 提交点（主服务器中的真实点，备份中的最新听到的点）

+   AP 最高更新发送到磁盘

+   LB 磁盘已完成写入到此处

+   GLB 所有节点已将磁盘写入到此处

#### 为什么存在 FP-CP 间隔？

+   因此，主服务器无需等待每个备份的 ACK。

    +   在将下一个操作发送到备份之前

+   主服务器将操作 CP..FP 流水线传输到备份。

+   如果有并发客户端请求，则吞吐量更高。

#### 为什么存在 AP-LB 间隔？

+   允许 Harp 在等待磁盘之前发出许多操作作为磁盘写入

+   如果有大量写入（例如，ARM 调度），则磁盘更高效

#### LB 是什么？

+   此副本在磁盘上拥有所有小于等于`LB`的内容。

+   因此，它将不再需要这些日志记录。

#### 为什么存在 LB-GLB 间隔？

+   GLB 是所有服务器的 LB 的最小值。

+   GLB 是最早的记录，如果某个服务器失去内存，则*某些*服务器可能需要它。

#### Harp 何时执行客户端操作？

有两种答案！

1.  当操作到达时，主服务器确切地确定应该发生什么。

    +   生成修改了 i-node、目录等的结果磁盘字节。

    +   这是影子状态。

    +   这发生在 CP 之前，因此主服务器必须查阅日志中的最近操作以找到最新的文件系统状态。

1.  操作提交后，主服务器和备份可以将其应用到其文件系统上。

    +   它们将日志条目的影子状态复制到文件系统；

    +   它们实际上并未执行该操作。

    +   现在主服务器可以回复客户端的 RPC 了。

#### Harp 为何以这种方式分割执行？

+   如果服务器崩溃并重新启动，则通过重放可能已错过的日志条目将其更新到最新状态。Harp 无法确定崩溃前的最后一个操作是什么，因此可能会重复一些操作。完全执行某些操作两次是不正确的，例如文件追加。

+   因此，Harp 日志条目包含*结果*状态，这是应用的内容。

追加示例：

```
 /---> picks up the modified inode
                        /
 --------------------------------
      |   Append1   |  Append2  | 
 ---- * -------------------------
     / \        \
      |          \-> new inode stored here
     CP 
```

如果备份在将 A1 写入磁盘后崩溃，但在回复给主服务器之前，备份重新启动时没有明显的方法告知其是否执行了 A1。因此，它必须重新执行它。因此，这些日志记录必须是“可重复的”。

*实际上*，许多复制系统都必须处理这个问题，这是一种处理方式。这也说明了复制可能是多么不简单。

+   Harp 需要意识到 FS 级 inode，例如

#### 关键在于，多次重放意味着复制对服务不透明。

+   服务必须修改以生成和接受来自客户端操作的状态修改。

+   一般而言，在将复制应用于现有服务时，服务必须进行修改以处理多次重放。

#### Harp 主服务器能否执行只读操作而不复制到备份？

+   例如读取文件。

+   这将更快--毕竟，没有新数据需要复制。

+   我们将只读操作转发到备份的原因是确保我们找出是否分区，并且执行了我们不知道的 1000 次操作：确保我们不会用正在读取的数据的旧写入进行回复

+   有何危险？

+   Harp 的理念：租约

+   备份承诺不会在一段时间内形成新的视图。

    +   （即不要将任何操作作为主节点处理）

+   主节点可以在本地执行只读操作，时间减去误差。

    +   因为它知道在那段时间内备份不会作为主节点执行任何操作（备份承诺了这一点！）

+   取决于相当同步的时钟：

    +   Robert Morris：“对此并不太满意。”

    +   主节点和备份之间必须对时间流逝速度有界的不同意见。

    +   这实际上需要有界的频率偏差

        +   显然硬件在提供这个方面做得很糟糕

#### 主节点执行只读操作时应该使用什么状态？

+   它必须等待所有先前到达的操作都提交吗？

    +   不！那几乎和提交只读操作一样慢。

+   它应该查看 FP 操作时的状态吗，即最新的读/写操作？

    +   不行！该操作尚未提交；不允许显示其效果。

+   因此，Harp 使用 CP 时的状态执行只读操作。

+   如果客户端发送了一个写操作，然后（在写操作完成之前）读取了相同的数据怎么办？

    +   读操作可能在写操作之前看到数据！

    +   为什么可以这样？

    +   客户端同时发送了读操作和写操作。它没有权利期望其中一种顺序。因此，如果读操作没有看到写操作的效果，那是可以接受的——如果读操作通过网络比写操作更快，那么你会得到相同的答案，这种情况可能发生。只有在发出写操作，等待写操作的回复，然后发出读操作时，才能期望读操作看到写操作的效果。

#### 失败恢复是如何工作的？

+   即，Harp 在视图更改期间如何恢复复制的状态？

为以下情景做准备：

5 个服务器：S1 通常是主节点，S2+S3 是备份，S4+S5 是见证者

情景：

```
 S1+S2+S3; then S1 crashes
  S2 is primary in new view (and S4 is promoted) 
```

+   S2 会拥有每一个已提交的操作吗？

    +   是的。

+   S2 会拥有 S1 接收的每一个操作吗？

    +   不。不，可能操作从 S1 到 S2，然后 S1 崩溃了。

+   S2 的日志尾部会与 S3 的日志尾部相同吗？

    +   不一定。

        +   可能操作已经到达了 S2，但未到达 S3，然后 S1 崩溃了。

        +   可能操作已经到达了 S2，而 S3 崩溃了，所以 S4 被提升为领导者。然后 S3 恢复了？

+   S2 和 S3 的日志尾部可以相差多远？

    +   不是由 CP 决定，因为已提交的操作可能是在提升见证者的帮助下提交的`=>`备份日志不同

+   如何使 S2 和 S3 的日志保持一致？

    +   必须提交在 S2+S3 日志中都出现的操作

    +   那些只出现在一个日志中的操作怎么处理？

        +   在这种情况下，可以丢弃，因为可能没有提交。

        +   但通常已提交的操作可能仅在一个日志中可见。

+   从什么时候开始，被提升的见证者必须开始保留日志？

#### 如果 S1 在回复客户端之前崩溃了怎么办？

+   客户端会收到回复吗？

S1 恢复后，磁盘完好无损，但内存丢失。

+   它将成为主节点，但 Harp 不能立即使用其状态或日志。

    +   与 Raft 不同，领导者只有在拥有最佳日志时才会选举。

    +   Harp 必须从被提升的见证者（S4）重新播放日志

+   S1 在崩溃前是否执行了一个操作，而副本在接管后没有执行？

    +   不，只执行到 CP，而 CP 在 S2+S3 上是安全的。

新场景：S2 和 S3 被分区（但仍然存活）。

+   S1+S4+S5 能够继续处理操作吗？

    +   是的，晋升为见证人 S4+S5

+   S4 移动到 S2/S3 分区

+   S1+S5 能够继续吗？

    +   不，主要的 S1 没有得到足够的备份 ACK

+   S2+S3+S4 能够继续吗？

    +   是的，新视图将日志条目从 S4 复制到 S2、S3，现在 S2 是主要的

+   注意：

    +   新主要缺少了许多已提交的操作

    +   一般来说，一些 *已提交* 操作可能仅在一个服务器上

新场景：S2 和 S3 被分区（但仍然存活）。

+   S4 崩溃，丢失内存内容，重新启动到 S2/S3 分区

+   他们可以继续吗？

    +   仅当没有形成并提交更多操作的其他视图时。

+   如何检测？

    +   取决于 S4 的磁盘视图号是什么。

    +   如果 S4 的磁盘视图号与 S2+S3 的相同，则可以。

        +   未形成新的视图。

        +   S2+S3 必须知道旧视图中所有已提交操作的情况。

#### 每个人（S1-5）都遭遇断电。

+   S4 的磁盘和内存丢失了，但修复后会重新启动。

+   S1 和 S5 永远无法恢复。

+   S2 和 S3 在磁盘上保存所有内容，重新启动没有问题。

+   S2+S3+S4 能够继续吗？

+   （比看起来更难）

    +   相信答案是“否”：无法确定故障之前 S4 的状态。

    +   可能通过 S1+S5 形成一个新视图，并执行一些操作。

        +   S2 和 S3 能知道先前的视图吗？不总是。

#### Harp 何时可以形成新视图？

1.  没有其他可能的视图。

1.  知道最近视图的视图号。

1.  知道最近视图的所有操作。

细节：

+   如果你有 n+1 个节点在新视图中，则 (1) 为真。

+   如果你有 n+1 个节点，并且自上一个视图以来没有丢失视图号，则为真。

    +   视图号存储在磁盘上，所以它们只需要知道磁盘是正常的。

    +   其中之一 *必须* 在先前的视图中。

    +   因此只需取最高视图号。

+   还有 #3 吗？

    +   需要一个磁盘映像和一个日志，共同反映出上一个视图结束时的所有操作。

    +   可能来自不同的服务器，例如晋升的见证人的日志，多个视图之前失败的备份。

#### Harp 有性能优势吗？

+   在图 5-1 中，为什么 Harp 比非复制服务器 *更快*？

+   通过将 RPC 替换为磁盘操作，我们可以期待多少胜利？

#### 为什么图形 x=load y=response-time？

+   为什么这张图有意义？

+   为什么不只是图形总时间执行 X 操作？

+   一个原因是系统有时在高负载下变得更有效率/不那么有效率。

+   我们非常关心他们在超载情况下的表现。

#### 为什么响应时间随负载增加而增加？

+   为什么首先是渐进的...

    +   排队和随机突发？

    +   有些操作比其他操作更昂贵，导致临时延迟。

+   然后几乎笔直上升？

    +   可能有硬限制，如每秒磁盘 I/O 次数。

    +   一旦提供的负载 > 容量，队列长度就会发散
