# 对等系统

# 6.824 2015 年第 22 讲：对等系统

**注意：**这些讲义与 2015 年春季 6.824 [课程网站](http://nil.csail.mit.edu/6.824/2015/schedule.html)上发布的略有不同。

## P2P 系统

+   今天：看看像 BitTorrent 和 Chord 这样的对等系统

+   文件共享服务的经典实现：用户与集中服务器交流以下载文件

+   用户应该可以相互交流以获取文件

+   对等梦想：没有集中的组件，只是由人们的计算机构建

## 为什么使用 P2P？

+   **(+)**在大量 PC 上分散提供文件的工作

+   **(+)**可能比集中式系统更容易部署

    +   没有人必须购买带宽和存储量大的集中服务器

+   **(+)**如果你打好牌，资源数量应该随用户数量自然增长=> 减少过载的机会

+   **(+)**没有集中的服务器=>失败的可能性较小（更难以通过 DoS 攻击）

+   `=>`这么多优点！为什么还会有人构建非 P2P 系统？

+   **(-)**在百万用户的系统中查找文件需要复杂的设计`<=>`查找东西很难（不能只是问一个数据库）

+   **(-)**用户计算机不像数据中心中的服务器那样可靠。用户会关闭他们的计算机等等。

+   **(-)**一些 P2P 系统是开放的（BitTorrent），但有些是封闭的，比如只有亚马逊的计算机参与其中（有点像 Dynamo）。

    +   `=>`在开放系统中，会有恶意用户`=>`易受攻击

结果是 P2P 软件在某些特定领域有一定的市场

+   有很多数据的系统，比如在线视频服务

+   聊天系统

+   在不自然使用中心服务器的设置中，比如比特币

    +   DNS 分散化将是一个好主意

+   *似乎*主要用途是提供非法文件

## BitTorrent

### 预 DHT BitTorrent

示意图：

```
 ---        ---
| W |      | T |
 ---        ---
/\         /\
| click on /  .torrent file
\         /
 ---/----        ---  
| C |  <------> | C |
 ---             --- 
```

+   客户端访问网页服务器并下载一个 .torrent 文件

+   种子文件存储了数据的哈希和追踪器的地址

+   追踪器记录所有已下载该文件并且可能仍然拥有它并且可能愿意为其他客户端提供它的所有客户端

+   客户端联系追踪器，询问其他客户端

+   这里的巨大收益在于文件直接在客户端之间传输，网页服务器和追踪器没有太多成本

### 基于 DHT 的 BitTorrent（无追踪器的种子）

+   追踪器是单点故障

+   可以通过复制它们或使用额外的追踪器来解决这个问题

+   下载文件的用户也形成了一个巨大的分布式键值存储（DHT）

+   客户端仍然必须与原始网页服务器交流以获取它想要下载的`infohash`文件

+   它使用它作为在 DHT 中查找的键

+   值是拥有文件的其他客户端的 IP 地址

+   这真的替代了追踪器

+   如何在 DHT 中找到入口节点？也许客户端有一些硬编码的众所周知的节点

+   *也许* DHT 更可靠，更不容易受到法律（传票）和技术攻击的影响

+   BitTorrent 非常受欢迎：数以千万计的用户 `=>` 巨大的 DHT，然而，大多数种子都由真实的追踪器支持

## DHT 是如何开始的？

+   15 年前就有了一堆关于如何构建 DHT 的研究

+   关键是利用互联网上数百万台计算机提供接近数据库的东西

+   接口非常简单：`Put(k, v), Get(k)`

    +   希望 puts 在一段时间后反映在 gets 中

+   实际上构建一致性系统是很困难的

+   对数据的可用性和一致性没有多少保证

    +   系统不能保证在执行 `Put()` 时保留您的数据

+   即使有了这些弱保证，构建起来仍然很困难

## DHT 设计

1.  当您想要获取一个键时，向所有人发送 Get 的洪水

    +   `=>` 系统无法处理过多负载

1.  假设每个人都同意 DHT 中的节点完整列表。

    +   然后你可以有一些散列约定，将一个键散列到一个确切的节点，并且查找是有效的。

    +   非常重要的是，所有人都同意，否则 A 发送 put 给 X，B 发送 get 给 Y，用于相同的键 `k`

    +   真正的问题在于很难保持表格的更新和准确性。

我们想要的是：

+   我们正在寻找一个系统，其中每个节点只需知道其他几个节点。

+   我们不希望节点发送太多消息来执行查找

+   所有 DHT 采取的大致方法是定义一个跨越节点的全局数据结构

+   不好的想法：将所有节点组织成二叉树，数据存储在叶节点中，较低的键位于最左边的节点

    +   所有流量都通过根节点（不好） => 如果根节点宕机，则分区

    +   如何替换掉宕机的节点？

## Chord

+   在 0 到 2¹⁶⁰ - 1 的循环 ID 空间中的数字（如模 p 的整数）

+   每个节点在此空间中选择一个随机 ID 作为其节点 ID

+   这个空间的键具有标识符，并且我们希望标识符具有均匀分布，因为我们用它来将键映射到节点标识符 `=>` 对实际键使用哈希函数来获取它们的标识符

+   负责键的节点是在顺时针方向上第一个*最接近*该键的节点：称为其**后继**

    +   近似距离 `= |节点 ID - 键 ID|`

慢但正确的方案：

+   通过某种巫术，每个节点只需知道自己的后继（比如说，节点 2 的后继是节点 18，等等）

+   我们始终可以通过跟随这些后继指针从每个节点开始进行查找

    +   这被称为*路由*

    +   所有关于将查找消息转发到环上更远节点的事情

+   这很慢，如果有数百万个节点，可能会为单个查找发送数百万条消息

+   需要时间对 DHT 中节点总数的对数 `=>` 每一跳都必须能够计算它与任何目标键之间的距离

+   在 Chord 中，每个节点都有一个包含 160 个条目的*指针表*

+   节点 `n` 的指针表具有条目 `i`：

    +   `f[i] = successor(n + 2^i)`

    +   `=>` 第 159 个条目将指向某个节点 `n + 2¹⁵⁹` 大致位于 ID 空间的中间位置

+   每一跳大约是 50 毫秒，如果跳跃是环球的话

    +   `=>` 大约 1 秒钟可以通过 20 个节点 `=>` 一些应用可能无法很好地处理这一点（BitTorrent 可以，因为它只在 DHT 中存储 IP 地址）

+   当节点加入时，它们会得到入口节点的指纹的副本

    +   对于新节点来说不是很准确，但足够好

    +   `=>` 必须纠正表 `=>` 对于第 `i` 个条目，查找 `n+2^i` 并将 `f[i]` 设置为回复的节点的地址

+   每次查找都可能遇到一个死节点，超时需要很长时间

+   BitTorrent 中的流动性相当高

    +   1000 万人参与这个小时，下一个小时将有另外 1000 万人 `=>` 难以保持 Finger 表的更新

+   `log n` 的查找时间并不那么好

+   Finger 表仅用于加速查找

+   每个节点必须正确地知道它的后继节点才能使 Chord 正确

    +   这样，一个节点的获取会看到另一个节点的放置

+   当一个节点首次加入时，它会对自己的标识符进行查找以找到它的后继节点

    +   `--> 10 -- [新节点 15] --> 20 -->`

    +   15 将其后继指针设置为 20

    +   到目前为止没有人在进行查找

    +   直到 10 知道它，15 才真正成为系统的一部分

    +   每个节点定期询问其后继节点他们认为谁是他们的前驱节点

        +   `10: 嘿 20，你的前驱是谁？`

        +   `20: 我的前驱是 15`

        +   `10: 哦，以为是我，所以让我把 15 设置为我的后继节点`

        +   `15: 哦，嗨 10，谢谢你把我添加为你的后继节点，让我把你添加为我的前驱节点`

        +   这被称为稳定化

例子：

```
10 -> 20

12, 18 join

10 -> 20, 12->20, 18->20, 20->18

10 -> 18, 18->10, 18->20, 20->18, 12 ->18

10 -> 18,18->20, 20->18, 12 ->18, 18->12

10 -> 12, 12->10 12->18, 18->12, 18->20, 20->18 
```

+   当一个节点获得一个更接近的前驱节点时，它会将更接近前驱节点的键转移到那里

如果节点失败了，我们能正确进行查找吗？

+   假设在查找过程中一个中间节点失败了，那么发起节点可以简单地选择另一个

+   在查找过程的末尾，不再使用 Finger 表。而是跟随后继指针 `=>` 如果一个节点失败，那么查找无法继续 `=>` 节点必须记住它们的后继节点的后继节点才能继续

+   在互联网上发生分区的概率很低

6.824 2015 年的原始笔记

```
Lecture outline:
  peer-to-peer (P2P)
  BitTorrent
  DHTs
  Chord

Peer-to-peer
  [user computers, files, direct xfers]
  users computers talk directly to each other to implement service
    in contrast to user computers talking to central servers
  could be closed or open
  examples:
    skype, video and music players, file sharing

Why might P2P be a win?
  spreads network/caching costs over users
  absence of server may mean:
    easier to deploy
    less chance of overload
    single failure won't wreck the whole system
    harder to attack

Why don't all Internet services use P2P?
  can be hard to find data items over millions of users
  user computers not as reliable than managed servers
  if open, can be attacked via evil participants

The result is that P2P has some successful niches:
  Client-client video/music, where serving costs are high
  Chat (user to user anyway; privacy and control)
  Popular data but owning organization has no money
  No natural single owner or controller (Bitcoin)
  Illegal file sharing

Example: classic BitTorrent
  a cooperative download system, very popular!
  user clicks on download link for e.g. latest Linux kernel distribution
    gets torrent file w/ content hash and IP address of tracker
  user's BT client talks to tracker
    tracker tells it list of other user clients w/ downloaded file
  user't BT client talks to one or more client's w/ the file
  user's BT client tells tracker it has a copy now too
  user's BT client serves the file to others for a while
  the point:
    provides huge download b/w w/o expensive server/link

BitTorrent can also use a DHT instead of / as well as a tracker
  this is the topic of today's readings
  BT clients cooperatively implement a giant key/value store
  "distributed hash table"
  the key is the file content hash ("infohash")
  the value is the IP address of a client willing to serve the file
    Kademlia can store multiple values for a key
  client does get(infohash) to find other clients willing to serve
    and put(infohash, self) to register itself as willing to serve
  client also joins the DHT to help implement it

Why might the DHT be a win for BitTorrent?
  single giant tracker, less fragmented than many trackers
    so clients more likely to find each other
  maybe a classic tracker too exposed to legal &c attacks
  it's not clear that BitTorrent depends heavily on the DHT
    mostly a backup for classic trackers?

How do DHTs work?

Scalable DHT lookup:
  Key/value store spread over millions of nodes
  Typical DHT interface:
    put(key, value)
    get(key) -> value
  loose consistency; likely that get(k) sees put(k), but no guarantee
  loose guarantees about keeping data alive

Why is it hard?
  Millions of participating nodes
  Could broadcast/flood request -- but too many messages
  Every node could know about every other node
    Then hashing is easy
    But keeping a million-node table up to date is hard
  We want modest state, and modest number of messages/lookup

Basic idea
  Impose a data structure (e.g. tree) over the nodes
    Each node has references to only a few other nodes
  Lookups traverse the data structure -- "routing"
    I.e. hop from node to node
  DHT should route get() to same node as previous put()

Example: The "Chord" peer-to-peer lookup system
  By Stoica, Morris, Karger, Kaashoek and Balakrishnan; 2001

Chord's ID-space topology
  Ring: All IDs are 160-bit numbers, viewed in a ring.
  Each node has an ID, randomly chosen

Assignment of key IDs to node IDs?
  Key stored on first node whose ID is equal to or greater than key ID.
    Closeness is defined as the "clockwise distance"
  If node and key IDs are uniform, we get reasonable load balance.
  So keys IDs should be hashes (e.g. bittorrent infohash)

Basic routing -- correct but slow
  Query is at some node.
  Node needs to forward the query to a node "closer" to key.
    If we keep moving query closer, eventually we'll win.
  Each node knows its "successor" on the ring.
    n.lookup(k):
      if n < k <= n.successor
        return n.successor
      else
        forward to n.successor
  I.e. forward query in a clockwise direction until done
  n.successor must be correct!
    otherwise we may skip over the responsible node
    and get(k) won't see data inserted by put(k)

Forwarding through successor is slow
  Data structure is a linked list: O(n)
  Can we make it more like a binary search?
    Need to be able to halve the distance at each step.

log(n) "finger table" routing:
  Keep track of nodes exponentially further away:
    New state: f[i] contains successor of n + 2^i
    n.lookup(k):
      if n < k <= n.successor:
        return successor
      else:
        n' = closest_preceding_node(k) -- in f[]
        forward to n'

for a six-bit system, maybe node 8's looks like this:
  0: 14
  1: 14
  2: 14
  3: 21
  4: 32
  5: 42

Why do lookups now take log(n) hops?
  One of the fingers must take you roughly half-way to target

There's a binary lookup tree rooted at every node
  Threaded through other nodes' finger tables
  This is *better* than simply arranging the nodes in a single tree
    Every node acts as a root, so there's no root hotspot
    But a lot more state in total

Is log(n) fast or slow?
  For a million nodes it's 20 hops.
  If each hop takes 50 ms, lookups take a second.
  If each hop has 10% chance of failure, it's a couple of timeouts.
  So in practice log(n) is better than O(n) but not great.

How does a new node acquire correct tables?
  General approach:
    Assume system starts out w/ correct routing tables.
    Use routing tables to help the new node find information.
    Add new node in a way that maintains correctness.
  New node m:
    Sends a lookup for its own key, to any existing node.
      This yields m.successor
    m asks its successor for its entire finger table.
  At this point the new node can forward queries correctly
  Tweaks its own finger table in background
    By looking up each m + 2^i

Does routing *to* new node m now work?
  If m doesn't do anything,
    lookup will go to where it would have gone before m joined.
    I.e. to m's predecessor.
    Which will return its n.successor -- which is not m.
  So, for correctness, m's predecessor needs to set successor to m.
    Each node keeps track of its current predecessor.
    When m joins, tells its successor that its predecessor has changed.
    Periodically ask your successor who its predecessor is:
      If that node is closer to you, switch to that guy.
    So if we have x m y
      x.successor will be y (now incorrect)
      y.predecessor will be m
      x will ask its x.successor for predecessor
        x learns about m
        sets x.successor to m
        tells m "x is your predecessor"
        called "stabilization"
  Correct successors are sufficient for correct lookups!

What about concurrent joins?
  Two new nodes with very close ids, might have same successor.
  Example:
    Initially 40 then 70
    50 and 60 join concurrently
    at first 40, 50, and 60 think their successor is 70!
    which means lookups for e.g. 45 will yield 70, not 50
    after one stabilization, 40 and 50 will learn about 60
    then 40 will learn about 50

To maintain log(n) lookups as nodes join,
  Every one periodically looks up each finger (each n + 2^i)

Chord's routing is conceptually similar to Kademlia's
  Finger table similar to bucket levels
    Both halve the metric distance for each step
    Both are about speed and can be imprecise
  n.successor similar to Kademlia's requirement that
    each node know of all the nodes that are very close in xor-space
    in both cases care is needed to ensure that different lookups
      for same key converge on exactly the same node

What about node failures?
  Assume nodes fail w/o warning. Strictly harder than graceful departure.
  Two issues:
    Other nodes' routing tables refer to dead node.
    Dead node's predecessor has no successor.
  If you try to route via dead node, detect timeout, treat as empty table entry.
    I.e. route to numerically closer entry instead.
  For dead successor
    Failed node might have been just before key ID!
      So we need to know what its n.successor was
    Maintain a _list_ of successors: r successors.
    Lookup answer is first live successor >= key
      or forward to *any* successor < key

Kademlia has a faster plan for this
  send alpha (or k) lookup RPCs in parallel, to different nodes
  send more lookups as previous ones return info about nodes closer to key
  single non-responsive node won't cause lookup to suffer a timeout

Dealing with unreachable nodes during routing is extremely important
  "Churn" is very high in open p2p networks
  People close their laptops, move WiFi APs, &c pretty often
  Measurement of Bittorrent/Kademlia suggest lookups are not very fast

Geographical/network locality -- reducing lookup time
  Lookup takes log(n) messages.
    But they are to random nodes on the Internet!
    Will often be very far away.
  Can we route through nodes close to us on underlying network?
  This boils down to whether we have choices:
    If multiple correct next hops, we can try to choose closest.

Idea:
  to fill a finger table entry, collect multiple nodes near n+2^i on ring
  perhaps by asking successor to n+2^i for its r successors
  use lowest-ping one as i'th finger table entry

What's the effect?
  Individual hops are lower latency.
  But less and less choice (lower node density) as you get close in ID space.
  So last few hops likely to be very long. 
  Though if you are reading, and any replica will do,
    you still have choice even at the end.

What about security?
  Self-authenticating data, e.g. key = SHA1(value)
    So DHT node can't forge data
    Of course it's annoying to have immutable data...
  Can someone cause millions of made-up hosts to join?
    They don't exist, so routing will break?
    Don't believe new node unless it responds to ping, w/ random token.
  Can a DHT node claim that data doesn't exist?
    Yes, though perhaps you can check other replicas
  Can a host join w/ IDs chosen to sit under every replica?
    Or "join" many times, so it is most of the DHT nodes?
    Maybe you can require (and check) that node ID = SHA1(IP address)

Why not just keep complete routing tables?
  So you can always route in one hop?
  Danger in large systems: timeouts or cost of keeping tables up to date.

How to manage data?
  Here is the most popular plan.
  DHT doesn't guarantee durable storage
    So whoever inserted must re-insert periodically if they care
    May want to automatically expire if data goes stale (bittorrent)
  DHT does replicate each key/value item
    On the nodes with IDs closest to the key, where looks will find them
    Replication can help spread lookup load as well as tolerate faults
  When a node joins:
    successor moves some keys to it
  When a node fails:
    successor probably already has a replica
    but r'th successor now needs a copy

Retrospective
  DHTs seem very promising for finding data in large p2p systems
    Decentralization seems good for load, fault tolerance
  But: the security problems are difficult
  But: churn is a serious problem, particularly if log(n) is big
  So DHTs have not had the impact that many hoped for 
```
