# MapReduce

# 6.824 2015 年讲座 13：MapReduce

**注意：**这些讲义笔记是从 2015 年春季的 6.824[课程网站](http://nil.csail.mit.edu/6.824/2015/schedule.html)上略微修改的。

## 介绍

+   第二次查看这篇论文，更多地讨论容错。

    +   参见第一讲

+   程序员的真正胜利是简单性

+   巧妙的设计技巧以获得良好的性能。

## 例如：构建倒排索引

+   你需要一个倒排索引来进行搜索索引

+   将关键字映射到它们所在的文档

例如：

```
doc 31: I am Alex
doc 32: Alex at 8 am 
```

我们想要的输出是一个索引：对于输入中的每个单词，我们希望得到该单词出现的每个地方的列表（文档+偏移量）：

```
alex: 31/2, 32/0 ...
am:   31/1, 32/3 ... 
```

用于构建倒排索引的实际映射/减少函数：

```
map(doc file)
    split doc into words
    for each word
        emit(word, {doc #, offset})

reduce(word string, occurrences list<doc #, offset>)
    emit(word, sorted list of ocurrences by doc # and then by offset) 
```

## 输入文件

在 MapReduce 中，输入存储在 GFS（Google 的文件系统）中

```
Input, M splits, one    R reduce tasks
map function for each 
split

---------               ---------------------------------
|       |               |   |   | * |           |   |   |
---------               ----------|----------------------
|       |               |   |   | * |           |   |   |
---------               ----------|----------------------
|       |               |   |   | * |           |   |   |
---------               ----------|----------------------
|       |               .   .   . |  
---------               .   .   .  \
|       |               .   .   .   \-----> data for a single reduce task
---------               .   .   .           is all the data in the column
|       |               .   .   .
---------               .   .   .
|       |               .   .   .
---------               .   .   . 
```

如果一个减少工作者的数据列不适合内存会发生什么？似乎会写入磁盘。

请注意，对于每个唯一的关键字，都会发生一次单独的减少调用。因此，在我们的倒排索引示例中，这意味着对于关键字“the”，可能会出现一亿次在大量文档中。因此，这会花一些时间。MapReduce 无法并行处理减少调用中的工作（对于某些可组合的减少函数（比如 f(reduce(k, l1), reduce(k, l2)) = reduce(k, l1+l2)）来说是一个失去的机会）。

+   我认为论文中提到的*组合器函数*可以缓解这个问题。

## 性能

+   一切都是关于数据移动的

    +   在集群中推送几千兆字节的数据

    +   1000 台机器

        +   可能可以将数据推送到 RAM（1GB/s）以达到 1000GB/s

        +   可能可以将数据推送到磁盘（100MB/s）以达到 100GB/s

        +   网络在一台机器上可以以 1Gbit/s = 100MB/s 的速度运行。

            +   对于 1000 台机器，布线是昂贵的，并且会降低速度

            +   网络通常是一棵树，叶子节点上有服务器，内部节点上有更大的交换机。

            +   瓶颈是根交换机，在谷歌以 18GB/s 运行

        +   因此，网络只能以 18GB/s 的速度推送数据`=>` *瓶颈*

## 设计见解

需要应对网络问题。

分布式共享内存（DSM）在任何机器都可以在（分布式）内存中写入内存方面非常灵活。问题在于，你最终会得到非常低效的带宽和延迟敏感的系统。如果允许对数据进行任意读/写，则最终会出现一堆跨网络的延迟敏感的小数据移动。

当单台机器死机时，DSM 使容错变得非常困难，因为每台机器都可以做任何想做的事情（读取或写入任何内存位置），因此很难对系统进行检查点。

**关键点：**

+   `Map()` 和 `Reduce()` 仅在本地数据上运行。

+   `Map()` 和 `Reduce()` 只对大块数据进行操作

    +   分摊发送网络成本

+   系统各部分之间的交互非常少

    +   映射无法相互通信。

    +   减少调用不能相互通信。

    +   映射和减少调用无法相互通信

        +   除了将映射数据发送到减少函数的隐式通信之外，减少函数无法相互通信。

+   给程序员抽象控制网络通信

    +   一些控制如何将键映射到减少分区

输入通常存储为条纹（64MB 块）在 GFS 中，分布在许多磁盘和机器上。

+   必须要聪明，因为这将意味着 Map 任务受限于网络带宽

MapReduce 利用 GFS 的知识，实际上在存储文件块的 GFS 机器上本地运行映射任务。`=>`将带宽从 maps 增加到 18GB/s 到 100GB/s

map 生成的中间映射文件也会被存储在本地。缺点是数据只有一份副本存储在那台机器上，减少工作者只能与它进行通信`=>`带宽有限。

+   如果机器停止或崩溃，数据将丢失，必须重新启动映射

GFS 中的数据实际上是复制的（2 或 3 个副本），这给 MapReduce 提供了一个可以在每个映射任务上运行的选择 2-3 台服务器。

+   对负载平衡有利（MR 主节点可以将慢速映射任务移动到其他机器上）

    +   减少任务无法获得此优势

减少的输出存储在 GFS 中`=>`减少的输出通过网络写入。`=>`如果这是你的横截面带宽，MapReduce 系统的总输出为 18GB/s。

## QOTD

减少在映射发出一些数据后多快可以开始？

Morris：只要一列数据填满了`<=>`只要所有映射完成。

显然，只要一个映射任务发出关键字，就可以通过在减少任务的迭代器中生成值来进行，但在这种情况下性能可能很难实现。

## MapReduce 的扩展性好吗？

分布式系统的一个重要好处是，通过购买更多的机器，你*可能*能够加速它。购买机器比支付程序员更便宜。

`nx`硬件 => `nx`性能？ `n > 1`

随着机器数量的增长（增加 10 倍），输入大小保持不变`=>`输入大小必须减少（减少 10 倍）。更小的切分（减小 10 倍）。

如果我们有数百万台机器，切分的大小可能是几千字节`=>`网络延迟会影响我们的性能。

你不能比你有的键更多的减少工作者。

可扩展性受限于

+   映射切分大小

+   键的数量`>=`减少工作者的数量

+   网络带宽（随着购买更多机器，需要购买更多的“网络”）

    +   一个非常重要的问题

答案：肯定会获得一些扩展，但不是无限的（受网络限制）

## 容错性

挑战：如果在数千台计算机上运行大型作业，你肯定会遇到一些故障。因此不能简单地重新启动整个计算。必须只重做失败的机器的工作。

对于 DSM 难以实现，对于 MapReduce 较容易。

假设独立故障（也因为映射/减少是独立的）

如果工作者失败：

+   可以重新启动

+   可以保存中间输出并在故障后恢复

如果映射失败，我们必须重新运行它，因为它将其输出存储在同一台机器上，这已完成。主节点知道映射正在处理的内容，因此可以重新启动。

如果 reduce worker 崩溃，因为他们的输出存储在 GFS 上，分布在不同的服务器上。我们有很大的机会不需要重新计算，如果 reduce worker 已经完成了。

## 论文的性能评估。

论文中的图 2。为什么带宽需要 60 秒才能达到 30GB/s？

MR 作业有 1800 个 mappers，还有一些*可怜的* master 需要给每个 mapper 分配工作。所以也许 master 联系每个人需要一段时间。

为什么只有 30GB/s？这些是 map 任务，所以没有网络开销。也许 CPU 是瓶颈？不太可能。看起来像是磁盘带宽问题。30GB/s / 1800 台机器 `=>` 每个磁盘 17MB/s。

论文中的图 3。对 1TB 的数据进行排序需要 800 秒 `=>` 1.25GB/s 的排序吞吐量。

需要注意的一件事是 1800 台机器的内存足以容纳一 terrabyte 的数据。

在一台有足够内存的单机上，Morris 推断，对 1TB 的数据进行排序需要大约 30,000 秒（对 100GB 的数据进行排序需要 2500 秒）。

中间的图表表明他们只能以 5GB/s 的速度在网络上传输数据。简单地移动 1TB 的数据将需要 200 秒。而且 MapReduce 会多次移动它：从 map 到 reduce，从 reduce 到 GFS（多次复制）。

*重要洞察：* 计算涉及数据的移动。不仅仅是 CPU 周期。

# 6.824 原始笔记。

```
 Why MapReduce?
      Second look for fault tolerance and performance
      Starting point in current enthusiasm for big cluster computing
      A triumph of simplicity for programmer
      Bulk orientation well matched to cluster with slow network
      Very influential, inspired many successors (Hadoop, Spark, &c)

    Cluster computing for Big Data
      1000 computers + disks
      a LAN
      split up data+computation among machines
      communicate as needed
      similar to DSM vision but much bigger, no desire for compatibilty

    Example: inverted index
      e.g. index terabytes of web pages for a search engine
      Input:
        A collection of documents, e.g. crawled copy of entire web
        doc 31: i am alex
        doc 32: alex at 8 am
      Output:
        alex: 31/3 32/1 ...
        am: 31/2 32/4 ...
      Map(document file i):
        split into words
        for each offset j
          emit key=word[j] value=i/j
      Reduce(word, list of d/o)
        emit word, sorted list of d/o

    Diagram:
      * input partitioned into M splits on GFS: A, B, C, ...
      * Maps read local split, produce R local intermediate files (A0, A1 .. AR)
      * Reduce # = hash(key) % R
      * Reduce task i fetches Ai, Bi, Ci -- from every Map worker
      * Sort the fetched files to bring same key together
      * Call Reduce function on each key's values
      * Write output to GFS
      * Master controls all:
        Map task list
        Reduce task list
        Location of intermediate data (which Map worker ran which Map task)

    Notice:
      Input is huge -- terabytes
      Info from all parts of input contributes to each output index entry
        So terabytes must be communicated between machines
      Output is huge -- terabytes

    The main challenge: communication bottleneck
      Three kinds of data movement needed:
        Read huge input
        Move huge intermediate data
        Store huge output
      How fast can one move data?
        RAM: 1000*1 GB/sec =    1000 GB/sec
        disk: 1000*0.1 GB/sec =  100 GB/sec
        net cross-section:        10 GB/sec
      Explain host link b/w vs net cross-section b/w

    How to cope with communication bottleneck
      Locality: split storage and computation the same way, onto same machines
        Because disk and RAM are faster than the network
      Batching: move megabytes at a time, not e.g. little key/value puts/gets
        Because network latency is a worse problem than network throughput
      Programming: let the developer indicate how data should move between machines
        Because the most powerful solutions lie with application structure

    The big programming idea in MapReduce is the key-driven shuffle
      Map function implicitly specifies what and where data is moved -- with keys
      Programmer can control movement, but isn't burdened with details
      Programs are pretty constrained to help with communication:
        Map can only read the local split of input data, for locality and simplicity
        Just one batch shuffle per computation
        Reduce can only look at one key, for locality and simplicity

    Where does MapReduce input come from?
      Input is striped+replicated over GFS in 64 MB chunks
      But in fact Map always reads from a local disk
        They run the Maps on the GFS server that holds the data
      Tradeoff:
        Good: Map can read at disk speed, much faster than reading over net from GFS server
        Bad: only two or three choices of where a given Map can run
             potential problem for load balance, stragglers

    Where does MapReduce store intermediate data?
      On the local disk of the Map server (not in GFS)
      Tradeoff:
        Good: local disk write is faster than writing over network to GFS server
        Bad: only one copy, potential problem for fault-tolerance and load-balance

    Where does MapReduce store output?
      In GFS, replicated, separate file per Reduce task
      So output requires network communication -- slow
      The reason: output can then be used as input for subsequent MapReduce

    The Question: How soon after it receives the first file of
      intermediate data can a reduce worker start calling the application's
      Reduce function?

    Why does MapReduce postpone choice of which worker runs a Reduce?
      After all, might run faster if Map output directly streamed to reduce worker
      Dynamic load balance!
      If fixed in advance, one machine 2x slower -> 2x delay for whole computation
        and maybe the rest of the cluster idle/wasted half the time

    Will MR scale?
      Will buying 2x machines yield 1/2 the run-time, indefinitely?
      Map calls probably scale
        2x machines -> each Map's input 1/2 as big -> done in 1/2 the time
        but: input may not be infinitely partitionable
        but: tiny input and intermediate files have high overhead
      Reduce calls probably scale
        2x machines -> each handles 1/2 as many keys -> done in 1/2 the time
        but: can't have more workers than keys
        but: limited if some keys have more values than others
          e.g. "the" has vast number of values for inverted index
          so 2x machines -> no faster, since limited by key w/ most values
      Network may limit scaling, if large intermediate data
        Must spend money on faster core switches as well as more machines
        Not easy -- a hot R+D area now
      Stragglers are a problem, if one machine is slow, or load imbalance
        Can't solve imbalance w/ more machines
      Start-up time is about a minute!!!
        Can't reduce that no matter how many machines you buy (probably makes it worse)
      More machines -> more failures

    Now let's talk about fault tolerance
      The challenge: paper says one server failure per job!
      Too frequent for whole-job restart to be attractive

    The main idea: Map and Reduce are deterministic and functional,
        so MapReduce can deal with failures by re-executing
      Often a choice:
        Re-execute big tasks, or
        Save output, replicate, use small tasks
      Best tradeoff depends on frequency of failures and expense of communication

    What if a worker fails while running Map?
      Can we restart just that Map on another machine?
        Yes: GFS keeps copy of each input split on 3 machines
      Master knows, tells Reduce workers where to find intermediate files

    If a Map finishes, then that worker fails, do we need to re-run that Map?
      Intermediate output now inaccessible on worker's local disk.
      Thus need to re-run Map elsewhere *unless* all Reduce workers have
        already fetched that Map's output.

    What if Map had started to produce output, then crashed:
      Will some Reduces see Map's output twice?
      And thus produce e.g. word counts that are too high?

    What if a worker fails while running Reduce?
      Where can a replacement worker find Reduce input?
      If a Reduce finishes, then worker fails, do we need to re-run?
        No: Reduce output is stored+replicated in GFS.

    Load balance
      What if some Map machines are faster than others?
        Or some input splits take longer to process?
      Don't want lots of idle machines and lots of work left to do!
      Solution: many more input splits than machines
      Master hands out more Map tasks as machines finish
      Thus faster machines do bigger share of work
      But there's a constraint:
        Want to run Map task on machine that stores input data
        GFS keeps 3 replicas of each input data split
        So only three efficient choices of where to run each Map task

    Stragglers
      Often one machine is slow at finishing very last task
        h/w or s/w wedged, overloaded with some other work
      Load balance only balances newly assigned tasks
      Solution: always schedule multiple copies of very last tasks!

    How many Map/Reduce tasks vs workers should we have?
      They use M = 10x number of workers, R = 2x.
      More => finer grained load balance.
      More => less redundant work for straggler reduction.
      More => spread tasks of failed worker over more machines, re-execute faster.
      More => overlap Map and shuffle, shuffle and Reduce.
      Less => big intermediate files w/ less overhead.
      M and R also maybe constrained by how data is striped in GFS.
        e.g. 64 MByte GFS chunks means M needs to total data size / 64 MBytes

    Let's look at paper's performance evaluation

    Figure 2 / Section 5.2
      Text search for rare 3-char pattern, just Map, no shuffle or reduce
      One terabyte of input
      1800 machines
      Figure 2 x-axis is time, y-axis is input read rate
      60 seconds of start-up time are *omitted*! (copying program, opening input files)
      Why does it take so long (60 seconds) to reach the peak rate?
      Why does it go up to 30,000 MB/s? Why not 3,000 or 300,000?
        That's 17 MB/sec per server.
        What limits the peak rate?

    Figure 3(a) / Section 5.3
      sorting a terabyte
      Should we be impressed by 800 seconds?
      Top graph -- Input rate
        Why peak of 10,000 MB/s?
        Why less than Figure 2's 30,000 MB/s? (writes disk)
        Why does read phase last abt 100 seconds?
      Middle graph -- Shuffle rate
        How is shuffle able to start before Map phase finishes? (more map tasks than workers)
        Why does it peak at 5,000 MB/s? (??? net cross-sec b/w abt 18 GB/s)
        Why a gap, then starts again? (runs some Reduce tasks, then fetches more)
        Why is the 2nd bump lower than first? (maybe competing w/ overlapped output writes)
      Lower graph -- Reduce output rate
        How can reduces start before shuffle has finished? (again, shuffle gets all files for some tasks)
        Why is output rate so much lower than input rate? (net rather than disk; writes twice to GFS)
        Why the gap between apparent end of output and vertical "Done" line? (stragglers?)

    What should we buy if we wanted sort to run faster?
      Let's guess how much each resource limits performance.
      Reading input from disk: 30 GB/sec = 33 seconds (Figure 2)
      Map computation: between zero and 150 seconds (Figure 3(a) top)
      Writing intermediate to disk: ? (maybe 30 Gb/sec = 33 seconds)
      Map->Reduce across net: 5 GB/sec = 200 seconds
      Local sort: 2*100 seconds (gap in Figure 3(a) middle)
      Writing output to GFS twice: 2.5 GB/sec = 400 seconds
      Stragglers: 150 seconds? (Figure 3(a) bottom tail)
      The answer: the network accounts for 600 of 850 seconds

    Is it disappointing that sort harnesses only a small fraction of cluster CPU power?
      After all, only 200 of 800 seconds were spent sorting.
      If all they did was sort, they should sell CPUs/disks and buy a faster network.

    Modern data centers have relatively faster networks
      e.g. FDS's 5.5 terabits/sec cross-section b/w vs MR paper's 150 gigabits/sec
      while CPUs are only modestly faster than in MR paper
      so today bottleneck might have shifted away from net, towards CPU

    For what applications *doesn't* MapReduce work well?
      Small updates (re-run whole computation?)
      Small unpredictable reads (neither Map nor Reduce can choose input)
      Multiple shuffles (can use multiple MR but not very efficient)
        In general, data-flow graphs with more than two stages
      Iteration (e.g. page-rank)

    MapReduce retrospective
      Single-handedly made big cluster computation popular
        (though coincident with big datacenters, cheap machines, data-oriented companies)
      Hadoop is still very popular
      Inspired better successors (Spark, DryadLINQ, &c) 
```
