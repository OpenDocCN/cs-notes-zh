# "扁平数据中心存储" 案例研究

# 6.824 2015 年第 4 讲：“扁平数据中心存储”案例研究

**注意：**这些讲义内容与 6.824 [课程网站](http://nil.csail.mit.edu/6.824/2015/schedule.html)2015 年春季发布的讲义略有修改。

## 扁平数据中心存储

扁平数据中心存储，*Nightingale, Elson, Fan, Hofmann, Howell, Suzue*，OSDI 2012

### 我们为什么要看这篇论文？

+   当 Lab 2 长大后想要变成这样

    +   尽管细节各不相同

+   出色的性能 -- 世界纪录集群排序

+   良好的系统论文 -- 从应用程序到网络的细节

### FDS 是什么？

+   一个集群存储系统

+   存储巨大的 blob -- 128 位 ID，多兆字节的内容

+   客户端和服务器通过具有高叉带宽的网络连接

+   用于大数据处理（如 MapReduce）

    +   由数千台计算机组成的并行数据处理集群

### 高级设计 -- 一个常见的模式

+   很多客户端

+   很多存储服务器（"tractservers"）

+   任意两个服务器之间有很大的带宽

+   数据存储在 blob 中

    +   通过 128 位 ID 进行寻址

    +   进一步分割成 tracts

        +   从 0 开始编号

        +   8MB 大小的

+   对数据进行分区

+   主节点（"元数据服务器"）控制分区

+   为可靠性设置副本组

+   区段表定位器（TLT）存储了一堆条目

    +   在一个 `k` 副本系统中，每个条目都有 `k` 个 tractservers

+   如何找到 blob `b` 的 tract `t` 的位置？

    +   计算 TLT 条目为 `(h(b) + t) mod len(tlt)`

        +   您将获得该条目中的服务器列表

    +   blob 元数据是 *分布式的*，而不是存储在 TLT 中

+   如何从一个 blob 中写入一段（tract）？

    +   如上所述查找

    +   将写入发送到 TLT 条目中的所有服务器

    +   只有当 *所有* 服务器都回复时才向客户端确认写入

+   如何从一个 blob 中读取一段？

    +   如上所述查找

    +   将读取发送到 TLT 条目中的 *随机* 服务器

### 为什么这种高层设计是有用的？

+   成千上万的磁盘空间

    +   存储巨大的 blob，或者许多大的 blob

+   成千上万的服务器/磁盘/并行吞吐量

+   可以随时间扩展 -- 重新配置

+   大型存储服务器池，用于在故障后进行即时替换

### 动机应用：MapReduce 风格的排序

+   每个 mapper 读取其拆分的 `1/M` 部分的输入文件（例如，一个 tract）

    +   每个记录拆分发射一个 `<key, record>`

    +   将映射分区键分配到 `R` 个中间文件中（总共 `M*R` 个中间文件）

+   每个 reducer 读取每个 mapper 生成的 `R` 个中间文件中的 1 个

    +   读取 `M` 个中间文件（每个大小为 `1/R`）

    +   对其输入进行排序

    +   生成最终排序输出文件的 `1/R` 部分（`R` 个 blob）

+   FDS 排序

    +   FDS 排序不会将中间文件存储在 FDS 中

    +   客户端既是 mapper 也是 reducer

    +   FDS 排序不具有局部感知能力

        +   在 MapReduce 中，主节点将工作节点调度到靠近数据的机器上

        +   例如，在同一集群的后续版本中，FDS 排序使用更细粒度的工作分配，例如，mapper 不再获得输入文件的 1/N，而是获得一些更小的值，更好地处理 stragglers。

### 摘要的主要论点是关于性能的。

+   他们在 2012 年创造了磁盘到磁盘排序的世界纪录，称为 MinuteSort

    +   1,033 个磁盘和 256 台计算机（136 个区块服务器，120 个客户端）

    +   59.4 秒内 1,401 Gbyte

**问：** 摘要中的每个客户端 2 GByte/秒看起来很吸引人吗？

+   从 Athena AFS 读取文件有多快？（约 10 MB/秒）

+   你能以多快的速度读取典型硬盘？

+   典型网络能够多快地传输数据？

**问：** 摘要声称从丢失的磁盘（92 GB）恢复需要 6.2 秒，这是怎么做到的？

+   那是每秒 15 GByte

+   令人印象深刻吗？

+   那怎么可能？那是磁盘速度的 30 倍！

+   谁可能关心这个度量？

### 从这篇论文中我们应该想要了解什么？

+   应用程序编程接口？

+   布局？

+   查找数据？

+   添加一个服务器？

+   复制？

+   失败处理？

+   故障模型？

+   一致性读/写？（即读取是否看到最新写入？）

    +   FDS 中没有：“复制的当前协议取决于客户端向所有副本发出所有写入。这个决定意味着 FDS 为客户端提供了弱一致性保证。例如，如果客户端将一个区块写入 3 个副本中的一个，然后崩溃，读取该区块不同副本的其他客户端将观察到不同的状态。”

    +   “写入的顺序不能保证按发出顺序提交。具有排序要求的应用程序负责在收到先前确认后再发出操作，而不是同时发出。FDS 保证原子性：写入要么完全提交，要么完全失败。”

+   配置管理器的故障处理？

+   良好的性能吗？

+   对应用程序有用吗？

#### 应用程序编程接口

+   图 1

+   128 位 blob ID

+   blob 有一个长度

+   只有整个区块的读取和写入 -- 8 MB

**问：** 为什么 128 位 blob ID 是一个好接口？

+   为什么不使用文件名？

**问：** 为什么 8 MB 区块有意义？

+   （图 3...）

**问：** API 面向哪些客户端应用程序？

+   不是针对哪个方向？

#### 布局：它们如何在服务器上分布数据？

+   第 2.2 节

+   将每个 blob 分成 8 MB 区块

+   TLT 由元数据服务器维护

    +   有 `n` 个条目

    +   对于 blob `b` 和区块 `t`，`i = (hash(b) + t) mod n`

    +   `TLT[i]` 包含具有区块副本的区块服务器列表

+   客户端和服务器都有最新的 TLT 表副本

#### 无复制的四个条目 TLT 示例：

```
 0: S1
  1: S2
  2: S3
  3: S4
  suppose hash(27) = 2
  then the tracts of blob 27 are laid out:
  S1: 2 6
  S2: 3 7
  S3: 0 4 8
  S4: 1 5 ...
  FDS is "striping" blobs over servers at tract granularity 
```

**问：** 为什么要有区块？为什么不只在一个服务器上存储每个 blob？

+   什么样的应用程序将从分段中受益？

+   什么样的应用程序不会？

**问：** 客户端能够以多快的速度读取单个区块？

**问：** 摘要中的单客户端 2 GB 数字是从哪里得来的？

**问：** 为什么不采用 UNIX i-node 方法？

+   为每个区块存储一个数组，按区块号索引，得到区块服务器

+   因此，您可以做出每个区块的放置决策

    +   例如，将新的区块写入最轻负载的服务器

**问：** 为什么不是 `hash(b + t)`？

**问：** 应该有多少个 TLT 条目？

+   `n = 区块服务器数量` 怎么样？

+   为什么他们声称这样做效果不好？第 2.2 节

系统需要选择要放入 TLT 条目的服务器对（或三元组等）

+   用于复制

+   第 3.3 节

**问：** 关于：

```
 0: S1 S2
   1: S2 S1
   2: S3 S4
   3: S4 S3
   ... 
```

+   为什么这样做是一个坏主意？

+   修复需要多长时间？

+   如果两个服务器故障会有什么风险？

**Q:** 为什么论文的 `n²` 方案更好？

示例：

```
 0: S1 S2
   1: S1 S3
   2: S1 S4
   3: S2 S1
   4: S2 S3
   5: S2 S4
   ... 
```

+   具有 `n²` 条目的 TLT，每对服务器出现一次

+   修复需要多长时间？

+   如果两个服务器失败会有什么风险？

**Q:** 他们为什么实际上使用最低复制级别为 3？

+   与之前相同的 `n²` 表，第三个服务器是随机选择的

+   修复时间会有什么影响？

+   两个服务器失败会有什��影响？

+   如果三个磁盘失败会发生什么？

#### 添加磁道服务器

+   为了增加磁盘空间/并行吞吐量

+   元数据服务器选择一些随机的 TLT 条目

+   在这些 TLT 条目中为现有服务器替换新服务器

### 扩展磁道的大小

+   新创建的 blob 具有 0 个磁道的长度

+   应用程序在写入其末尾之前必须扩展 blob。

+   扩展操作是原子的，可以安全地与其他客户端并发执行，并返回客户端调用的结果中 blob 的新大小。

+   单独的 API 告诉客户端 blob 的当前大小。

+   对于一个 blob 的扩展操作被发送到拥有该 blob 元数据磁道的磁道服务器。

+   磁道服务器对其进行串行化，原子地更新元数据，并将新大小返回给每个调用者。

+   如果所有写入者都遵循这种模式，扩展操作将提供调用者可以写入而不冲突的一系列磁道。因此，扩展 API 在功能上等同于 Google 文件系统的“原子追加”。

+   磁道服务器上懒惰地分配空间，因此声明但未使用的磁道不会浪费存储空间。

#### 他们如何在服务器离开和加入时保持 `n²` 加一的排列？

不清楚。

**Q:** 添加磁道服务器需要多长时间？

**Q:** 当磁道正在传输时客户端 `write` 会发生什么？

+   接收磁道服务器可能具有来自客户端和旧服务器的副本

+   它如何知道哪个是最新的？

**Q:** 如果客户端读取/写入但具有旧的磁道表会发生什么？

+   磁道服务器告诉他

#### 复制

+   写入客户端向 TLT 中的每个磁道服务器发送一份副本。

+   读取客户端向一个磁道服务器询问。

**Q:** 为什么他们不通过主要发送写入？

+   给主要带来很多工作？必须查找并了解 TLT

+   目标不是只为主要备份，而是有效地在许多磁盘上复制和分割数据

**Q:** 由于缺乏主要内容，他们可能会遇到什么问题？

+   为什么这些问题不是致命的？

#### 磁道服务器失败后会发生什么？

+   元数据服务器停止接收心跳 RPC。

+   为每个 TLT 条目中失败服务器的随机替换选择

+   新的 TLT 获得新的版本号

+   替换服务器获取副本

每个服务器持有的磁道示例：

```
 S1: 0 4 8 ...
  S2: 0 1 ...
  S3: 4 3 ...
  S4: 8 2 ... 
```

**Q:** 为什么不只选择一个替代服务器？

+   它将不得不接收大量写入以获取丢失数据 `=>` 性能不佳。

**Q:** 复制所有磁道需要多长时间？

**Q:** 如果磁道服务器的网络中断然后修复，可能会提供旧数据吗？

**Q:** 如果服务器崩溃并重新启动，磁盘完好无损，内容可以使用吗？

+   例如，如果只错过了几次写入？

+   3.2.1 的“部分故障恢复”

+   但是它难道不已经被替换了吗？

+   如何知道它错过了哪些写入？

**问：** 何时更好地使用 3.2.1 的部分故障恢复？

#### 元数据服务器崩溃时会发生什么？

**问：** 当元数据服务器宕机时，系统可以继续吗？

+   是的，拥有 TLT 的客户端可以继续

**问：** 是否有备份元数据服务器？

+   不在论文中，他们说他们可能使用 Paxos 进行复制

+   **TODO：** 不清楚为什么复制元数据服务器会导致一致性问题

**问：** 重启的元数据服务器如何获得 TLT 的副本？

+   嗯，也许它在磁盘上？

+   也许它只是简单地从所有的心跳中重建？

**问：** 他们的方案看起来正确吗？

+   元数据服务器如何知道它已经收到了所有的 tractservers 的信息？

    +   它不会，它只会在它们发送心跳时添加服务器

+   它怎么知道所有的 tractservers 都是最新的？

    +   **TODO：** 与什么是最新的？

#### 随机问题

**问：** 元数据服务器可能会成为一个瓶颈吗？

+   很难说。使用案例是什么？

+   如果你有一个客户端记住了 TLT 那么他只联系元数据服务器一次，然后开始做他所有的读写

+   如果有很多客户端加入系统，或者回来但忘记了 TLT（可能是因为存储不足），那么元数据服务器将会被大量使用

    +   然而，一旦客户端下载了 TLT，这不会影响客户端获得的带宽

**问：** 为什么他们需要在 2.3 中提到的清洗程序？

+   为什么他们在 blob 被删除时不删除 tracts？

    +   执行 GC 比调度和执行删除更快吗？

+   在删除后可以写入 blob 吗？

    +   **TODO：** 不确定，似乎是的，因为该 blob 的元数据位于 tract -1 中，我认为 `WriteTract` 不会在每次写入之前检查元数据，所以你可能会有竞争

#### 性能

**问：** 我们如何知道我们看到了“良好”的性能？你能期望的最好的是什么？

+   你能期望的最好的是利用每个磁盘的带宽，使系统的带宽为 `# of disk * disk bandwidth`

**问：** 单客户端 2 GBps 的限制资源？

+   假设这是 5.2 的结束

+   30 个 tractservers 意味着最多 30 * 130MB/s = 3.9GBps

+   所以限制资源是网络带宽

**问：** 图 4a：为什么开始低？为什么上升？为什么水平稳定？为什么在特定性能水平上水平稳定？

+   由于单个客户端的带宽受限，所以开始较低

+   上升是因为随着客户端数量的增加，每个客户端都会向系统添加更多的带宽

+   水平稳定是因为在某一点上客户端带宽 > 服务器的带宽

+   为什么在拥有 516 个磁盘的 `x` 客户端上水平稳定在 32 GBps？

    +   图 3 表明 10,000 转/分钟的硬盘可以以大约 130MB/s 的速度读取 5MB 的块

        +   写入类似

    +   从对数刻度图中不清楚 `x` 是什么

        +   `10 < x < 50`（也许 `25 < x < 50`？）

    +   `516 disks * 130MB/s = 67 GBps`，所以看起来最好的情况下性能应该在 32 GBps 以上平稳？

        +   实际上并非所有的磁盘都是 130MB/s 可能？（只有 10,000 转/分钟的 SAS 才那么快）

        +   实际上，单个节点上的多个磁盘可能会使这个数字变小，也许？

        +   无论如何，大约像 `x=40` 个客户端会有 `40 * 10Gbps = 40 * 1.25GBps = 50 Gbps`，这比服务器实际（声称的）带宽 32 GBps 更高

**问：** 图 4b 显示随机读写与顺序读写一样快（图 4a）。这是你期望的吗？

+   是的。不同磁道的随机读写请求会发送到不同的服务器，就像顺序请求一样 `=`> 没有区别

**问：** 在图 4c 中，为什么带有复制的写入比读取慢？

+   一次写入发送到所有磁道服务器？直到它们全部回复为止。

    +   具有更多客户端写入的情况下 `=>` 每台服务器完成的工作更多

    +   论文中提到："正如预期的那样，写入带宽约为读取带宽的三分之一，因为客户端必须发送每个写入的三个副本"

+   一次读取只发送到一台？

**问：** 6.2 秒内的 92 GB 是从哪里来的？

+   表 1，第四列

+   那是每秒 15 GB，读和写都是

+   1000 块磁盘，三重复制，128 台服务器？

+   限制资源是什么？磁盘？CPU？网络？

每个排序桶有多大？

+   也就是说，每个桶的排序都在内存中吗？

+   总共 1400 GB

+   128 台计算服务器

+   每台服务器的 RAM 在 12 到 96 GB 之间

+   嗯，平均说 50，所以总 RAM 可能是 6400 GB

+   因此每个桶的排序都在内存中，不会将写入传递给 FDS

+   因此总时间只是 1400 GB 的四次传输

    +   客户端限制：`128 * 2 GB/s = 256 GB / 秒`

    +   磁盘限制：`1000 * 50 MB/s = 50 GB / 秒`

+   因此瓶颈很可能是磁盘吞吐量
