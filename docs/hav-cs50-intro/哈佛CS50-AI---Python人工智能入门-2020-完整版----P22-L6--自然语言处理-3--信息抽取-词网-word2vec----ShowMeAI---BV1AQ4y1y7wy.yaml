- en: 哈佛CS50-AI ｜ Python人工智能入门(2020·完整版) - P22：L6- 自然语言处理 3 (信息抽取，词网，word2vec) - ShowMeAI
    - BV1AQ4y1y7wy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 哈佛CS50-AI ｜ Python人工智能入门(2020·完整版) - P22：L6- 自然语言处理 3 (信息抽取，词网，word2vec) - ShowMeAI
    - BV1AQ4y1y7wy
- en: and we don't care about the order now，when we get into the world of semantics。we
    really do start to care about what it。![](img/819284bd8940912d2ebc94c72b16d584_1.png)
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们不关心顺序，当我们进入语义世界时。我们确实开始关心这些词的实际含义！[](img/819284bd8940912d2ebc94c72b16d584_1.png)
- en: is that these words actually mean how it，is these words relate to each other
    and。in particular how we can extract，information out of that text information。extraction
    is somehow extracting，knowledge from our documents figuring。out given a whole
    bunch of text can we，automate the process of having an AI。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这些词如何相互关联，特别是我们如何从文本中提取信息。信息抽取就是从我们的文档中提取知识，弄清楚给定一大堆文本，我们能否自动化AI的过程。
- en: look at those documents and get out what。![](img/819284bd8940912d2ebc94c72b16d584_3.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这些文件，提取出什么。![](img/819284bd8940912d2ebc94c72b16d584_3.png)
- en: they useful or relevant knowledge inside，those documents happens to be so let's。take
    a look at an example I'll give you，two samples from news articles here up。above
    is a sample of a news article from，the Harvard Business Review。there was about
    Facebook down below is，an example of a Business Insider article。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文件中有用或相关的知识是，因此让我们看一个例子。我将给你两个来自新闻文章的样本，上面是《哈佛商业评论》的新闻文章样本，关于Facebook，下面是《商业内幕》的文章示例。
- en: from 2018 that was about Amazon and。![](img/819284bd8940912d2ebc94c72b16d584_5.png)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 关于亚马逊的2018年信息。![](img/819284bd8940912d2ebc94c72b16d584_5.png)
- en: there's some information here that we，might want an AI to be able to extract。information
    knowledge about these，companies we might want to extract and。in particular what
    I might want to，extract is let's say I want to know data。about when companies
    were founded but I，want to know that Facebook was founded。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些信息，我们可能希望AI能够提取。关于这些公司的知识，我们可能想要提取，特别是我想提取的数据，比如我想知道公司的成立时间，但我想知道Facebook是在。
- en: in 2004 and was on founded in 1994 that，that is important information that I。happen
    to care about well how do I，extract that information from the text。what is my
    way of being able to，understand this text and figure out all。right Facebook was
    founded in 2004 well，what I can look for are templates or。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在2004年成立，1994年成立的重要信息，我对此非常关心。我该如何从文本中提取这些信息？我如何理解这段文字并弄清楚所有内容？好吧，Facebook是在2004年成立的，我可以寻找模板或。
- en: patterns seen seen things that happen to，show up across multiple different。documents
    that give me some sense for，what this knowledge happens to mean and。we'll notice
    is a common pattern between，both of these passages which is this。phrasing here
    when Facebook was founded，in 2004 comma，and then down below when Amazon was。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式出现在多个不同文档中，给我对这种知识的含义有一些理解。我们会注意到这两段之间的一个共同模式，就是这种表达：“Facebook是在2004年成立的，”然后下面是“亚马逊是在。”
- en: founded in 1994 ，and those two templates end up giving us，a mechanism for trying
    to extract。information that this notion when，company was founded in year comma
    this。![](img/819284bd8940912d2ebc94c72b16d584_7.png)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 成立于1994年，而这两个模板最终为我们提供了一种提取信息的机制，即当公司在年份成立时，逗号。这个！[](img/819284bd8940912d2ebc94c72b16d584_7.png)
- en: can tell us something about when a，company was founded because if we set。our
    a I loose on the web let it look at，a whole bunch of papers or a whole bunch。of
    article ism and it finds this pattern，when blank was founded in blank comma。well
    then our a I can pretty reasonably，conclude there's a good chance that this。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 能告诉我们某个公司成立时间的信息，因为如果我们让AI在网上搜索，让它查看一大堆论文或文章，并找到这种模式：“在blank成立于blank，”，那么我们的AI可以合理地得出结论。
- en: is going to be like some company and，this is going to be like the year that。company
    was founded for example might，not be perfect but at least it's a good。heuristic
    and so you might imagine that，if you wanted to train an AI to be able。to look
    for information you might give，the AI templates like this not only give。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是某个公司的名称，而这将是该公司成立的年份。例如，可能不是完美的，但至少这是一个好的启发式方法。所以你可以想象，如果你想训练一个AI来查找信息，你可以给AI这样的模板，不仅提供。
- en: it a template like when company blank，was founded in blank but give it like。the
    book blank was written by a blank，for example just give it some templates。where
    it can search the web search a，whole big corpus of documents looking。for templates
    that match that and if it，finds that then it's able to figure out。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的想法是，人工智能可以开始寻找模板，比如“当公司____成立于____”，但也可以给它类似“书籍____是由____编写的”的模板，例如，给它一些模板，让它在网络上搜索，查找与之匹配的文档。
- en: all right here's the company and here's，the year but of course that requires
    us。to write these templates it requires us，to figure out like what is the structure。of
    this information likely going to look，like and it might be difficult to know。that
    different websites are of course，going to do this differently this type。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这里是公司和年份，但这当然需要我们编写这些模板，需要我们弄清楚这些信息的结构可能是什么样子的，而不同的网站显然会以不同的方式处理这种类型。
- en: '![](img/819284bd8940912d2ebc94c72b16d584_9.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/819284bd8940912d2ebc94c72b16d584_9.png)'
- en: of method isn't going to be able to，extract all of the information because。if
    the words are slightly in a different，order it won't match on that particular。template
    but one thing we can do is。![](img/819284bd8940912d2ebc94c72b16d584_11.png)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法无法提取所有的信息，因为如果词语的顺序稍微不同，它就无法匹配特定的模板，但我们可以做的一件事是。！[](img/819284bd8940912d2ebc94c72b16d584_11.png)
- en: rather than give our AI the template we，can give a I'd the data we can tell
    the。AI you know Facebook was founded in 2004，and Amazon was founded in 1994 and
    just。how the AI those two pieces of，information and then set the AI loose on。the
    web and now the idea is that the AI，can begin to look for where to Facebook。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将数据提供给人工智能，而不是给人工智能模板。我们可以告诉人工智能，比如说Facebook成立于2004年，亚马逊成立于1994年，然后让人工智能处理这两条信息，然后在网络上自由搜索。
- en: in 2004 show up together where to Amazon，in 1994 show up together and it can。discover
    these templates for itself it，can discover that this kind of phrasing。when blank
    was founded in blank tends to，relate Facebook to 2004 and it relates。Amazon to
    1994 so maybe it will hold the，same relation for others as well and。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在2004年，Facebook和1994年的亚马逊可以一起出现，它可以自行发现这些模板，它可以发现“当____成立于____”这种措辞往往将Facebook与2004年关联，将亚马逊与1994年关联，因此也许它会将其他公司与此类关系联系起来。
- en: this ends up being this kind of，automated template generation ends up。being
    quite powerful and we'll go ahead，and take a look at that now as well what。I have
    here inside of templates，directory is a file called company in。CSV and this is
    all of the data that I，am going to give to my AI I'm gonna give。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这最终成为一种自动化模板生成的方式，结果相当强大。我们现在就来看看我在模板目录中有一个名为公司（company）的CSV文件，这就是我将要提供给我的人工智能的所有数据。
- en: it the pair Amazon 1994 and Facebook，2004 and what I'm going to tell my a AI。to
    do is search a corpus of documents，for other data these pears like this。other
    relationships I'm not telling the，eye that this is a company and the date。that
    it was founded I'm just giving it，Amazon 1994 and Facebook 2004 and。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这是亚马逊1994年和Facebook 2004年，而我将告诉我的人工智能去搜索文档语料库中的其他数据，这些对，比如其他的关系。我并没有告诉人工智能这是一家公司及其成立的日期，我只是提供亚马逊1994年和Facebook
    2004年。
- en: letting the AI do the rest and what the，AI is going to do is it's going to look。through
    my corpus here's my corpus of，documents and it's gonna find like。inside of Business
    Insider that we have，sentences like back when Amazon was。founded in 2004 comma
    and that kind of，phrasing is going to be similar to like。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让人工智能做其余的工作，人工智能将会查看我的语料库，这是我的文档语料库，它将找到像在《商业内幕》中的句子，比如“亚马逊成立于2004年”，这种措辞将会类似于。
- en: this Harvard Business Review story that，has a sentence like when Facebook was。founded
    in 2004 and it's going to look，across a number of other documents for。similar
    types of patterns to be able to，extract that kind of information and。![](img/819284bd8940912d2ebc94c72b16d584_13.png)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果找到匹配，它就能够识别出这样的信息，比如《哈佛商业评论》的一个故事，其中有句子“Facebook成立于2004年”，它将会在其他许多文档中寻找类似的模式，以提取这种信息。！[](img/819284bd8940912d2ebc94c72b16d584_13.png)
- en: what it'll do is if I go ahead and run，I'll go ahead and go into templates so。I'll
    say Python search PI I'm going to，look for the data like the data in。company of
    CSV inside of the company's，directory which contains a whole bunch。of news articles
    that I've curated in，advance and here's what I get Google in。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我继续运行，我会进入模板。我会说Python搜索PI，我将要。查找公司目录中像CSV那样的数据，里面包含了我提前策划的一大堆新闻文章。这里是我得到的关于谷歌的内容。
- en: '![](img/819284bd8940912d2ebc94c72b16d584_15.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/819284bd8940912d2ebc94c72b16d584_15.png)'
- en: 1998 Apple 976 Microsoft 1975 so on and，so forth Walmart 1962 for example these。are
    all of the pieces of data that，happened to match that same template。that we were
    able to find before and how，was it able to find this well it's。probably because
    if we look at like the，Forbes article for example that it has a。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 1998年苹果 976 微软 1975等等，沃尔玛1962年，例如这些。都是与我们之前能够找到的相同模板相匹配的数据。它是如何找到这些的呢？很可能是因为如果我们查看像是，福布斯文章的例子，它有一个。
- en: phrase in it like when Walmart was，founded in 1962 comma that it's able to。identify
    these sorts of patterns and，extract information from them now。granted I have curated
    all these stories，in advance in order to make sure that。there is data that is
    able to match on，and in practice it's not always going to。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 像“沃尔玛成立于1962年”的短语，它能够。识别这些类型的模式并，从中提取信息。现在，当然，我已经提前策划了所有这些故事，以确保。能够匹配的数据，而实际上并不总是。
- en: be in this exact format when you're，seeing a company of related to the year。in
    which it was founded but if you give，the AI access to enough data like all of。the
    data of text on the internet and，just have the AI crawl the internet。looking for
    information it can very，reliably or with some probability try。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当你看到与成立年份相关的公司时，必须以这个确切的格式出现。但如果你给人工智能足够的数据，比如互联网上的所有文本数据，并且让人工智能爬行互联网。寻找信息，它可以非常可靠或以某种概率尝试。
- en: and extract information you，these sorts of templates and be able to。generate
    interesting sense of knowledge，and the more knowledge it learns the。more new templates
    is able to construct，looking for constructions that show up。in other locations
    as well so let's take，a look at another example。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 并提取信息，你，这些类型的模板能够。生成有趣的知识感知，而它学习的知识越多，。它能够构建的新模板就越多，寻找在其他地方出现的构造。让我们来看另一个例子。
- en: '![](img/819284bd8940912d2ebc94c72b16d584_17.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/819284bd8940912d2ebc94c72b16d584_17.png)'
- en: then I'll here show you president's CSV，where I have two presidents and their。inauguration
    dates are George Washington，1789 Barack Obama 2009 for example and I。also am going
    to give to our AI a corpus，that just contains a single document。which is the Wikipedia
    article for the，list of presidents of the United States。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我会向你展示总统的CSV，其中我有两位总统和他们的就职日期：乔治·华盛顿1789年，巴拉克·奥巴马2009年，例如，我。还将给我们的人工智能一个语料库，里面只包含一份文件。即美国总统名单的维基百科文章。
- en: for example just information about。![](img/819284bd8940912d2ebc94c72b16d584_19.png)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，仅仅是关于的信息。![](img/819284bd8940912d2ebc94c72b16d584_19.png)
- en: presidents and I'd like to extract from，this like raw HTML document on a web。page
    information about the President so，I can say search in president's ESV。president's
    and what I get is a whole，bunch of data about presidents and what。year they were
    likely inaugurated in by，looking for patterns that matched Barack。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 总统，我想从这个网页上的原始HTML文档提取关于总统的信息，因此。 我可以说搜索总统的CSV。总统，我得到的是一大堆关于总统的数据，和他们可能的就职年份，通过寻找与巴拉克·奥巴马2009年匹配的模式。
- en: Obama 2009 for example looking for these，sorts of patterns that happen to give
    us。some clues as to what it is that a story，happens to be about so here's another。![](img/819284bd8940912d2ebc94c72b16d584_21.png)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，寻找这些类型的模式，给我们一些线索，了解这个故事。大致是关于什么的，这里是另一个。![](img/819284bd8940912d2ebc94c72b16d584_21.png)
- en: example if I open up inside the Olympics，here is a scrape version of the Olympic。homepage
    that has information about，various different Olympics and maybe I。![](img/819284bd8940912d2ebc94c72b16d584_23.png)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我打开奥运会内部，这里是奥运会主页的抓取版本，其中有关于。各种不同奥运会的信息，也许我。![](img/819284bd8940912d2ebc94c72b16d584_23.png)
- en: want to extract like Olympic locations，and years from this particular page。well
    the way I can do that is using the，exact same algorithm I'm just saying。alright
    here are two Olympics and where。![](img/819284bd8940912d2ebc94c72b16d584_25.png)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 想要从这个特定页面提取奥运会地点和年份。好吧，我可以用完全相同的算法来做到这一点，我只是说。好吧，这里有两个奥运会以及它们的地点。![](img/819284bd8940912d2ebc94c72b16d584_25.png)
- en: they were located so 2012 London for，example let me go ahead and just run in。this
    process Python search on Olympics，CSV look at all the Olympic data set and。here
    I get some information back now，this information not totally perfect。![](img/819284bd8940912d2ebc94c72b16d584_27.png)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的位置信息，比如2012年伦敦，比如让我运行这个Python搜索，查看奥运会的CSV，查看所有奥运数据集，现在我得到一些信息返回，虽然这些信息并不完全完美。![](img/819284bd8940912d2ebc94c72b16d584_27.png)
- en: there are a couple of examples that are，obviously not quite right because my。template
    might have been a little bit，too general like maybe it was looking。for a broad
    category of things and，certain strange things happen to capture。on that particular
    template so you can，imagine adding rules to try and make。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个例子显然不太正确，因为我的模板可能太笼统，比如可能在寻找某个广泛类别的事物，而某些奇怪的事情恰好捕捉到了这个特定模板，因此你可以想象添加规则来尝试使。
- en: this process more intelligent making，sure the thing on the left is just a。year
    for example for instance and doing，other sorts of analysis but purely just。based
    on some data we are able to，extract some interesting information。using some algorithms
    and all search pi，is really doing here is it is taking my。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程更加智能，确保左侧的东西只是一个年份，例如，进行其他各种分析，但纯粹只是基于某些数据，我们能够提取一些有趣的信息。使用某些算法，所有搜索实际上就是在进行数据的提取。
- en: corpus of data finding templates that，match it，here I'm filtering down to just
    look up。like the top two templates that happen，to match and then using those templates。to
    extract results from the data that I，have access to being able to look for。all
    the information that I care about，and that's ultimately what's going to。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据语料库，找到与之匹配的模板，这里我正在过滤，只查看恰好匹配的前两个模板，然后使用这些模板从我能够访问的数据中提取结果，能够寻找我关心的所有信息，而这最终将会。
- en: '![](img/819284bd8940912d2ebc94c72b16d584_29.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/819284bd8940912d2ebc94c72b16d584_29.png)'
- en: help me to print out those results to，figure out what the matches happen to
    be。and so information extraction is another，powerful tool when it comes towards。trying
    to extract information but of，course it only works in very limited。context it
    only works when I'm able to，find templates that look exactly like。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 帮我打印出这些结果，以找出匹配项。因此，信息提取是另一个强大的工具，旨在提取信息，但当然它仅在非常有限的上下文中有效。它只在我能够找到完全相似的模板时有效。
- en: this in order to come up with some sort。![](img/819284bd8940912d2ebc94c72b16d584_31.png)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做是为了得出某种结果。![](img/819284bd8940912d2ebc94c72b16d584_31.png)
- en: of match that is able to connect this to，some pair of data that this company
    was。founded in this year what I might want，to do is we start to think about the。semantics
    of words and is to begin to，imagine some way of coming up with。definitions for
    all words being able to，relate all of the words in a dictionary。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配能够将其连接到某对数据的，这家公司成立于这一年。我想做的是开始考虑词语的语义，并设想一些定义所有词语的方式，能够将字典中的所有词关联起来。
- en: to each other because that's ultimately，what's going to be necessary if we want。our
    AI to be able to communicate we need，some representation of what it is that。words
    mean and one approach of doing，this as famous data set called word net。and word
    net is is the human curated，researchers have curated together a。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 彼此之间，因为这最终是必要的，如果我们希望我们的人工智能能够沟通，我们需要某种表示来表达词语的含义，而一个著名的数据集叫做词网（WordNet）。词网是人类策划的，研究人员共同策划了一个。
- en: whole bunch of words their definitions。![](img/819284bd8940912d2ebc94c72b16d584_33.png)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一大堆词语及其定义。![](img/819284bd8940912d2ebc94c72b16d584_33.png)
- en: their various different senses because a，word might have multiple different。meanings
    and also how those words relate。![](img/819284bd8940912d2ebc94c72b16d584_35.png)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的各种不同含义，因为一个词可能有多个不同的意思，以及这些词之间的关系。![](img/819284bd8940912d2ebc94c72b16d584_35.png)
- en: to one another and so what we mean by，this is I can show you an example of。word
    net word net comes built into n ltk，using NLT k you can download and access。word
    net and so let me go into word net，and go ahead and run word net and。extract information
    about a word a word，like city for example go to impress。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 彼此相关，因此我们的意思是，我可以向你展示一个词网的例子，词网内置于NLTK中，使用NLTK你可以下载并访问词网，所以让我进入词网，并运行词网，提取一个词的信息，比如“城市”这个词，去查看它的印象。
- en: return and here is the information that，I get back about a city it turns out。that
    city has three different senses，three different meanings according to。word net
    and it's really just kind of，like a dictionary where each sense is。associated
    with its meaning just some，definition provided by human and then。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的信息是，我关于城市的信息，结果发现，城市有三种不同的感知，根据WordNet有三种不同的含义，这其实就像一本字典，每种感知都有其对应的含义，定义由人提供。
- en: it's also got categories for example，that a word belongs to that a city is a。type
    of municipality a city is a type of，administrative district and that allows。me
    to relate words to other words so one，of the powers of wordnet is the ability。to
    like take one word and connect it to，other related words that we might be。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个单词所属的类别，一个城市是一种市政类型，城市是一种行政区，这让我能够将单词与其他单词关联起来。因此，WordNet的一个优势是能够将一个单词与我们可能感兴趣的其他相关单词连接起来。
- en: able to say for example if I do another，example let me try the word house。since
    I'll type in the word house and，see what I get back well all right the。house is
    a kind of building the house is，somehow related to like a family unit。and so you
    might imagine trying to come，up with these various different ways of。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我再做一个例子，试试“房子”这个词。我输入“房子”，看看我能得到什么，好的，房子是一种建筑，房子在某种程度上与家庭单位相关。因此，你可能想象出这些不同的关联方式。
- en: describing houses it is a building and，as a dwelling and researchers have just。curated
    this these relationships between，these various different words to say。![](img/819284bd8940912d2ebc94c72b16d584_37.png)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个建筑和居住地，研究人员已经整理了这些不同单词之间的关系，以说明。![](img/819284bd8940912d2ebc94c72b16d584_37.png)
- en: that a house is a type of building that，a house is a type of dwelling for。example
    but this type of approach while，certainly helpful for being able to。relate words
    to one another doesn't，scale particularly well as you start to。think about language
    changing as you，start to think about all the various。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，房子是一种建筑，房子是一种居住地。但这种方法虽然在能够将单词关联起来时非常有帮助，但随着语言的变化以及各种变化的思考，它并不特别适用。
- en: different relationships that words might，have to one another this challenge
    of。word representation ends up being，difficult but what we've done is just。defined
    a word yes just like a sentence，that explains what it is that that word。is but
    what we really would like in some，way to represent the meaning of a word。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 单词之间可能存在的不同关系。这一单词表示的挑战最终变得困难，但我们所做的就是定义一个单词，就像一句话那样，解释这个单词的意思。但我们真正希望以某种方式表示一个单词的意义。
- en: '![](img/819284bd8940912d2ebc94c72b16d584_39.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/819284bd8940912d2ebc94c72b16d584_39.png)'
- en: in a way that our AI is going to be able，to do something useful with that anytime。we
    want our AI to be able to look at，text and really understand what that。text means
    to relate text and words to，similar words and understand the。relationship between
    words we'd like，some way that a computer can represent。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们的人工智能能够以某种方式，随时利用这些信息做有用的事情。我们希望我们的人工智能能够看懂文本，并真正理解这些文本的含义，将文本和单词与相似的单词关联起来，并理解单词之间的关系，我们希望有某种方式让计算机能够表示。
- en: this information and what we've seen all，throughout the course multiple times
    now。is the idea that when we want our AI to，represent something it can be helpful
    to。have the AI represent it using numbers，that we've seen that we can represent。utilities
    in a game like winning or，losing or drawing as a number one。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在整个课程中多次看到的信息是，当我们希望我们的人工智能表示某些东西时，使用数字来表示可能是有帮助的。我们看到，我们可以将游戏中的效用（如胜利、失败或平局）表示为数字1。
- en: negative one or two zero we've seen，other ways that we can take data and。turn
    it into like a vector of features，where we just have a whole bunch of。numbers
    that represent some particular，piece of data and if we ever want to。pass words
    into a neural network for，instance to be able to say given some。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 负数、一、二、零，我们已经看到了其他方式，可以将数据转换为特征向量，其中我们有一堆数字表示某些特定的数据。如果我们想将单词输入到神经网络中，例如能够给定一些描述房子的内容。
- en: word translate this sentence into，another sentence or to be able to do。interesting
    classifications with neural，networks on individual words we need。some representation
    of words just in，terms of vectors way to represent two。words just by using individual
    numbers，do we do that，how do we take words and turn them into。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个句子翻译成另一个句子，或者能够用神经网络对单个词进行有趣的分类，我们需要某种词的表示，仅仅是通过向量的方式来表示两个词。我们该如何做到呢？我们如何将词转换成向量？
- en: vectors that we can use to represent the，meaning of those words well one way
    is。to do this if I have four words and I，want to encode like he wrote a book。I
    can just say let's let the word he be，this vector 1 0 0 0 wrote will be 0 1 0。2
    0 a will be 0 0 1 0 book will be 0 0 0，1 effectively what I have here is what's。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用来表示这些词含义的一种方法是，如果我有四个词，我想编码“他写了一本书”。我可以说，让“他”是这个向量`1 0 0 0`，写是`0 1 0 0`，而“书”则是`0
    0 0 1`。实际上，我所拥有的就是这样的表示。
- en: known as a one hot representation or a，one hot encoding which is a。representation
    of meaning where meaning。![](img/819284bd8940912d2ebc94c72b16d584_41.png)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为**独热表示**或**独热编码**，它是一种含义表示，其中的含义。
- en: is a vector that has a single one in it，and the rest are zeros the location
    of。the 1 tells me the meaning of the word，that one in the first position that。means
    he 1 in the second position that，means wrote and every word in the。dictionary
    is going to be assigned to，some representation like this where we。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个只在一个位置有1，其他位置全是0的向量。1的位置告诉我这个词的含义，在第一个位置的1意味着“他”，在第二个位置的1意味着“写”，字典中的每个词都会被分配一个这样的表示。
- en: just assign one place in the vector that，has a 1 for the word and 0 for the
    other。words and now I have representations of，words that are different for a whole。bunch
    of different words this is this，one hot representation so what are the。drawbacks
    of this why is this not，necessarily a great approach well here。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 只需在向量中分配一个位置，那个位置对这个词为1，其他词为0。现在我有了不同词的表示，这就是所谓的**独热表示**。那么，这种方法有什么缺点呢？为什么这并不一定是一个好的方法呢？
- en: I'm only creating enough vectors to，represent four words in a dictionary。if
    you imagine a dictionary with you，know 50，000 words that I might want to。represent
    now these vectors get enormous，ly long these are 50，000 dimensional。vectors to
    represent a vocabulary of，50，000 words that you know he is 1。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我只创建了足够的向量来表示字典中的四个词。如果你想象一个包含50,000个词的字典，我可能想表示的这些向量会变得异常庞大，这些是50,000维的向量，用来表示50,000个词的词汇。
- en: followed by all these wrote has a whole，bunch of zeros in it that's not a。particularly
    tractable way of trying to，represent numbers if I'm gonna have to。deal with you
    know vectors of length，50，000 another problem a subtler problem。is that ideally
    I'd like for these，vectors to somehow represent meaning in。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 后面的“写”有一大堆零，这不是一个特别易于处理的方式，如果我必须处理长度为50,000的向量。另一个问题是，我希望这些向量在某种程度上能够表示它们之间的含义。
- en: a way that I can extract useful，information out of that if I have the。sentence
    he wrote a book and he authored，a novel well wrote and authored are。going to be
    two totally different，vectors and book and novel are going to。be two totally different
    vectors inside，of my vector space that have nothing to。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我有句子“他写了一本书”和“他创作了一部小说”，我可以从中提取有用的信息。这里“写”和“创作”将是两个完全不同的向量，而“书”和“小说”在我的向量空间中也会是两个完全不同的向量，它们之间没有任何关系。
- en: do with each other the one is just，located in a different position and。really
    what I would like to have happen，is for wrote and authored to have。vectors that
    are similar to one another，and for book and novel to have vector。representations
    that are similar to one，another because they're words that have。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个词只是位于不同的位置，而我真正希望的是“写”和“创作”有相似的向量表示，“书”和“小说”的向量表示也应该相似，因为它们是含义相关的词。
- en: similar meanings because their meanings，are similar ideally I'd like for when
    I。put them in vector form and use a vector，to represent meaning，I would like for
    those vectors to be。similar to one another as well so rather，than this one hot
    representation where。we represent a words meaning by just，giving it a vector that
    is 1 in a。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我希望当我将它们放入向量形式并用向量来表示含义时，这些向量彼此之间也应相似。因此，与其使用这种独热表示，即通过给一个词一个在某个位置为1的向量来表示其含义，
- en: particular location what we're going to，do which is a bit of a strange thing
    the。first time you see it is what we're，going to call a distributed。representation
    we are going to represent，the meaning of a word as just a whole。bunch of different
    values not just a，single one in the rest zeroes but a。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在特定位置，我们将要做的事情是一个有点奇怪的事情，第一次看到时会觉得陌生，这就是我们要称之为的分布式表示，我们将用一大堆不同的值来表示一个词的意义，而不仅仅是一个值和其余的零。
- en: whole bunch of values so for example in，he wrote a book he might just be a big。vector
    maybe it's 50 dimensions maybe，it's a hundred dimensions but certainly。less than
    like tens of thousands where，each value is just you know some number。in same thing
    for wrote an A and book，and the idea now is that using these。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一大堆数值，比如他写了一本书，可能是一个大的向量，也许有50维，也许有100维，但肯定不会是数万维，每个值都是一个数字。写了一本书的想法是用这些。
- en: vector representations I'd hope that，wrote and authored have vector。representations
    that are pretty close to，one another their distance is not too。far apart and same
    with the vector，representations for book and novel so。this is going to be the
    goal of a lot of，what statistical machine learning。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 向量表示，我希望“写作”和“作者”这两个词的向量表示非常接近，它们之间的距离不会太远，书和小说的向量表示也是如此。因此，这将是许多统计机器学习的目标。
- en: approaches the natural language，processing is about is using these。vector representations
    of words but how，on earth do we define a word is just。like a whole bunch of these
    sequences of，numbers like what does it even mean to。like talk about the meaning
    of a word，well to answer the question the famous。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理的方法就是使用这些词的向量表示，但我们究竟该如何定义一个词呢？它就像是一串串数字，那么谈论一个词的意义到底是什么意思呢？对此，著名的。
- en: quote that answers this question is from，a British linguist on the 1950s jr。Firth
    who said you shall know a word by。![](img/819284bd8940912d2ebc94c72b16d584_43.png)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这个问题的引用来自20世纪50年代的英国语言学家J.R. Firth，他说：“你将通过……了解一个词。”![](img/819284bd8940912d2ebc94c72b16d584_43.png)
- en: the company it keeps you shall know a，word by the company it keeps and what
    we。mean by that is the idea that we can，define a word in terms of the words that。show
    up around it that we can get at the，meaning of a word based on the context。in
    which that word happens to appear and，that if I have a sentence like this for。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过与之相伴的词来了解一个词的意义，我们的意思是，定义一个词的方式就是根据那些出现在它周围的词来确定这个词的意义，如果我有这样一个句子。
- en: words and sequence for blank he ate what，goes in the blank。![](img/819284bd8940912d2ebc94c72b16d584_45.png)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 他吃什么的空白处需要填的词。![](img/819284bd8940912d2ebc94c72b16d584_45.png)
- en: well you might imagine that in English，the types of words that might fill the。blank
    are words like breakfast or lunch，or dinner these are the kinds of words。that
    fill in that blank and so if we，want to define you know what does lunch。or dinner
    mean we can define it in terms，of what words happen to show up around。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想象，在英语中，填补空白的词可能是像早餐、午餐或晚餐这样的词，这些就是填入那个空白的词。因此，如果我们想定义午餐或晚餐的意义，可以通过它们周围出现的词来定义。
- en: it that if a word shows up in a，particular context and another word。happens
    to show up in very similar，context then those two words。![](img/819284bd8940912d2ebc94c72b16d584_47.png)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个词在特定上下文中出现，而另一个词恰好在非常相似的上下文中出现，那么这两个词就有关系。![](img/819284bd8940912d2ebc94c72b16d584_47.png)
- en: are probably related to each other they，probably have a similar meaning to one。another
    and this then is the。![](img/819284bd8940912d2ebc94c72b16d584_49.png)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 它们可能彼此相关，可能具有相似的意义，这就是。![](img/819284bd8940912d2ebc94c72b16d584_49.png)
- en: foundational idea of an algorithm known，as word to Veck which is a model for。generating
    word vectors you give word，Tyvek a corpus of documents just a whole。bunch of text
    and what word Tyvek will，produce is it will produce vectors for。each word and
    there are number of ways，that it can do this one common way is。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一个被称为“词向量”的算法的基础思想是生成词向量的模型，你给“词向量”一个文档语料库，也就是一大堆文本，而“词向量”将为每个词生成向量，有多种方法可以做到这一点，一种常见的方法是。
- en: through what's known as the Skip Graham，architecture which basically uses a。neural
    network to predict context words，given a target word so given a word like。lunch
    use a neural network to try and，predict given the word lunch what words。are going
    to show up around it and this，is the way we might represent this is。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通过被称为Skip-gram架构的方法，该架构基本上使用神经网络预测上下文单词，给定一个目标单词，因此，给定一个单词，如“午餐”，使用神经网络尝试预测给定单词“午餐”周围将出现哪些单词，这就是我们可能表示的方式。
- en: with a big neural network like this，where we have one input cell for every。word
    every word gets one node inside，this neural network and the goal is to。use this
    node network to predict given a，target word a context word given a word。like lunch
    can I predict the，probabilities of other words showing up。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这样一个大型神经网络，我们为每个单词设置一个输入单元，每个单词在这个神经网络中都有一个节点，目标是使用这个节点网络来预测给定目标单词的上下文单词，比如“午餐”，我能否预测其他单词出现的概率。
- en: in a context of like one word away or，two words away for instance in some sort。of
    window of context and if you just，give the AI this neural network a whole。bunch
    of data of words and what words，show up in context you can train a。neural network
    to do this calculation to，be able to predict given a target word。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说在上下文中相隔一个单词或两个单词的情况，如果你给AI这个神经网络大量的单词数据，以及单词在上下文中出现的情况，你可以训练一个神经网络来进行这个计算，能够预测给定的目标单词。
- en: can I predict what those context words，ultimately should be and it will do so。using
    the same methods we've talked，about back propagating the error from。the context
    word back through this，neural network and what you get is if we。use the single
    layer just a single layer，of hidden nodes what I get is for every。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我能否预测那些上下文单词最终应该是什么，并且将使用我们所讨论的相同方法，从上下文单词反向传播误差通过这个神经网络，得到的结果是如果我们只使用一个单层的隐藏节点。
- en: single one of these words I get you know，from this word for example I get five。edges
    each of which has a weight to each，of these five hidden nodes in other。words I
    get five numbers and then，effectively are going to represent this。particular target
    word here and the，number of hidden nodes I choose in this。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单词，我得到的，比如说从这个单词，我得到五条边，每条边都有一个权重对应于这五个隐藏节点，换句话说，我得到了五个数字，然后有效地表示这个特定的目标单词，以及我在这里选择的隐藏节点数量。
- en: middle layer here I can pick that maybe，I'll choose to have 50 hidden nodes
    or。100 hidden nodes and then for each of，these target words I'll have 50。different
    values or a hundred different，values and those values we can。effectively treat
    as the vector new，erekle representation of that word and。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里的中间层，我可以选择可能有50个隐藏节点或100个隐藏节点，然后对于每个目标单词，我将有50个不同的值或100个不同的值，这些值我们可以有效地视为该单词的新向量表示。
- en: the general idea here is that if two，words are similar，two words show up in
    similar context。meaning using those same target words，I'd like to predict similar
    contexts。where it's well then these vectors in，these values I choose in these
    vectors。here these numerical values for the，weights of these edges are probably。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的基本思想是，如果两个单词相似，它们在相似的上下文中出现。换句话说，使用这些相同的目标单词，我想预测相似的上下文，那么这些向量中的值我选择的这些向量，这些边的权重的数值可能。
- en: going to be similar because for two of，different words that show up in similar。contexts
    I would like some these values，that are calculated to ultimately be。very similar
    to one another and so，ultimately the high-level way you can。picture this is that
    what this word，Tyvek training method is going to do。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因为对于出现在相似上下文中的两个不同单词，我希望计算出的这些值最终非常相似，因此，最终你可以高层次地想象，这个单词的Tyvek训练方法将会做什么。
- en: it's given a whole bunch of words where，initially recall we initialize these。weights
    randomly and just pick random，weights that we choose over time as we。train the
    neural network we're going to，adjust these weights adjust the vector。representations
    of each of these words，so that gradually words that show up in。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一堆单词，最初我们随机初始化这些权重，选择随机权重，随着训练神经网络，我们会调整这些权重，调整每个单词的向量表示，使得在某种上下文窗口中，逐渐出现的单词。
- en: '![](img/819284bd8940912d2ebc94c72b16d584_51.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/819284bd8940912d2ebc94c72b16d584_51.png)'
- en: similar contexts grow closer to one，another and words that show up in。different
    contexts get farther away from。![](img/819284bd8940912d2ebc94c72b16d584_53.png)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 相似的上下文彼此靠近，而在不同上下文中出现的单词则相距较远。![](img/819284bd8940912d2ebc94c72b16d584_53.png)
- en: one another and as a result hopefully I，get vector representations of words
    like。breakfast and lunch and dinner that are，similar to one another and then words。like
    book and memoir and novel are also，going to be similar to one another as。well
    so using this algorithm we are able，to take a corpus of data and just train。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望能得到像`breakfast`、`lunch`和`dinner`这样的词的向量表示，它们彼此相似，而像`book`、`memoir`和`novel`这样的词也是彼此相似的。因此，使用这个算法，我们能够处理一组数据并进行训练。
- en: our computer train this neural network，to be able to figure out what vector。what
    sequence of numbers is going to，represent each of these words which is。again a
    bit of a strange concept to，think about like representing a word。just as a whole
    bunch of numbers but，we'll see in a moment just how powerful。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练这台计算机，训练这个神经网络，以便找出哪些向量、哪些数字序列将代表这些单词。这是一个有点奇怪的概念，想象一个单词仅用一堆数字表示，但我们马上就会看到它的强大之处。
- en: this really can be so we'll go ahead and，go into vectors and what I have inside。of
    vectors dot pi which will open up now，is I'm opening up words text which is a。pre
    trained model that just I've already，run word Tyvek and it's already given me。a
    whole bunch of vectors for each of，these possible words and and I'm just，going
    to take like 50。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这真的很有趣，所以我们继续进入`vectors`，我将打开`vectors.py`，现在我打开的是`words.txt`，这是一个预训练模型，我已经运行过Word2Vec，它已经为每个可能的单词给了我一堆向量，我只想取50个。
- en: 000 of them and go，ahead and save their vectors inside of a，dictionary called
    words and then I've。also defined some functions called，distance closest words
    that'll get me。like what are the closest words to a，particular word and then closest
    word。that just gets me the one closest word。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将它们保存到一个名为`words`的字典中，然后我还定义了一些函数，比如`distance`和`closest words`，这些函数将告诉我某个特定单词的最接近单词，以及`closest
    word`函数，它只会给我一个最接近的单词。
- en: '![](img/819284bd8940912d2ebc94c72b16d584_55.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/819284bd8940912d2ebc94c72b16d584_55.png)'
- en: for example and so now let me try doing，this let me open，the Python interpreter
    and say something。like from vectors import star just，import everything from vectors
    and now。let's take a look at the meanings of，some words let me look at the word
    like。city for example and here is a big array，that is the vector representation
    of the。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，现在让我尝试做这件事，让我打开Python解释器并说点什么。像是从`vectors`中导入`star`，也就是从`vectors`中导入所有内容，现在让我们看看一些单词的含义，比如`city`这个词。
- en: word city and you know this doesn't mean，anything in terms of what these numbers。exactly
    are but this is how my computer，is representing the meaning of the word。city and
    into a different word like，words house and here then is the vector。representation
    of the word house for，example just a whole bunch of numbers。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`city`这个词，这些数字究竟代表什么并不重要，但这就是我的计算机如何表示`city`这个词的含义。然后是像`house`这样的不同单词，这里是`house`的向量表示，仅仅是一堆数字。'
- en: and this is encoding somehow the meaning，of the word house and how do I get
    at。that idea well one way to measure how，good this is is by looking at what is。the
    distance between various different，words so distance there are a number of。ways
    you can define distance in context，of vectors one common way is what's。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这在某种程度上编码了`house`这个词的含义。那我如何获取这个想法呢？一种衡量其优劣的方法是查看不同单词之间的距离。在向量上下文中，定义距离的方式有很多种，常见的方式是使用。
- en: known as like the cosine distance that，has to do with measuring the angle。between
    vectors but in short it's just，measuring like how far apart are these。two vectors
    from each other so if I take，a word like you know the word book how。far away is
    it from itself like how far，away is the word book from book。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 被称为余弦距离，它与测量向量之间的角度有关。简而言之，它只是测量这两个向量之间的距离。所以如果我取一个词，比如`book`，它与自己之间的距离有多远？`book`与`book`的距离是多少？
- en: well that's zero right the word book is，zero distance away from itself but let's。see
    how far away word book is from a，word like breakfast what we're gonna say。like
    one is very far away zero is not，far away all right book is about point。six four
    away from breakfast they seem，to be pretty far apart but let's now try。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是零，`book`这个词与它自己之间的距离为零。但我们来看看`book`这个词与像`breakfast`这样的词有多远。我们可以说，一个是很远的，零是很近的。好吧，`book`与`breakfast`的距离大约是0.64，似乎相距很远，但现在让我们尝试。
- en: and calculate the distance from words，book two words novel for example now。those
    two words are closer to each other，point three four that the vector。representation
    of the word book is，closer to the vector representation of。the word novel than
    it is to the vector，representation of the word breakfast and。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 计算单词之间的距离，比如“书”和“小说”这两个词，例如这两个词彼此更接近，点三四，单词“书”的向量表示比“早餐”的向量表示更接近单词“小说”的向量表示。
- en: I can do the same thing and say compare，breakfast to lunch for example and those。two
    words are even closer together that，they have an even more similar。relationship
    between one word and，another so now it seems we have some。representation of words
    representing a，word using vectors that allows us to be。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以做同样的事情，比如比较“早餐”和“午餐”，例如这两个词更加接近，它们之间有更加相似的关系。所以现在看起来我们有一些表示单词的方式，使用向量来表示一个单词，使我们能够。
- en: able to say something like words that，are similar to each other ultimately，have
    a small。distance that happens to be between them，and this turns out to be incredibly。powerful
    to be able to represent the，meaning of words in terms of their。relationships to
    other words as well I，can show you as well I have a function。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 能够说出像是相似的单词，最终在它们之间会有一个小距离，而这证明了在代表单词的意义时，可以用它们与其他单词的关系来表示，这是非常强大的，我也可以给你展示，我有一个函数。
- en: called closest words that basically just，takes a whole bunch of words and gets。all
    the closest words to it so let me，get the closest words - like book for。example
    and maybe get it like the ten，closest words will limit ourselves to。ten and the
    right book is obviously，closest to itself the word book but is。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 叫做“最近单词”，基本上只需获取一堆单词并获取与其最接近的单词，所以让我获取最接近的单词，比如“书”，例如，也许我获取大约十个最近的单词，我们将自己限制为十个，而单词“书”显然与它自己最接近。
- en: also closely related to like books and，essay and memoir and essays and novella。anthology
    the and why are these words，that I was able to computer close to it。well because
    based on the corpus of，information that this this algorithm was。trained on the
    vector is that a rose a，rose based on what words show up in a。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 还与像“书”和“论文”、“回忆录”、“论文”、“中篇小说”、“选集”等密切相关。为什么这些单词我能够让计算机接近呢？因为基于这个算法训练的语料库，向量就像玫瑰一样，基于在类似上下文中出现的单词。
- en: similar context that the word book shows，up in a similar context similar other。words
    to words like memoir and essays，for example and if I do something like。let me
    get the closest words to city you，end up getting in a city town township。village
    these are words that happen to。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 单词“书”在类似的上下文中出现，类似的其他单词也类似于“回忆录”和“论文”，例如，如果我做一些事情，比如获取与“城市”最接近的单词，你最终会得到“城市”、“镇”、“乡镇”、“村庄”，这些单词恰好出现在与“城市”相似的上下文中。
- en: '![](img/819284bd8940912d2ebc94c72b16d584_57.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/819284bd8940912d2ebc94c72b16d584_57.png)'
- en: show up in a similar context to the word，city now where things get really。interesting
    is that because these are，vectors we can do mathematics with them。we can calculate
    the relationships，between various different words so I can。![](img/819284bd8940912d2ebc94c72b16d584_59.png)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有趣的是，因为这些是向量，我们可以对它们进行数学运算。我们可以计算不同单词之间的关系，所以我可以。![](img/819284bd8940912d2ebc94c72b16d584_59.png)
- en: say something like alright what if I had，like man and King these are two。different
    vectors and this is a famous。![](img/819284bd8940912d2ebc94c72b16d584_61.png)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，好吧，如果我有“男人”和“国王”，这两个是不同的向量，而这是一个著名的。![](img/819284bd8940912d2ebc94c72b16d584_61.png)
- en: example that comes out of word Tyvek you，know I can take these two vectors and。just
    subtract them from each other this，line here this distance here is another。![](img/819284bd8940912d2ebc94c72b16d584_63.png)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 例子来自单词“Tyvek”，你知道，我可以将这两个向量相减，这里这条线，这里的距离又是另一个。![](img/819284bd8940912d2ebc94c72b16d584_63.png)
- en: vector that represents like king - man，now what does it mean to like take a。word
    and subtract another word normally，that doesn't make sense in the world of。vectors
    though you can take some vector，some sequence of numbers subtract some。![](img/819284bd8940912d2ebc94c72b16d584_65.png)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 表示像“国王 - 男人”的向量，现在从一个单词中减去另一个单词，这在向量的世界里通常是没有意义的，不过你可以取某个向量，某个数字序列来进行减法。![](img/819284bd8940912d2ebc94c72b16d584_65.png)
- en: other sequence of numbers and get a new，vector get a new sequence of numbers
    and。what this new sequence of numbers is，effectively going to do is it is going。to
    tell me like what do I need to do to，get from man to King what is the。relationship
    then between these two，words and this is some vector。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 其他数字序列并获得新的向量，得到新的数字序列，而这个新的数字序列实际上将告诉我，从“男人”到“国王”需要做什么，这两个单词之间的关系是什么，而这就是某种向量。
- en: representation of what makes go takes us，from man to king and we can then take。![](img/819284bd8940912d2ebc94c72b16d584_67.png)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 描述了如何从“男人”到“国王”，然后我们可以再取。![](img/819284bd8940912d2ebc94c72b16d584_67.png)
- en: this value and add it to another vector，you might imagine that like the word，woman
    for exam。is another vector that exists somewhere，inside of this space somewhere
    inside of。this vector space and what might happen，if I took this same idea King
    - man took。that same vector and just added it -，woman like what would we find。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这个值并将其添加到另一个向量中，你可能会想象，“女人”这个词例如，是在这个空间的某个地方存在的另一个向量，如果我采取这个相同的思路“国王 - 男人”，然后添加“女人”，我们会发现什么。
- en: around here it's an interesting question，we might ask and we can answer it very。easily
    because I have vector，representations of all of these things I。can say let's go
    back here let me look，at the representation of the word man。here's the vector
    representation of man，let's look at the representation of the。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个有趣的问题，我们可以很容易地回答，因为我拥有所有这些事物的向量表示。我可以说，让我们回去看看“男人”这个词的表示。这里是“男人”的向量表示，让我们看看它的表示。
- en: word King here's the representation of，the word King and I can subtract these，of
    King。- man into this array right here a whole，bunch of values so King - man now。represents
    the relationship between King，and man in some sort of numerical vector。format
    so what happens then if I add，woman to that say whatever took us from。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: “国王”这个词的表示，这里是“国王”的表示，我可以将这些从“国王”减去“男人”，进入这个数组，很多值，所以“国王 - 男人”现在在某种数值向量格式中表示了“国王”和“男人”之间的关系。那么，如果我添加“女人”到这个值，比如说，是什么将我们从。
- en: man to King go ahead and apply that same，vector to the vector representation
    of。the word woman and that gives us this，vector here and now just out of。curiosity
    let's take this expression and，find what is the closest word to that。expression
    and amazingly what we get is。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: “男人”到“国王”，继续应用同样的向量到“女人”的向量表示，这给了我们这个向量，现在出于好奇，让我们取这个表达，找出与这个表达最接近的单词，惊人的是我们得到的是。
- en: '![](img/819284bd8940912d2ebc94c72b16d584_69.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/819284bd8940912d2ebc94c72b16d584_69.png)'
- en: we get the word queen that somehow when，you take the different distance between。men
    and King this numerical，representation of how man is related to。![](img/819284bd8940912d2ebc94c72b16d584_71.png)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的“女王”这个词，似乎在你测量“男人”和“国王”之间的不同距离时，这个数字表示了“男人”与“国王”的关系。![](img/819284bd8940912d2ebc94c72b16d584_71.png)
- en: King and add that same notion King - men，to the vector representation of the
    word。woman what we get is we get the vector，representation or something close
    to the。vector representation of the word Queen，because this distance somehow encoded。the
    relationship between these two words，when you run it through this algorithm。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 将“国王”和“男人”的相同概念添加到单词的向量表示中。“女人”得到的结果是，我们得到了接近于单词“女王”的向量表示，因为这种距离以某种方式编码了这两个单词之间的关系，当你通过这个算法运行时。
- en: it's not programmed to do this but if，you just try and figure out how to。predict
    words based on context words and，you get vectors that are able to make。these sort
    of like SAT like analogies，out of the information that it's been。given so there
    are more examples of this，we can say all right let's figure out。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 它并不是被编程来做这个，但如果你试着找出如何根据上下文词来预测单词，你会得到能够基于所提供信息生成类似SAT类比的向量，这里有更多的例子，我们可以说，好吧，让我们找出。
- en: what is the distance between Paris and，France so Paris and France are words
    the。each of a vector representation this，then is a vector representation of the。distance
    between Paris and France like，what takes us from France to。Paris and let me go
    ahead and add the，vector representation of England to that。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 巴黎和法国之间的距离是什么，所以巴黎和法国是单词，各自都有向量表示，那么这是巴黎和法国之间的距离的向量表示，像是从法国到巴黎的距离。让我继续将“英国”的向量表示添加到这里。
- en: so this then is the vector，representation of going Paris - France。plus England
    so the distance between，friends and Paris as vectors add the。england vector and
    let's go ahead and，find the closest word to that and it。turns out to be london
    do you do this，relationship the relationship between。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这是巴黎 - 法国加上英格兰的向量表示，朋友和巴黎之间的距离作为向量，添加英格兰向量，让我们找到最接近的词，结果是伦敦。你进行这种关系。
- en: France and Paris go ahead and add the，england vector to it and the closest。vector
    to that happens to be the vector，for the word london we can do more。examples i
    can say you know what let's，take the word for teacher that vector。representation
    and let me subtract the，vector representation of school so what。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 法国和巴黎，继续添加英格兰向量，最接近的向量是伦敦的向量，我们可以进行更多的例子。我可以说，让我们取“教师”的向量表示，并减去“学校”的向量表示，那么。
- en: I'm left with is what takes us from，school to teacher and apply that vector。to
    a word like hospital and see what is，the closest word to that turns out the。closest
    word is nurse let's try a couple，more example as a closest word to you。know ramen
    for example subtract closest，word to Japan so what is the。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所剩下的是，从“学校”到“教师”的关系，并将该向量应用于“医院”这个词，看看最接近的词是什么，结果发现最接近的词是“护士”。让我们再试几个例子，比如“拉面”的最接近词，减去与日本的最接近词。
- en: relationship between Japan and ramen add，the word for America to that we take
    a。guess is what you might get as a result，it turns out you get burritos as the。relationship
    if you do the subtraction，do the addition this is the answer that。![](img/819284bd8940912d2ebc94c72b16d584_73.png)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 日本和拉面之间的关系，添加“美国”这个词，猜测一下结果是什么，结果是你得到了“卷饼”作为关系，如果你进行减法和加法，这就是答案！![](img/819284bd8940912d2ebc94c72b16d584_73.png)
- en: you happen to get as a consequence of，this as well so these very interesting。analogies
    arise in the relationships，between these two words but if you just。map out all
    of these words into a vector，space you can get some pretty。interesting results
    as a consequence of，that and this idea of representing words。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 结果会引出非常有趣的类比，这些类比存在于这两个词之间。如果你将所有这些词映射到一个向量空间，你可以得到一些相当有趣的结果，这种表示词的想法。
- en: as vectors turns out to be incredibly，useful and powerful anytime we want to。be
    able to do some statistical work with，regards to natural language to be able。to
    have represent words not just this，their character ISM but to represent。them as
    numbers numbers that say，something or mean something about the。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 将词作为向量来处理是极其有用和强大的，尤其是当我们想进行与自然语言相关的统计工作时，可以不仅仅将词表示为其字符，而是将其表示为数字，这些数字能够传达某种意义。
- en: words themselves and somehow relate the，meaning of a word to other words that。might
    happen to exist some many tools，then for being able to work inside of。this world
    of natural language the，natural language is tricky we have to。deal with the syntax
    of language and the，semantics of language but we've really。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 单词本身与其他单词之间的关系以某种方式关联起来，这些工具使我们能够在自然语言这个领域中工作，处理语言的句法和语义很棘手，但我们确实。
- en: just seen just the beginning of some of，the ideas that are underlying。a lot
    of natural language processing the，ability to like take text extract。information
    out of it get some sort of，meaning out of it generate sentences。maybe by having
    some knowledge of the，grammar or maybe just by looking at。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一些基本思想的开始，这些思想支撑着许多自然语言处理的能力，比如从文本中提取信息，从中获取某种意义，生成句子，可能通过了解语法，或者仅仅通过观察。
- en: probabilities of what words are likely，to show up based on other words that。have
    shown up previously and then，finally the ability to take words and。come up with
    some distributed，representation of them to take words and。represent them as numbers
    and use those，numbers to be able to say something。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 基于其他先前出现的单词，计算哪些单词可能会出现的概率，最后能够将词转化为某种分布式表示，将词表示为数字，并利用这些数字传达某种信息。
- en: '![](img/819284bd8940912d2ebc94c72b16d584_75.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/819284bd8940912d2ebc94c72b16d584_75.png)'
- en: meaningful about those words as well so，this then is yet another topic in this。broader
    heading of artificial，intelligence and just as a look back at。where we've been
    now we started our，conversation by talking about the world。of search about trying
    to solve problems。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对这些词汇的意义也是如此，因此这是在人工智能这个更广泛主题下的另一个话题。回顾一下我们所经历的，我们开始我们的讨论时谈论的是搜索世界，尝试解决问题。
- en: '![](img/819284bd8940912d2ebc94c72b16d584_77.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/819284bd8940912d2ebc94c72b16d584_77.png)'
- en: like tic-tac-toe by searching for a，solution by exploring our various。different
    possibilities and looking at，what algorithms we can apply to be able。to efficiently
    try and search a space we，looked at some simple algorithms and。then looked at
    some optimizations we，could make to those algorithms and。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于井字棋，通过寻找解决方案，探索我们各种不同的可能性，并查看我们可以应用哪些算法，以有效地尝试搜索空间。我们研究了一些简单的算法，然后研究了我们可以对这些算法进行的优化。
- en: ultimately that was in service of trying，to get our AI to know things about
    the。world and this has been a lot of what，we've talked about today as well trying。to
    get knowledge out of text-based，information the ability to take。![](img/819284bd8940912d2ebc94c72b16d584_79.png)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，这些都服务于让我们的AI了解世界的尝试，这也是我们今天讨论的很多内容，试图从文本信息中提取知识，具备从中提取信息的能力。![](img/819284bd8940912d2ebc94c72b16d584_79.png)
- en: information draw conclusions based on，those information if I know these two。![](img/819284bd8940912d2ebc94c72b16d584_81.png)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些信息得出结论，如果我知道这两个。![](img/819284bd8940912d2ebc94c72b16d584_81.png)
- en: things for certain maybe I can draw a，third conclusion as well that then words。related
    to to the idea of uncertainty if，we don't know something for sure and can。we predict
    something figure out the，probabilities of something and we saw。that again today
    in the context of，trying to predict whether a tweet or。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我可以得出第三个结论，那就是与不确定性相关的词汇。如果我们对某事没有确切的了解，我们能否预测某事，并找出某事的概率？今天我们再次看到这一点，试图预测一条推文或。
- en: whether a message is positive sentiment，or negative sentiment and trying to
    draw。that conclusion as well then we took a，look at optimization the sorts of。problems
    where we're looking for a local。![](img/819284bd8940912d2ebc94c72b16d584_83.png)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 无论消息是积极情绪还是消极情绪，试图得出这样的结论。然后我们研究了优化，寻找局部的。![](img/819284bd8940912d2ebc94c72b16d584_83.png)
- en: global or local maximum or minimum this，has come up time and time again。especially
    most recently in the context，of neural networks which are really just。a kind of
    optimization problem where，we're trying to minimize the total。amount of loss based
    on the setting of，our weights of our own neural network。
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 全局或局部最大值或最小值，这一点一次又一次地出现。尤其是在最近的神经网络背景下，神经网络实际上是一种优化问题，我们试图最小化基于我们神经网络权重设置的总损失。
- en: based on the setting of you know what，vector representations for words we。happen
    to choose them and those，ultimately helped us to be able to solve。learning related
    problems the ability to，take a whole bunch of data and rather。than us tell the
    AI exactly what to do，let the AI learn patterns from the data。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们所选择的词汇向量表示，这最终帮助我们能够解决与学习相关的问题，具备从大量数据中提取模式的能力，而不是告诉AI该做什么，让AI从数据中学习。
- en: for itself let it figure out what makes，an inbox message different from a spam。message
    let it figure out what makes a，counterfeit bill different from an，authentic bill
    and。being able to draw that analysis as well，and one of the big tools in learning。that
    we used were neural networks these，structures that allow us to relate。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让它自己去发现什么使收件箱消息与垃圾邮件不同，让它找出伪钞与真钞之间的区别，并能够进行这样的分析。我们使用的一个重要工具是神经网络，这些结构使我们能够关联事物。
- en: inputs to outputs by training these，neural networks to learn some sort of。function
    that map's us from some input，to some output ultimately yet another。model in this
    language of artificial，intelligence that we can use to。communicate with our AI
    and then finally，today we looked at some ways that AI can。
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 通过训练这些神经网络将输入转化为输出，以学习某种函数，将我们从某个输入映射到某个输出，最终形成另一种人工智能语言模型，以便与我们的AI进行沟通。最后，今天我们探讨了一些AI可以使用的方法。
- en: begin to communicate with us looking at，ways that AI can begin to get an。understanding
    for the syntax and the，semantics of language to be able to。generate sentences
    to be able to predict，things about text that's written in a。spoken language or
    a written language，like English and to be able to do。
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 开始与我们沟通，探索人工智能如何开始理解语言的语法和语义，以便能够生成句子，预测关于书面或口头语言（如英语）所写文本的内容，并能够做到这一点。
- en: interesting analysis there as well and，there's so much more in active research。that's
    happening all over the areas，within artificial intelligence today and。we've really
    only just seen the，beginning of what AI has to offer so I。hope you enjoyed this
    exploration into，this world of artificial intelligence。
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个有趣的分析，同时在积极研究中还有更多的内容，这些研究正在人工智能的各个领域进行。我们实际上只见到了人工智能所能提供的开始，所以我希望你喜欢这次探索人工智能的世界。
- en: with Python a big thank you to the，courses teaching staff and the。production
    team for making this class，possible this was an introduction to。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢Python课程的教学人员和制作团队，让这门课成为可能，这是一段介绍。
