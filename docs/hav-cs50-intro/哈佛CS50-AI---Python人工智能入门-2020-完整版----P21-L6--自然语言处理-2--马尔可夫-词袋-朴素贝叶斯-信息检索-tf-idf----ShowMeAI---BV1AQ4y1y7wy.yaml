- en: 哈佛CS50-AI ｜ Python人工智能入门(2020·完整版) - P21：L6- 自然语言处理 2 (马尔可夫，词袋，朴素贝叶斯，信息检索，tf-idf)
    - ShowMeAI - BV1AQ4y1y7wy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 哈佛CS50-AI ｜ Python人工智能入门(2020·完整版) - P21：L6- 自然语言处理 2 (马尔可夫，词袋，朴素贝叶斯，信息检索，tf-idf)
    - ShowMeAI - BV1AQ4y1y7wy
- en: happen to come up multiple times across。![](img/2782ceb1249cc2bcb3d95101cf1913de_1.png)
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 偶尔会多次出现。![](img/2782ceb1249cc2bcb3d95101cf1913de_1.png)
- en: this particular corpus so what are the，potential use cases here now we have。some
    sort of data we have data about how，often particular sequences of words show，up
    in。particular order and using that we can，begin to do some sort of predictions
    we。might be able to say that if you see the，words it was you know there's a。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特定的语料库，那么这里的潜在使用案例是什么？现在我们有一些数据，我们有关于特定单词序列出现的频率，按特定顺序排列，并利用这些数据，我们可以开始做一些预测。我们可能会说，如果你看到这些单词，它是。
- en: reasonable chance the word that comes，after it should be the word a and if I。see
    the words one of it's reasonable to，imagine that the next word might be the。word
    the for example because we have，this data about trigram sequences of。three words
    and how often they come up，and now based on two words you might be。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 有合理的机会，后面跟着的单词应该是单词“a”，如果我看到单词“one of”，可以合理地想象下一个单词可能是单词“the”，例如，因为我们有关于三元组序列的数据，以及它们出现的频率。现在基于两个单词，你可能会。
- en: able to predict what the third word，happens to be and one model we can use。for
    that is a model we've actually seen，before it's the Markov model recall。again
    that the Markov model really just，refers to some sequence of events that。![](img/2782ceb1249cc2bcb3d95101cf1913de_3.png)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 能够预测第三个单词是什么，而我们可以用来实现这一点的模型是我们之前见过的模型，它是马尔可夫模型。再次回想，马尔可夫模型实际上只指某种事件序列。![](img/2782ceb1249cc2bcb3d95101cf1913de_3.png)
- en: happen one time step after a one time，step where every unit has some ability。to
    predict what the next unit is going，to be or maybe the past two units。predict
    what the next unit is going to，be or the past three predict what the。next one
    is going to be and we can use a，Markov model and apply it to language。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 发生在一个时间步之后，每个单位都有某种能力来预测下一个单位会是什么，或者可能是过去两个单位预测下一个单位会是什么，或者过去三个单位预测下一个单位会是什么。我们可以使用马尔可夫模型并将其应用于语言。
- en: for a very naive and simple approach at，trying to generate natural language
    at。getting our AI to be able to speak，English like text and the way it's going。to
    work is we're going to say something，like come up with some probability。distribution
    given these two words what，is the probability distribution over。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 一种非常幼稚且简单的方法来尝试生成自然语言，让我们的AI能够像英语文本一样说话，它的工作方式是，我们将说一些内容，比如在给定这两个单词的情况下，得到一些概率分布，这个概率分布是什么。
- en: what the third word could possibly be，based on all the data if you see it was。what
    are the possible third words we，might have how often do they come up and。using
    that information we can try and，construct what we expect the third word。to be
    and if you keep doing this the，effect is that our Markov model can。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 根据所有的数据，第三个单词可能是什么，如果你看到它，可能的第三个单词有哪些，它们出现的频率如何。利用这些信息，我们可以尝试构建我们期望的第三个单词是什么。如果你不断这样做，效果就是我们的马尔可夫模型可以。
- en: effectively start to generate text and。![](img/2782ceb1249cc2bcb3d95101cf1913de_5.png)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有效地开始生成文本并。![](img/2782ceb1249cc2bcb3d95101cf1913de_5.png)
- en: be able to generate text that was not in，the original corpus but that sounds
    kind。of like the original corpus it's using，the same sorts of rules that the。original
    corpus was using so let's take，a look at an example of that as well。we're here
    now I have another corpus，that I have here and it is the corpus of。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 能够生成不在原始语料库中的文本，但听起来有点像原始语料库，使用相同的规则。那么我们也来看看一个例子。我们现在在这里，我还有另一个语料库，这是我手上的语料库。
- en: all of the works of William Shakespeare，so I've got a whole bunch of stories。from
    Shakespeare and all of them are，just inside of this big text file and so。what
    I might like to do is look at what，all of the engrams are maybe look at all。the
    trigrams inside of Shakespeare text，and figure out given two words。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 威廉·莎士比亚的所有作品，所以我有一整堆故事。来自莎士比亚，所有的故事都在这个大的文本文件中。因此，我想做的是看看，所有的语言图式，也许看看莎士比亚文本中的所有三元组，然后弄清楚给定两个单词的情况下。
- en: can I predict what the third word is，likely to be and then just keep。repeating
    this process I have two words，predict the third word then from the。second and
    the third word predict the，fourth word and from the third and。fourth word predict
    the fifth word，ultimately generating random sentences。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我能预测第三个单词，可能是什么，然后继续。重复这个过程，我有两个单词，预测第三个单词，然后从第二和第三个单词预测，第四个单词，从第三和。第四个单词预测第五个单词，最终生成随机句子。
- en: that sound like Shakespeare that are，using similar patterns of words that。Shakespeare
    used but that never actually，showed up in Shakespeare itself instead。to do so
    I'll show you generator dot Pi，which again is just going to read data。from a particular
    file and I'm using a，Python library called Markova Phi which。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来像莎士比亚的句子，使用莎士比亚所使用的相似单词模式，但实际上从未在莎士比亚中出现过。为了做到这一点，我将展示 generator.py，这将从特定文件读取数据。我使用的一个，Python库叫做
    Markova Phi。
- en: is just going to do this process for me，so there are libraries out here that
    can。just train on a bunch of text and come，up with a Markov model based on that。text
    and I'm gonna go ahead and just。![](img/2782ceb1249cc2bcb3d95101cf1913de_7.png)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 将为我完成这个过程，所以这里有一些库，可以。训练一堆文本，并基于该文本生成马尔可夫模型。我将继续并且。![](img/2782ceb1249cc2bcb3d95101cf1913de_7.png)
- en: generate five randomly generated，sentences so we'll go ahead and go into。Markov
    I'll run the generator on，Shakespeare text what we'll see is it's。going to load
    that data and then here's，what we get we get you know five。different sentences
    and these are，sentences that never showed up in any。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 生成五个随机生成的句子，所以我们接下来将深入探讨。马尔可夫，我将对莎士比亚的文本运行生成器，我们看到的是它。会加载这些数据，然后这是，我们得到的五个。不同的句子，这些是，句子在任何地方都没有出现过。
- en: Shakespeare play but that are designed，to sound like Shakespeare that are。designed
    to just take two words and，predict given those two words what would。Shakespeare
    have been likely to choose，as the third word that follows him and。you know these
    sentences probably don't，have any meaning it's not like the AI is。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 莎士比亚的戏剧，但设计成，听起来像莎士比亚，旨在仅仅取两个单词，并且，预测给定这两个单词莎士比亚可能会选择的第三个单词，跟随他，你知道这些句子可能没有，任何意义，不是说人工智能。
- en: trying to express any sort of underlying，meaning here it's just trying to。understand
    based on the sequence of，words what is likely to come after it。as a next word
    for example and these are，the types of sentences that it's able to。come up with
    just generating and if you，ran this multiple times and you would。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试表达任何潜在的，含义，这只是试图。理解基于单词的顺序，接下来可能会出现什么。作为下一个单词，例如，这些是，它能够生成的句子类型。如果你多次运行这个，你会。
- en: end up getting different results that I，could run this again then get an。entirely
    different set of five different，sentences that also are supposed to。![](img/2782ceb1249cc2bcb3d95101cf1913de_9.png)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最终会得到不同的结果，我，可能再次运行这个，然后得到一个。完全不同的一组五个不同，句子也应该是。![](img/2782ceb1249cc2bcb3d95101cf1913de_9.png)
- en: sound kind of like the way that，Shakespeare sentences sounded as well。and so
    that then was a look at how it is，we can use Markov models to be able to。naively
    attempt just generating language，the language doesn't mean a whole lot。right now
    you wouldn't want to use the，system in this current form to do。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来有点像，莎士比亚的句子声音一样。因此，这就是我们如何使用马尔可夫模型，简单地尝试生成语言，语言目前并没有太多意义。你不想在这个当前形式下使用，系统来做。
- en: something like machine translation，because it wouldn't be able to，encapsulate
    any meaning but we're。starting to see now that our AI is，getting a little bit
    better at trying to。speak our language or trying to be able，to process natural
    language in some sort，of meaningful way。so we'll now take a look at a couple of，other
    tasks that we might want our AI to。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 像机器翻译这样的事情，因为它无法，封装任何意义，但我们。现在开始看到我们的人工智能，逐渐变得更好，尝试。说我们的语言或以某种方式处理自然语言，具有一定的意义。因此我们现在将看一下几项，其他任务，我们可能希望我们的人工智能。
- en: be able to perform and one such task is，text categorization which really is
    just。![](img/2782ceb1249cc2bcb3d95101cf1913de_11.png)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 能够执行的任务之一是，文本分类，这实际上就是。![](img/2782ceb1249cc2bcb3d95101cf1913de_11.png)
- en: a classification problem and we've，talked about classification problems。already
    these problems where we would，like to take some object and categorize。it into
    a number of different classes。![](img/2782ceb1249cc2bcb3d95101cf1913de_13.png)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一种分类问题，我们已经，讨论过分类问题。这些问题是，我们希望将某个对象分类。到多个不同类别。![](img/2782ceb1249cc2bcb3d95101cf1913de_13.png)
- en: and so the way this comes up in text is，anytime you have some sample of text
    and。you want to put it inside of a category，where I want to say something like
    given。an email does it belong in the inbox or，does it belong in spam like which
    of。these two categories does it belong in，and you do that by looking at the text。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种文本的表现方式是，无论何时你有一些文本样本，并且想把它归入某个类别，比如说，给定一封邮件，它是否属于收件箱，还是属于垃圾邮件？这两个类别中它属于哪个，你是通过查看文本来实现的。
- en: and being able to do some sort of，analysis on that text to be able to draw。conclusions
    to be able to say that given，the words that show up in the email I。think this
    is probably belonging in the，inbox or I think it probably belongs in。spam instead
    and you might imagine doing，this for a number of different types of。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 能够对这些文本进行某种分析，以得出结论，比如说根据出现在邮件中的词汇，我认为这可能属于收件箱，或者我认为它可能属于垃圾邮件，你可能会想象为多种不同类型的。
- en: classification problems of this sort so，you might imagine that another common。example
    of this type of idea is，something like sentiment analysis where。I want to analyze
    given a sample of text，doesn't have a positive sentiment or。does it have a negative
    sentiment and，this might come up in the case of like。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分类问题，你可能想象另一个常见的例子是情感分析，我想分析给定的文本样本，是否有正面情感，还是有负面情感，这可能出现在例如。
- en: product reviews on a website for example，or feedback on a website where you
    have。![](img/2782ceb1249cc2bcb3d95101cf1913de_15.png)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，网站上的产品评论，或是你有的反馈！[](img/2782ceb1249cc2bcb3d95101cf1913de_15.png)
- en: a whole bunch of data samples of texts，that are provided by users of a website。and
    you want to be able to quickly，analyze are these reviews positive are。the reviews
    negative what is it the，people are saying just to get a sense。for what it is that
    people are saying to，be able to categorize texts into one of。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一堆由网站用户提供的数据样本，你想能够快速分析这些评论是正面的还是负面的，人们在说什么，以便了解他们在说什么，以便将文本分类为其中之一。
- en: these two different categories so how。![](img/2782ceb1249cc2bcb3d95101cf1913de_17.png)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种不同的类别，所以怎么。![](img/2782ceb1249cc2bcb3d95101cf1913de_17.png)
- en: might we approach this problem well，let's take a look at some sample product。reviews
    here are some sample product，reviews that we might come up with my。grandson loved
    it so much fun product，broke after a few days one of the best。games I've played
    in a long time kind of，cheap and flimsy not worth it right。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如何处理这个问题呢？让我们看看一些示例产品评论，这里有一些可能出现的产品评论：“我孙子非常喜欢这个，有趣的产品”，**几天后坏了**，这是我很久以来玩过的**最好的**游戏，“有点**廉价和脆弱**，不值得买”。
- en: different product reviews that you might，imagine seeing on Amazon or Ebay or
    some。other website where people are selling，products for instance and we humans。could
    pretty easily categorize these，into positive sentiment or negative。sentiment we'd
    probably say that the，first and the third one those are。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在亚马逊或易贝或其他某些人们销售产品的网站上看到的不同产品评论，我们人类可以相对容易地将其分类为正面情感或负面情感。我们可能会说第一条和第三条是正面的。
- en: positive sentiment messages the second，one and the fourth one those are。probably
    negative sentiment messages but，how，it try and take these reviews and assess。you
    know are they positive or are they，negative well ultimately it depends upon。the
    words that happen to be in this，particular these particular reviews。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正面情感的信息，第二和第四条可能是负面情感的信息，但我们如何尝试评估这些评论呢？你知道它们是正面还是负面，这最终取决于这些特定评论中的词汇。
- en: inside of these particular sentences for，now we're going to ignore the structure。and
    how the words are related to each，other and we're just going to focus on。what
    the words actually are so they're，probably some key words here words like。loved
    and fun and best those probably，show up in more positive reviews。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些特定句子中，现在我们将忽略结构，以及词汇之间的关系，我们只关注词汇本身，所以这里可能有一些关键词，例如**喜欢**、**有趣**和**最好**，这些词可能在更多的正面评论中出现。
- en: whereas words like broke and cheap and，flimsy well those are words that。probably
    are more likely to come up，inside of negative reviews instead of。positive reviews
    so one way to approach，this sort of text analysis idea is to。say let's for now
    ignore the structures，of these sentences to say we're not。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 而像**破碎**、**廉价**和**脆弱**这样的词，可能更容易出现在负面评论中，而非正面评论。因此，一种处理这种文本分析的方法是，暂时忽略这些句子的结构，也就是说我们不。
- en: gonna care about how it is the words，relate to each other we're not gonna try。and
    parse these sentences that，constructs their grammatical structure。like we saw
    a moment ago but we can，probably just rely on the words that。were actually used
    rely on the fact that，the positive review ISM are more likely。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关心的是单词之间的关系，我们不会尝试解析这些句子以构建它们的语法结构，就像我们刚才看到的那样，但我们可能只依赖于实际使用的单词，依赖于积极评价更有可能的事实。
- en: to have words like best and loved and，fun and that the negative reviews are。more
    likely to have the negative words，that we've highlighted there as well in。this
    sort of model this approach to，trying to think about language is。generally known
    as the bag of words，model where we're going to model a。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有“最好”、“喜爱”和“有趣”等单词，负面评论更可能包含我们在这种模型中突出显示的负面词汇，这种思考语言的方法通常被称为词袋模型，我们将对其进行建模。
- en: sample of text not by caring about its。![](img/2782ceb1249cc2bcb3d95101cf1913de_19.png)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 文本样本，不关心它的。![](img/2782ceb1249cc2bcb3d95101cf1913de_19.png)
- en: structure but just by caring about the，unordered collection of words that show。up
    inside of a sample that all we care，about is what words are in the text and。we
    don't care about what the order of，those words is we don't care about the。structure
    of the words we don't care，what nouns goes with what adjectives or。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 结构，但仅仅关注样本中出现的无序单词集合，我们关心的只是文本中的单词，而不关心这些单词的顺序，也不关心单词的结构，我们不在意什么名词与什么形容词搭配。
- en: how things agree with each other and we，just care about the words and it turns。out
    this approach tends to work pretty，well for doing classifications like。positive
    sentiment or negative sentiment，and you could imagine doing this in a。number of
    ways we've talked about，different approaches to trying to solve。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 事物之间如何相互关联，我们只关心单词，结果证明这种方法在进行分类时，比如积极情感或消极情感，效果相当不错，你可以想象用我们讨论过的多种方式来实现。
- en: classification style problems but when，it comes to natural language one of the。most
    popular approach ISM is the naive，Bayes approach and this is one approach。to trying
    to analyze the probability，that something is you know positive。sentiment or negative
    sentiment or just，trying to categorize some text into。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 分类样式的问题，但在自然语言中，最流行的方法之一是朴素贝叶斯方法，这是分析某事物是否是积极情感或消极情感的一种方法，或者只是试图将一些文本进行分类。
- en: possible categories and it doesn't just，work for text it works for other types。of
    ideas as well but it is quite popular，in the world of analyzing text and。natural
    language and the naivebayes，approach is based on Bayes rule which。you might recall
    back from when we，talked about probability that the Bayes。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 可能的类别，它不仅适用于文本，也适用于其他类型的概念，但在分析文本和自然语言的领域中相当流行，朴素贝叶斯方法基于贝叶斯规则，你可能还记得我们讨论概率时提到的贝叶斯。
- en: rule looks like this that the，probability of some event B given a can。be expressed
    using this expression over，here probability of B given a is the。probability of
    a given B multiplied by，the probability of B divided by the。probability of a and
    we saw that this，came about as a result of just the。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 规则看起来是这样的，给定某事件A的事件B的概率可以用这个表达式来表示，给定A的B的概率等于给定B的A的概率乘以B的概率除以A的概率，我们看到这只是因为。
- en: definition of conditional independence，and looking at what it means for two。events
    to happen together，this was our formulation then of Bayes，rule which turned out
    to be quite。helpful we were able to predict one，event in terms of another by flipping。the
    order of those events inside of this，probability calculation and it turns out。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 条件独立性的定义，以及两个事件一起发生的意义，这就是我们的贝叶斯规则的公式，结果证明它非常有用，我们能够通过翻转这些事件的顺序在这个概率计算中预测一个事件。
- en: this approach is going to be quite，helpful and we'll see why in a moment。for
    being able to do this sort of，sentiment analysis because I want to say。you know
    what is the probability that a，message is positive or what is the。probability
    that the message is negative，and I'll go ahead and simplify this just。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法将非常有帮助，我们稍后会看到原因。它能够进行情感分析，因为我想说，消息是积极的概率是多少，或消息是消极的概率是多少，我会简化这个。
- en: using the emojis just for simplicity，like probability of positive probability。of
    negative and that is what I would，like to calculate but I'd like to。calculate
    that given some information，given information like here is a sample。of text my
    grandson loved it and I would，like to know not just what is the。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用表情符号只是为了简单，比如积极的概率、消极的概率，这就是我想计算的，但我想在给定一些信息的情况下计算，比如这里是一个文本样本，我的孙子喜欢它，我想知道的不仅仅是什么。
- en: probability that any message is positive，but what is the probability that the。message
    is positive given my grandson，loved it as the text of the sample so。given this
    information that inside the，sample are the words my grandson loved。it what is
    the probability then that，this is a positive message well。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 任何消息是积极的概率是什么，但在给定我的孙子喜欢它作为样本文本的情况下，消息是积极的概率是什么？那么，在给定这个信息，即样本中包含单词“我的孙子喜欢它”的情况下，这个是积极消息的概率又是多少呢？
- en: according to the bag of words model what，we're going to do is really ignore
    the。ordering of the words not treat this as，like a single sentence that has some。structure
    to it but just treat it as a，whole bunch of different words we're。gonna say something
    like what is the，probability that this is a positive。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 根据词袋模型，我们将真正忽略单词的顺序，而不是将其视为有某种结构的单个句子，而是将其视为一堆不同的单词，我们将要说的是，这个是积极的概率是多少。
- en: message given that the word my was in，the message given that the word grandson。was
    in the message given that the word，loved within the message and given the。word
    it was in the message the bag of，words model here we're treating the。entire symp
    sample as just a whole bunch，of different words and so this then is。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 给定单词“我的”在消息中的情况下，给定单词“孙子”在消息中的情况下，给定单词“喜欢”在消息中的情况下，以及给定单词在消息中的情况下，词袋模型在这里我们将整个样本视为一堆不同的单词。
- en: what I'd like to calculate this，probability probability given although。words
    what is the probability that this，is a positive message and this is where。we can
    now apply Bayes rule this is，really the probability of some be given。some a and
    that now is what I'd like to。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我想计算的概率，给定这些单词，这个是积极消息的概率是多少，现在我们可以应用贝叶斯定理，这实际上是某个事件给定某个事件的概率，这正是我想要的。
- en: '![](img/2782ceb1249cc2bcb3d95101cf1913de_21.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2782ceb1249cc2bcb3d95101cf1913de_21.png)'
- en: calculate so we're according to Bayes，rule this whole expression is equal to。well
    it's the probability I switch the，order of them it's the probability of。all of
    these words given that it's a，positive message multiplied by the。probability that
    is a positive message，divided by the probability of all of。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 根据贝叶斯定理，这整个表达式等于……好吧，是我交换了它们的顺序，是所有这些单词在它是积极消息的情况下的概率，乘以它是积极消息的概率，除以所有单词的概率。
- en: those words so this then is just an，application of Bayes rule we've already。seen
    where I want to express the，positive probability of positive given。the words as
    related to somehow the，probability of the words given that it's。a positive message
    and it turns out that，is you might recall back when we talked。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这只是贝叶斯定理的一个应用，我们已经看到我想要将给定单词的积极概率表示为与积极消息的单词概率相关，结果是你可能会记得我们讨论过的。
- en: about probability that this denominator，is going to be the same regardless of。whether
    we're looking at positive or，negative messages the probability of。these words
    doesn't change because we，don't have a positive or negative down。below so we can
    just say that rather，than just say that this expression up。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 关于概率，这个分母无论我们看积极还是消极消息都是相同的，这些单词的概率并没有变化，因为我们下面没有积极或消极的东西，所以我们可以说，rather than
    just say that this expression up。
- en: here is equal to this expression down，below it's really just proportional to。just
    the numerator we can kind of ignore，the denominator for now using the。denominator
    would get us an exact，probability but it turns out that what。we're really just
    do is figure out what，the probability is proportional to and。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这里等于下面这个表达式，它实际上只是与分子成比例，我们可以暂时忽略分母，使用分母会得到一个确切的概率，但实际上我们要做的就是弄清楚概率与什么成比例。
- en: at the end we'll have to normalize the，probability distribution make sure the。probability
    distribution ultimately sums，up to the number one so now I've been。able to formulate
    this probability which，is what I want to care about is。proportional to multiplying
    these two，things together probability of words。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们必须归一化概率分布，确保概率分布最终的总和为一。所以现在我已经能够形成这个概率，这是我关心的，与这两件事相乘成比例，即单词的概率。
- en: given positive message multiplied by the，probability of positive message but。again
    if you think back to our，probability rules we can calculate this。really as just
    a joint probability of，all of these things happening that the。probability of a
    positive message，multiplied by the probability of these。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 给定正面消息，乘以正面消息的概率，但再次如果你回想我们的概率规则，我们实际上可以将其计算为所有这些事情发生的联合概率，即正面消息的概率乘以这些概率。
- en: words given the positive message well，that's just the joint probability of all。of
    these things this is the same thing，as the probability that it's a positive。message
    and my is in the sentence or in，the message and grandson is in the，sample and
    love。in the sample and it is in the sample so，using that rule for the definition
    of。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 给定正面消息的词，实际上就是这些事情的联合概率。这与它是正面消息的概率，以及 my 在句子或消息中，grandson 在样本中，love 在样本中，以及
    it 在样本中是一样的。所以，利用这个规则来定义。
- en: joint probability I've been able to say，that this entire expression is now。proportional
    to this sequence this joint，probability of these words and this。positive that's
    in there as well and so，now the interesting question is just how。to calculate
    that joint probability how，do I figure out the probability that。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 联合概率我能够说，这整个表达式现在是与这序列成比例的，这些词的联合概率以及其中的正面内容。所以，现在有趣的问题就是如何计算这个联合概率，我如何弄清楚概率。
- en: given some arbitrary message that it is，positive and the word my is in there
    and。the word grandson is in there and the，word loved is in there and the word
    it。is in there well you'll recall that we，can calculate a joint probability by。multiplying
    together all of these，conditional probabilities multiplying if。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 给定某个任意消息，它是正面的，并且其中包含单词 my，单词 grandson，单词 loved 和单词 it。你会记得，我们可以通过将所有这些条件概率相乘来计算联合概率。
- en: I want to know the probability of a and，B and C I can calculate that as the。probability
    of a time's the probability，of B given a time's the probability of C。given a and
    B I can just multiply these，conditional probabilities together in。order to get
    the overall joint，probability that I care about and we。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我想知道 A、B 和 C 的概率，我可以将其计算为 A 的概率乘以给定 A 的 B 的概率乘以给定 A 和 B 的 C 的概率。我可以将这些条件概率相乘，以获得我关心的总体联合概率。
- en: could do the same thing here I could say，let's multiply the probability of。positive
    by the probability of the word，my showing up in the message given that。it's positive
    multiplied by the，probability of grandson showing up in。the message given that
    the word my is in，there and that it's positive multiplied。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以在这里做同样的事情，我可以说，让我们将正面的概率与单词 my 在消息中出现的概率相乘，前提是它是正面的，乘以给定单词 my 在那里且它是正面的情况下，grandson
    出现在消息中的概率。
- en: by the probability of loved given these，three things multiplied by the。probability
    of it given these four，things and that's gonna end up being a。fairly complex calculation
    to make one，that we probably aren't gonna have a。good way of knowing the answer
    to like，what is the probability that grandson is。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 乘以给定这三样东西的 loved 的概率，乘以给定这四样东西的 it 的概率，而这将是一个相当复杂的计算，我们可能没有好的方法去知道答案，比如，孙子出现的概率是多少。
- en: in the message given that it is positive，and the word my is in the message that's。not
    something we're really gonna have a，readily easy answer to and so this is。where
    the naive part of naive Bayes，comes about we're gonna simplify this。notion rather
    than compute exactly what，that probability distribution is we're。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在消息中，前提是它是正面的，且单词 my 在消息中。这并不是我们会有一个容易回答的事情，这就是朴素贝叶斯的朴素之处。我们将简化这个概念，而不是精确计算这个概率分布。
- en: going to assume that these words are，going to be effectively independent of。each
    other if we know that it's already，a positive message that if it's a。positive
    message it doesn't change the，probability that the word grandson isn't。the message
    if I know that the word，loved isn't the message for example and。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设这些词在已知是积极信息的情况下彼此独立。如果这是一个积极的信息，那么“grandson”在消息中出现的概率并不会因为我知道“loved”不是消息而改变。
- en: that might not necessarily be true in，practice in the real world it might not。be
    the case that these words are，actually independent，but we're going to assume it
    to simplify。our model and it turns out that，simplification still lets us get pretty。good
    results out of it as well，and so what we're going to assume is。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际情况中，这可能并不一定成立，现实世界中这些词可能并不独立，但我们假设它们独立，以简化我们的模型。事实证明，这种简化仍然让我们获得相当不错的结果，所以我们要做的假设是。
- en: that the probability that all of these，words show up depend only on whether。it's
    positive or negative I can still，say that loved is more likely to come up。in a
    positive message than a negative，message which is probably true but we're。also
    going to say that it's not going to，change whether or not loved is more。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些词出现的概率仅仅取决于消息是积极还是消极。我仍然可以说，“loved”在积极信息中出现的可能性高于在消极信息中出现的可能性，这可能是对的，但我们也会说这不会改变“loved”出现的可能性。
- en: likely or less likely to come up if I，know that the word my is in the message。for
    example and so those are the，assumptions that we're going to make so。while top
    expression is proportional to，this bottom expression we're going to。say it's naively
    proportional to this，expression probability of being a。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我知道“my”这个词出现在消息中，那么它出现的可能性不会因为这是一个积极的信息而变得更可能或不太可能。这些是我们要做的假设，所以虽然上面的表达式与下面的表达式成正比，我们将简单地说它与这个表达式的概率成正比。
- en: positive message and then for each of，the words that show up in the sample I'm。going
    to multiply what's the probability，that my isn't the message given that。it's positive
    times the probability of，grandson being in the message given that。it's positive
    and then so on and so，forth for the other words that happen to。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 积极的信息，然后对于样本中出现的每个单词，我将乘以在已知这是积极的情况下，给定的不是消息的概率，乘以在已知这是积极的情况下，“grandson”出现在消息中的概率，然后依此类推，对其他出现的单词进行同样的处理。
- en: be inside of the sample and it turns out，that these are numbers that we can。calculate
    the reason we've done all of，this math is to get to this point to be。able to calculate
    this probability of，distribution that we care about given。these terms that we
    can actually，calculate and we can calculate them。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据会包含在样本中，结果是这些数字我们可以计算。我们之所以做这些数学运算，是为了能够计算我们关心的概率分布，基于这些我们实际上可以计算的项。
- en: given some data available to us and this，is what a lot of natural language。processing
    is about these days it's，about analyzing data if I give you a。whole bunch of data
    with a whole bunch，of reviews and I've labeled them as。positive or negative then
    you can begin，to calculate these particular terms I。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们可用的一些数据，这就是如今许多自然语言处理的内容，它涉及分析数据。如果我给你一堆标记为积极或消极的评论数据，那么你就可以开始计算这些特定的项。
- en: can calculate the probability that a，message is positive just by looking at。my
    data and saying all right how many，positive samples were there and divide。that
    by the number of total samples that，is my probability that a message is。positive
    what is the probability that，the word loved is in the message given。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以仅通过查看我的数据来计算一条消息是积极的概率，看看有多少个正样本，然后将其除以总样本数，这就是我认为一条消息是积极的概率，以及“loved”这个词出现在消息中的概率。
- en: that it's positive well I can calculate，that based on my data to let me just。look
    at how many positive samples have，the word loved in it and divide that by。my total
    number of positive samples and，that will give me an approximation for。what is
    the probability that love is，going to show up inside of the review。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这肯定是积极的，我可以根据我的数据来计算。让我看看样本中有多少个包含“love”这个词的正样本，并将其除以我的正样本总数，这将给我一个关于“love”在评论中出现的概率的近似值。
- en: given that we know that the review is，positive and so this then allows us to。be
    able to calculate these probabilities，so let's not actually do this，calculation
    let's calculate。for the sentence my grandson loved it is，that a positive or negative
    review how。could we figure out those probabilities，well again this up here is
    the。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们知道评论是正面的，因此这使我们能够计算这些概率。那么我们不妨进行这项计算，计算“我孙子喜欢它”这句话，是正面还是负面的评论。我们如何能够得出这些概率呢？再次上面的数据是。
- en: expression we're trying to calculate and，I'll give you here the data that is。available
    to us and the way to interpret，this data in this case is that of all of。the messages
    forty-nine percent of them，were positive and fifty-one percent of。them were negative
    maybe online reviews，tend to be a little bit more negative。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要计算的表达式，以及在这种情况下可用的数据。解读这些数据的方式是，在所有消息中，49%的消息是正面的，51%的消息是负面的，或许在线评论往往会稍微偏向负面。
- en: than they are positive or at least based，on this particular data sample that's。what
    I have and then I have，distributions for each of the various。different words that
    given that it's a，positive message how many positive。messages had the word my
    in them you，know it's about 30% and for negative。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 他们是正面的，至少基于这个特定的数据样本。这就是我所拥有的，然后我有各种不同词的分布，假设这是一个正面消息，那么有多少正面消息包含“我”这个词呢？你知道，大约是30%，而对于负面消息。
- en: messages how many of those had the word，my in them about 20% so it seems like。the
    word my comes up more often in，positive messages at least slightly more。often
    based on this analysis here，grandson for example maybe that showed。up in 1% of
    all positive messages and 2%，of all negative messages had the word。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 消息中有多少条包含“我”的词，大约是20%。所以似乎“我”这个词在正面消息中出现得更频繁，至少在这个分析中稍微多一些。以“孙子”为例，可能在1%的所有正面消息中出现，而在2%的所有负面消息中出现。
- en: grandson in it the word loved showed up，in 32% of all positive messages 8% of。all
    negative messages for example and，then the word it showed up in 30% in。positive
    messages 40% of negative，messages again this arbitrary data here。just for example
    but now we have data，with which we can begin to calculate。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: “孙子”这个词出现在32%的所有正面消息中，8%的所有负面消息中，例如，“它”这个词在30%的正面消息中出现，而在40%的负面消息中再次出现，这里是一些任意的数据，仅供参考，但现在我们有了可以开始计算的数据。
- en: this expression so how do I calculate，multiplying all these values together。well
    it's just going to be multiplying，probability that's positive times the。probability
    of my given positive times，the probability of grandson given。positive so on and
    so forth for each of，the other words and if you do that。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这个表达式，我该如何计算呢？将所有这些值相乘。这实际上是正面概率乘以我所给定的正面概率，再乘以孙子在正面消息中的概率，依此类推，针对其他每个单词，如果你这样做的话。
- en: multiplication multiply all of those，values together you get this point 0 0
    0。1 4 1 1 2 by itself this is not a，meaningful number but it's going to be。meaningful
    if you compare this，expression the probability that it's。positive times the probability
    of all of，the words given that I know that the。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些值相乘，你会得到这个0.00014112，单独看这个数字并没有什么特别的意义，但如果你将这个表达式与我知道它是正面的概率相乘，再乘以给定的所有词的概率。
- en: message is positive and compare it to，the same thing but for negative。sentiment
    messages instead I want to，know the probability that it's a。negative message times
    the probability，of all of these words given them it's a。negative message and so
    how might I do，that well to do that you just multiply，probability。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 消息是正面的，并且将其与负面情感消息进行比较。我想知道它是负面消息的概率，乘以给定的所有这些词的概率，得出这是一个负面消息的概率。那么我该如何做到这一点呢？为了做到这一点，你只需将负面概率与所有这些条件概率相乘。
- en: negative times all of these conditional，probabilities and if I take those five。values
    multiply all of them together，then what I get is this value for。negative point
    zero zero zero zero six，five to eight again in isolation not a。particularly meaningful
    number what is，meaningful is treating these two values。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我将这五个值相乘，那么我得到的值是负面0.00006528，再次强调，这个数值在孤立状态下并没有特别的意义，真正有意义的是处理这两个值。
- en: as a probability distribution and，normalizing them making it so that both。of
    these values and sum up to one the，way a probability distribution should。and we
    do so by like adding these two up，and then dividing each of these values。by their
    total - in order to be able to，normalize it and when we do that when we。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种概率分布，并且，通过归一化它们，使得这两个值的总和为1，这就是概率分布应有的方式。我们通过将这两个值相加，然后将每个值除以它们的总和来实现归一化——以便能够做到这一点。
- en: normalize this probability distribution，you end up getting something like this。positive
    point six eight three seven，negative point three one six three it。seems like we've
    been able to conclude，that we are you know about 68% confident。we think there's
    a probability of 0。68，that this message is a positive message。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当归一化这个概率分布时，你最终得到的结果大概是这样的：正面0.6837，负面0.3163。这似乎让我们能够得出结论，我们对这个消息的正面概率大约有68%的信心。
- en: my grandson loved it and why are we 68，percent confident well it seems like。we're
    more confident than not because，the word loved showed up in 32% of。positive messages
    but only 8% of，negative messages so that was a pretty。strong indicator and for
    the others well，it's true that like the word it showed。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: “我孙子喜欢这个”，为什么我们有68%的信心呢？似乎我们比不更有信心，因为“喜欢”这个词在32%的正面消息中出现，但在8%的负面消息中仅出现，因此这是一个相当强的指标。而对于其他词来说，确实像是这个词出现。
- en: up more often in negative messages it，wasn't enough to offset that love shows。up
    far more often in positive messages，than negative messages and so this type。of
    analysis is how we could apply naive，Bayes we've just done this calculation。and
    we end up getting not just a，categorization of positive or negative。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在负面消息中更常出现，无法抵消那种爱在积极消息中远远更常出现，因此这种类型的分析就是我们如何应用朴素贝叶斯，我们刚刚进行了这个计算，最终不仅得到了正面或负面的分类。
- en: but I get some sort of confidence that，like what do I think the probability
    is。that it's positive and I can say you，know I think it's positive with this。particular
    probability and so naive，Bayes can be quite powerful at trying to。achieve this
    using just this bag of，words model where all I'm doing is。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 但我获得了一种信心，比如我认为它是正面的概率是什么。我可以说，我认为它是正面的概率是这样，因此朴素贝叶斯在尝试实现这一点时可以非常强大，只需使用这个词袋模型。
- en: looking at what words show up in the，sample I'm able to draw these sorts of。conclusions
    now one potential drawback，something that you'll notice pretty。quickly if you
    start applying this rule，exactly as is is what happens depending。on if zeroes
    are inside this data，somewhere let's imagine for example this。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看样本中出现的单词，我能够得出这些结论。现在一个潜在的缺点是，如果你开始严格按照这个规则应用，你会很快注意到的事情是，数据中如果包含零的话会发生什么，假设例如这个情况。
- en: same sentence my grandson loved it but，let's instead imagine that this value。here
    instead of being point zero one，with zero meaning inside of our dataset。it has
    never before happened that in a，positive message the word grandson。showed up and
    that's certainly possible，you know if I have a pretty small。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的句子“我孙子喜欢这个”，但我们假设这里的值不是0.01，而是在我们的数据集中，从未发生过在正面消息中出现“孙子”这个词，这确实是可能的，如果我有一个相对较小的。
- en: dataset it's probably likely that not，all the messages are gonna have the word。grandson
    maybe it is the case that no，positive messages have ever had the word。grandson
    in it at least in my dataset，but if it is the case that 2 percent of。the negative
    messages have still had the，word grandson in it then we run into an。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可能并不一定所有消息都会包含“孙子”这个词，也许在我的数据集中没有任何正面消息包含“孙子”这个词，但如果有2%的负面消息仍然包含“孙子”这个词，那我们就会遇到一个问题。
- en: interesting challenge and the challenge，is this when I multiply all of the。positive
    numbers together and multiply，all the negative numbers together to。calculate these
    two probabilities what I，end up getting is a positive value of。zero point zero
    zero zero I get pure，zeros because when I multiply all of。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的挑战在于，当我将所有正数相乘并将所有负数相乘以计算这两种概率时，最终得到的是一个值为零的正值。我得到的是纯零，因为当我将所有的。
- en: these numbers together when I multiply，something by zero it doesn't matter what。the
    other numbers are the result is，going to be zero and the same thing can。be said
    of negative numbers as well so，this then would seem to be a problem。that because
    grandson has never showed，up in any of the positive messages。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当我将某个数乘以零时，无论其他数字是什么，结果都会是零，负数也是如此，因此这似乎是一个问题，因为“孙子”从未出现在任何正面消息中。
- en: inside of our symbol we're able to say，we seem to be concluding that there is
    a。zero percent chance that the message is，positive and therefore it must be。negative
    because in the only cases where，we've seen the word grandson come up is。inside
    of a negative message and in，doing some we've totally ignored all of。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的符号内部，我们似乎得出的结论是，信息是正面的概率为零，因此它必须是负面的，因为我们看到“孙子”这个词的唯一情况是在负面消息中。
- en: the other probabilities that a positive，message is much more likely to have
    the。word loved in it because we've，multiplied by zero which just means none。of
    the other probabilities can possibly，matter at all so this then is a。challenge
    that we need to deal with it，means that we're likely not going to be。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正面消息更可能包含“爱”这个词，因为我们乘以零，这意味着其他概率完全无关紧要，因此这是我们需要面对的挑战，这意味着我们可能不会每个值在我们的分布中，以便稍微平滑数据。
- en: able to get the correct results if we，just purely use this approach and it's。for
    that reason there are a number of，possible ways we can try and make sure。that
    we never multiply something by zero，it's okay to multiply something by a。small
    number because then it can still，be counterbalanced by other larger。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们纯粹使用这种方法，就能够获得正确的结果，正因为如此，有很多方法可以确保我们不会将某个东西乘以零，将某个东西乘以一个小数字是可以的，因为它可以被其他更大的数字抵消。
- en: numbers but multiplying by zero sort of，means it's the end of the story you。multiply
    a number by zero and the output，is going to be zero no matter how big。any of the
    other numbers happened to be，so one approach that's fairly common in。naive Bayes
    is this idea of additive，smoothing adding some value alpha to。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 但将数字乘以零似乎意味着故事结束了，你将一个数字乘以零，输出将是零，无论其他任何数字有多大，因此，在朴素贝叶斯中，一个相对常见的方法是这种加法平滑的想法，给其他概率加上一个值α。
- en: each of the values in our distribution，just to smooth the data a little bit
    one。such approach is called Laplace moving。![](img/2782ceb1249cc2bcb3d95101cf1913de_23.png)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一种这样的方式称为拉普拉斯平滑。 ![](img/2782ceb1249cc2bcb3d95101cf1913de_23.png)
- en: which basically just means adding one to，each value in our distribution so if
    I，have。100 samples and 0 of them contain the，word grandson well then I might say
    that。you know what instead let's pretend that，I've seen had one additional sample。where
    the word grandson appeared and one，additional sample where the word。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上意味着对我们分布中的每个值加一，所以如果我有100个样本，且其中0个包含“孙子”这个词，我可能会说，不如假设我看到了一个额外的样本，其中出现了“孙子”这个词。
- en: grandson didn't appear so I'll say that，all right now I have 1 out of 102 so
    one。sample that does have the word grandson，out of 102 total I'm basically creating。two
    samples that didn't exist before but，in order in doing so I've been able to。smooth
    the distribution a little bit to，make sure that I never have to multiply。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: “孙子”没有出现，所以我会说，现在我有102个样本中有1个样本包含“孙子”这个词，我基本上创造了两个之前不存在的样本，但通过这样做，我已经能够稍微平滑分布，以确保我从未乘以数字。
- en: anything by 0 by pretending I've seen，one more value in each category than I。actually
    have and this gets us at result，of that it's not having to worry about。multiplying
    a number by 0 so this then，is an approach that we can use in order。to try and
    apply naive Bayes even in，situations where we're dealing with。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通过假设每个类别中都有一个额外的值，我实际上没有的，这让我们得出了一个结果，那就是不必担心将数字乘以零，因此这是我们可以使用的方法，以便在处理的情况下应用朴素贝叶斯。
- en: words that we might not necessarily have，seen before and let's now take a look
    at。how we could actually apply that in，practice that it turns out that MLT came。in
    addition having the ability to，extract engrams and tokenize things into。words
    also has the ability to be able to，apply naive Bayes on some samples of。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何在实践中应用它，事实证明，MLT不仅具有提取n-gram和将内容标记为单词的能力，还能够在某些样本中应用朴素贝叶斯。
- en: text for example and so let's go ahead，and do that what I've done is inside
    of。sentiment I've prepared a corpus of just，you know reviews that I have generated。but
    you could imagine using real reviews，or I just have a couple of positive。reviews
    it was great so much fun would，recommend my grandson loved it those。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 例如文本，所以让我们继续进行，我在 sentiment 中准备了一个语料库，包含我生成的评论，但你可以想象使用真实评论，或者我只有几条正面评论，比如“太棒了”，“非常有趣”，“会推荐给我孙子，他喜欢这些”。
- en: sorts of messages and then I have a，whole bunch of negative reviews not。worth
    it kind of cheap really bad didn't，work the way we expected just one in。each line
    a whole bunch of positive，reviews and negative reviews and what。I'd like to do
    now is analyze them，somehow so here then is sentiment dot pi。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 各种消息，我有一堆负面评论，真的不值得，便宜得让人失望，没有按预期工作。每一行都是一堆正面评论和负面评论，我现在想做的是以某种方式分析它们，所以这里是
    sentiment.dot.pi。
- en: and what we're going to do first is，extract all of the positive and negative。sentences
    create a set of all of the，words that were used across all of the。messages and
    then we're going to go，ahead and train NLT K is naive Bayes。classifier on all
    of this training data，and what the training data effectively。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的第一件事是提取所有正面和负面的句子，创建一个使用过的所有单词的集合，然后我们将继续训练 NLTK 的朴素贝叶斯分类器，使用所有这些训练数据，而训练数据的有效性。
- en: is if I take all of the positive，messages and give them the label。positive all
    the negative messages and，give them the label negative and then。I'll go ahead
    and apply this classifier，to it where I'd say I would like to take。all of this
    training data and now have，the ability to classify it as positive，or negative。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我将所有正面的消息标记为正面，将所有负面的消息标记为负面，然后。我会继续将这个分类器应用于它，我会说我希望将所有的训练数据分类为正面或负面。
- en: I'll then take some input from the user，they can just type in some sequence
    of。words and then I would like to classify，that sequence as either positive or。negative
    and then I'll go ahead and，print out what the probabilities of each。happen to
    be and there are some helper，functions here that just organize things。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我会从用户那里获取一些输入，他们可以输入一串单词，然后我想将该序列分类为正面或负面，然后我将打印出每个分类的概率，并且这里有一些辅助函数来组织这些内容。
- en: in the way that ltk is expecting them to，be but the key idea here is that I'm。taking
    the positive messages labeling，them taking the negative messages。labeling them
    putting them inside of a，classifier and then now trying to。![](img/2782ceb1249cc2bcb3d95101cf1913de_25.png)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以ltk所期望的方式呈现，但关键思想是我正在将正面消息标记，将负面消息标记，然后放入分类器中，现在尝试进行分类。![](img/2782ceb1249cc2bcb3d95101cf1913de_25.png)
- en: classify some new text that comes about，so let's go ahead and try it I'll go。ahead
    and go into sentiment and we'll，run Python sentiment passing in as input。that
    corpus that contains all of the，positive and negative messages because。depending
    on the corpus that's going to，affect the probabilities the。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 分类一些新文本，所以让我们继续尝试，我将进入情感分析，并运行 Python sentiment，将包含所有正面和负面消息的语料库作为输入，因为根据语料库的不同，这将影响概率。
- en: effectiveness of our ability to classify，is entirely dependent on how good our。data
    is and how much data we have and，how well they happen to be labeled so。now I can
    try something and say like try，review like this was great just some。review that
    I might leave and it seems，that all right there is a 96% chance。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分类的有效性完全依赖于我们的数据有多好，以及我们拥有多少数据，以及这些数据标记得有多好。所以我现在可以尝试一下，像是尝试这样的评论：“太棒了”，就像我可能会留下的评论，似乎有
    96% 的机会。
- en: that estimates that this was a positive，message 4% chance that it was a negative。likely
    because the word grape shows up，inside of the positive messages but。doesn't show
    up inside of the negative，messages and that might be something。that our AI is
    able to capitalize on and，really what it's going to look for are。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 估计这是一个正面消息，4% 的机会是负面的，可能是因为“葡萄”这个词出现在正面消息中，但不出现在负面消息中，这可能是我们的 AI 能够利用的一个方面，实际上它会寻找的是。
- en: the differentiating words that if the，probability of words like this and was。is
    pretty similar between positive and，negative words then the naive Bayes。classifier
    isn't going to end up using，those values as having some sort of。importance in
    the algorithm because if，they're the same on both sides and you。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 区分性的词汇，如果这些词的概率在正面和负面词汇之间相似，那么朴素贝叶斯分类器就不会使用这些值，因为它们在两边是相同的，因此不会在算法中具有某种重要性。
- en: multiply that value for both positive，and negative you end up getting about。the
    same thing what ultimately makes the，difference in naive Bayes is when you。multiply
    by a value that's much bigger，for one category than for another。category when
    one word like great is，much more likely to show up in one type。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于正面和负面的值进行相乘，最终得到的结果差不多。最终，影响朴素贝叶斯分类器的区别在于，当你为一个类别乘以一个远大于另一个类别的值时，当一个词如"好"在某一种类型中出现的可能性远高于其他类型时。
- en: of message than another type of message，and that's one of the nice things about。naive
    Bayes that without me telling it，that great is more important to care。about than
    this or was naive Bayes can，figure that out based on the data it can。figure out
    that this shows up about the，same amount of time between the two but。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一种消息与另一种消息之间的差别，这是朴素贝叶斯分类器的一个好处，尽管我没有告诉它，"好"比"坏"更重要，但朴素贝叶斯可以根据数据来判断，它可以发现这两者出现的频率大致相同，但。
- en: great that is a discriminator a word，that can be different between the two。types
    of messages so I could try it，again type in a sentence like lots of。fun for example
    all right this one it's，a little less，sure about 62% chance that is positive。37%
    chance that it's negative maybe，because there aren't as clear。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '"好"是一个区分器，可以在这两种消息类型之间有所不同，因此我可以再试一次，输入一个句子，比如"非常有趣"，例如，这个句子就有点不那么确定，62%的概率是正面，37%的概率是负面，可能是因为不够明确。'
- en: discriminators or differentiators inside，of the state up I'll try one more say。like
    kind of overpriced and all right，now 95% 96% sure that this is a negative。sentiment
    likely because of the word，overpriced because it's shown up in a。negative sentiment
    expression before and，therefore things you know what this is。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在状态内部的区分器或区分因素，我再尝试一次，比如说"价格过高"，现在95%到96%肯定这是一个负面情绪，可能是因为"价格过高"这个词之前在负面情绪表达中出现过，因此你知道这是什么。
- en: probably going to be a negative sentence，and so now you've base has now given
    us。the ability to classify text given，enough training data given enough。examples
    we can train our AI to be able，to look at natural language human words。figure
    out which words are likely to，show up and positive as opposed to。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这很可能是一个负面句子，因此现在朴素贝叶斯给了我们能力来分类文本，给定足够的训练数据和示例，我们可以训练我们的AI来分析自然语言中的人类词汇，找出哪些词更可能出现在正面而非。
- en: negative sentiment messages and，categorize them accordingly and you。could imagine
    doing the same thing，anytime you want to take text and group。it into categories
    if I want to take an，email and categorize that as email or as。a good email or
    as a spam email you，could apply a similar idea try and look。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 负面情绪消息中，并据此进行分类，你可以想象在任何时候想要将文本分组为类别时做同样的事情。如果我想将一封电子邮件分类为邮件、好邮件或垃圾邮件，你可以应用类似的想法，尝试查看。
- en: for the discriminating words the words，that make it more likely to be a spam。email
    or not and just train a naive，Bayes classifier to be able to figure。out what that
    distribution is and to be，able to figure out how to categorize an。email as good
    or a spam now of course，it's not going to be able to give us a。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 针对具有区分性的词汇，这些词使得一封邮件更有可能被判定为垃圾邮件或不是垃圾邮件，并且仅仅训练一个朴素贝叶斯分类器，以便能够判断这种分布是什么，并能够判断如何将一封邮件分类为好邮件或垃圾邮件。当然，它不会给我们一个。
- en: definitive answer it gives us a，probability distribution something like。63%
    positive and 37% negative and that，might be why our spam filters and our。emails
    sometimes make mistakes sometimes，think that a good email is actually spam，or
    vice versa。because ultimately the best that it can，do is calculate a probability。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 它给出的不是确定的答案，而是一个概率分布，比如63%为正面，37%为负面，这可能是我们垃圾邮件过滤器和电子邮件有时出错的原因，有时会认为一封好的邮件实际上是垃圾邮件，反之亦然。因为最终，它能做的只是计算概率。
- en: distribution these natural language is，ambiguous we can usually just deal in。the
    world of probabilities to try and，get an answer that is reasonably good。even if
    we aren't able to guarantee for，sure that it is the number that we。actually expect
    for it to be so that，then was a look and how we can begin to。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言的分布是模糊的，我们通常可以在概率的世界中处理，试图得到一个合理的答案，即使我们不能确保这确实是我们所期望的数字，因此，我们可以看看如何开始处理这个问题。
- en: take some text and to be able to analyze，the text and group it into some sorts
    of。categories but ultimately in addition，just being able to analyze text and。categorize
    it we'd like to be able to，figure out information about the text。get at some sort
    of meaning out of the，text as well and this starts to get us。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 处理一些文本，并能够分析这些文本，将其归类为某些类别，但最终除了能够分析文本和分类之外，我们还希望能够，获取文本的信息，从文本中提炼出某种意义，这开始让我们深入思考。
- en: in the world of information of being，able to try and take data in the form of。text
    and retrieve information from it so，one type of problem is known as。information
    retrieval or ir which is the，task of like finding relevant documents。in response
    to a query so this is，something like you type in a query into。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在信息世界中，能够尝试以文本形式获取数据并从中检索信息，因此，一类问题被称为信息检索（IR），即在响应查询时寻找相关文档的任务。这就像你在某处输入查询。
- en: a search engine like Google or you're，typing in something into some system。that's
    going to look for inside of a，library catalog for example that's going。to look
    for responses to a query I want，to look for documents that are about you。know
    the US Constitution or something，and I would like to get a whole bunch of。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 像谷歌这样的搜索引擎，或者你在某个系统中输入某些内容，这将会在图书馆目录中查找，例如，这将会寻找对查询的响应，我想寻找关于美国宪法的文档，或者其他类似的东西。
- en: documents that match that query，back to me but you might imagine that。what I
    really want to be able to do is，in order to solve this task effectively。I need
    to be able to take documents and，figure out what are those documents。about I want
    to be able to say like what，is it that these particular documents。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 与匹配该查询的文档相联系，你可以想象我真正想要做的是，为了有效解决这个任务，我需要能够处理文档，找出这些文档的内容。我想知道这些特定文档到底在讲什么。
- en: are about one of the topics of those，documents so that I can then more。effectively
    be able to retrieve，information from those particular。documents and this refers
    to a set of，tasks generally known as topic modeling。![](img/2782ceb1249cc2bcb3d95101cf1913de_27.png)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 是关于那些文档的主题之一，这样我就能更有效地从这些特定文档中检索信息，这涉及到一组通常被称为主题建模的任务。![](img/2782ceb1249cc2bcb3d95101cf1913de_27.png)
- en: where I'd like to discover what the，topics are for a set of documents and。this
    is something that humans could do a，human could read a document and tell you。alright
    here's what this document is，about and give maybe a couple of topics。for who are
    the important people in this，document what are the important objects。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望探索一组文档的主题，这正是人类可以做到的，人类可以阅读文档并告诉你，好的，这个文档是关于什么的，并给出几个主题，比如文档中重要的人物和重要的对象。
- en: in the document can probably tell you，that kind of thing but we like for AI
    to。be able to do the same thing like given，some document can you tell me what
    the。important words in this document are，what are the words that set the stock。unit
    apart that I might care about if，I'm looking up documents based on。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 文档可以告诉你那类事情，但我们希望人工智能也能做到这一点，比如给定某个文档，你能告诉我这个文档中的重要词汇是什么吗？哪些词汇使得股票单位显得特别，如果我根据这些文档进行查找。
- en: keywords for example and so one，instinctive idea and intuitive idea that。![](img/2782ceb1249cc2bcb3d95101cf1913de_29.png)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 例如关键词，因此一种本能和直观的想法是。![](img/2782ceb1249cc2bcb3d95101cf1913de_29.png)
- en: probably makes sense is let's just use，term frequency term frequency is just。defined
    as the number of times a，particular term appears in a document if。I have a document
    with a hundred words。![](img/2782ceb1249cc2bcb3d95101cf1913de_31.png)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 可能比较合理的是，我们就用术语频率，术语频率被定义为特定术语在文档中出现的次数，如果我有一份包含一百个单词的文档。![](img/2782ceb1249cc2bcb3d95101cf1913de_31.png)
- en: and you know one particular word shows，up ten times it has a term frequency
    of。ten all right it shows up pretty often，maybe that's going to be an important。word
    and sometimes you'll also see this，frames is like a proportion of the total。number
    of words so ten words out of 100，maybe it has a term frequency of 0。1。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道某个特定的词出现了十次，它的词频就是十，没错，它出现得相当频繁，也许这将是一个重要的词。有时你还会看到这个框架，就像是总词数的一个比例，十个词中有100个，可能它的词频是0.1。
- en: meaning like 10 percent of all of the，words are this particular word that I。care
    about ultimately that doesn't，change relatively how important they are。for any
    one particular document but，they're the same idea the idea is look。for words that
    show up more frequently，because those are more likely to be the。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 意味着所有词中有10%是我关心的这个特定词，最终这并不会相对改变它们在任何特定文档中的重要性，但它们的理念是寻找出现频率较高的词，因为这些词更可能是。
- en: horton words inside of a corpus of。![](img/2782ceb1249cc2bcb3d95101cf1913de_33.png)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 霍顿词在一个语料库中的分布。![](img/2782ceb1249cc2bcb3d95101cf1913de_33.png)
- en: documents and so let's go ahead and give，that a try let's say I wanted to find。out
    what the Sherlock Holmes stories are，about I have a whole bunch of Sherlock。Holmes
    stories and I want to know in，general what are they about what are the。important
    characters who are the，important what are the important objects。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 文档，所以让我们试试看，比如我想找出福尔摩斯的故事是关于什么的，我有一堆福尔摩斯的故事，我想大致了解它们是关于什么的，重要角色是谁，重要的是什么物品。
- en: what are the important parts of this，story just in terms of words and I'd。like
    for the AI to be able to figure，that that on its own and will do so by。looking
    at term frequency by looking at，what are the words that show up the most。often
    so we'll go ahead and I'll go，ahead and go into the tf-idf directory。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个故事的重要部分是什么，仅仅从词的角度来看，我希望AI能够自己弄清楚这点，它将通过查看词频来实现，找出出现得最频繁的词。因此我们将继续，我将进入tf-idf目录。
- en: you'll see why it's called that in a，moment though let's first open up TF 0。pi
    which is going to calculate the top，10 term frequencies or maybe top 5 term。frequencies
    for a corpus of documents a，whole bunch of documents where each。document is just
    a story from Sherlock，Holmes we're going to load all the data。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你会很快明白为什么叫这个名字，但让我们先打开TF 0。pi，它将计算文档语料库中前10个词频，或者也许是前5个词频，文档是来自福尔摩斯的故事，我们将加载所有数据。
- en: into your corpus we're gonna figure out，what are all of the words that show
    up。inside of that corpus and we're going to，basically just assemble all of the。number
    of the term frequencies we're，gonna calculate how often do each of。these terms
    appear inside of the，document and we'll print out the top。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 进入你的语料库，我们将弄清楚，所有出现在该语料库中的词是什么，我们基本上只需汇总所有的词频，计算每个词在文档中出现的频率，然后打印出前。
- en: size and so there are some data，structures involved that you can take a。look
    at if you'd like Jim the exact code，is not so much important is the idea of。what
    we're doing we're taking each of，these documents and extracting first。sorting
    them we're saying like take all，the words that show up and sort them by。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有一些数据结构可以查看，如果你愿意，吉姆，确切的代码并不是那么重要，重要的是我们正在做的事情，我们将这些文档提取出来，首先进行排序，我们说像是取出所有出现的词并按。
- en: how often each word shows up and let's，go ahead and just for each document save。at
    the top 5 terms that happen to show，up in each of those documents so again。some
    helper functions you can take a。![](img/2782ceb1249cc2bcb3d95101cf1913de_35.png)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 每个词出现的频率，让我们为每个文档保存出现在这些文档中的前5个词。因此，一些辅助函数你可以查看。![](img/2782ceb1249cc2bcb3d95101cf1913de_35.png)
- en: look at if you're interested but the key，idea here is that all we're going to
    do。is run T of 0 on the Sherlock Holmes，stories and what I'm hoping to get out。of
    this process is I am hoping to figure，out what are the important words in。Sherlock
    Holmes for example so we'll go，ahead and run this and see what we get。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感兴趣可以查看，但这里的关键思想是我们要做的就是在福尔摩斯的故事上运行T 0，我希望从这个过程中弄清楚福尔摩斯中重要的词是什么，例如，我们将运行这个，看看结果。
- en: and it's loading the data and here's，what we get for this particular story。the
    important words are the and and an i，and ii and of those are the words that。show
    up more frequently in this，particular story gets the and and an i，and a and of。all
    right this is not particularly，useful to us like we're using term。
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 数据正在加载，这就是我们为这个特定故事得到的结果。重要的词是“the”、“and”、“an”、“i”、“ii”，这些词在这个故事中出现得最频繁，得出的词是“the”、“and”、“an”、“i”、“a”、“of”。
- en: frequencies we're looking at what words，show up the most frequently in each
    of。these various different documents but，what we get naturally are just the words。that
    show up a lot in English like the，word the' and of and and happen to show。up a
    lot in english and therefore they，happen to show up a lot in each of these。
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关注的是哪些词在这些不同文档中出现得最频繁，但我们自然获得的只是英语中出现频率很高的词，比如“the”、“of”和“and”，因此它们在这些文档中也很常见。
- en: various different documents this is not，a particularly useful metric for us
    to。be able to analyze what words are，important because these words are just。part
    of the grammatical structure of，english and it turns out we can。categorize words
    into a couple of，different categories these words happen。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不同的文档来说，这对我们分析重要词汇并没有特别大的帮助，因为这些词只是英语语法结构的一部分。事实证明，我们可以将词汇分为几类，而这些词正好符合。
- en: to be known as what we might call，function words functions the words that。have
    little meaning on their own but，that are used to grammatically connect。different
    parts of a sentence these are，words like am and buy and do when is and。![](img/2782ceb1249cc2bcb3d95101cf1913de_37.png)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些词被称为功能词，它们本身几乎没有意义，但用于在句子中语法性地连接不同部分。这些词包括“am”、“buy”、“do”、“when”等等。
- en: which and with and yet words that on，their own like what do they mean it's。hard
    to say they get their meaning from，how they connect to different parts of。the
    sentence and these function words。![](img/2782ceb1249cc2bcb3d95101cf1913de_39.png)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 像“which”、“with”和“yet”这样的词单独来看，它们的意思难以界定，它们的意义来自于它们如何连接句子的不同部分，而这些功能词也是如此。
- en: are what we may call a closed class of，words in a language like English there's。really
    just some fixed list of function，words and they don't change very often。there's
    just some list of words that are，commonly used to connect other。grammatical structures
    in the language，and that's in contrast with what we。
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在英语等语言中，我们可以称之为一个封闭的词类，实际上只是一些固定的功能词列表，它们很少变化。这些词是常用来连接语言中其他语法结构的，与我们所称之的内容词形成对比。
- en: might call content words words that，carry meaning independently words like。![](img/2782ceb1249cc2bcb3d95101cf1913de_41.png)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我们并没有特别大的帮助，我们所使用的术语可能被称为内容词，这些词独立承载意义，例如。
- en: algorithm category computer words that，actually have some sort of meaning and。these
    are usually the words that we care。![](img/2782ceb1249cc2bcb3d95101cf1913de_43.png)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 算法类别的计算机词汇实际上有某种意义，而这些通常是我们关心的词。
- en: about these are the words where we want，to figure out you know what are the。important
    words in a document we，probably care about the content words。more than we care
    about the function，words and so one strategy we could apply。is just ignore all
    of the function words，so here in TF one dot pi I've done the。
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些词，我们想弄清楚在文档中哪些是重要的词汇。我们可能更关心内容词，而不是功能词，因此一种策略是忽略所有的功能词。在TF one dot pi中，我已经完成了这个。
- en: same exact thing except I'm going to，load a whole bunch of words from a。functions
    words dot text file inside of，which are just a whole bunch of function。words in
    alphabetical order these are，just a whole bunch of function words。that are just
    words that are used to，connect other words in English and。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 完全相同的事情，除了我将从一个名为functions words dot text的文件中加载一大堆词，这些词是按字母顺序排列的功能词。这些只是一些用于连接英语中其他词的功能词。
- en: someone has just compiled this，particular list and these are the words。that
    I just want to ignore if any of，these words let's just ignore it as one。of the
    top terms because these are not，words that I probably care about if I。want to
    analyze what the important terms，inside of a document happen to be。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 有人编制了这个特定的列表，这些词是我想要忽略的。如果这些词中的任何一个出现，我们就把它当作顶级术语来忽略，因为如果我想分析文档中的重要术语，这些词可能并不重要。
- en: so in tf-idf one what we're Altima doing，is if the word is in my set of function。words
    I'm just going to skip over it，just ignore any of the function words by。continuing
    on to the next word and then，just calculating the frequencies for。those words
    instead so I'm gonna pretend。
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在TF-IDF中，我们实际上做的是，如果这个词在我的功能词集中，我会跳过它，忽略任何功能词，继续下一个词，并计算那些词的频率。因此，我将假装。
- en: '![](img/2782ceb1249cc2bcb3d95101cf1913de_45.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2782ceb1249cc2bcb3d95101cf1913de_45.png)'
- en: the function words aren't there and now，I see maybe I can get a better sense
    for。what terms are important in each of the，various different Sherlock Holmes。stories
    so now let's run T f1 on the，sherlock holmes corpus and see what we。get now and
    let's look at like what is，the most important term in each of the。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 功能词不在这里，现在我能更好地理解每个不同福尔摩斯故事中哪些词是重要的。现在让我们在福尔摩斯语料库上运行TF-IDF，看看我们能得到什么，让我们看看每个故事中最重要的词是什么。
- en: story ISM well it seems like for each of，the stories all right the most important。word
    is holes I guess that's what we，would expect they're all Sherlock Holmes。stories
    and Holmes is not a function，word it's not the or a or an so it。wasn't ignored
    but Holmes and man like，these are probably not what I mean when。
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 故事ISM，好吧，似乎每个故事中最重要的词是“福尔摩斯”。我想这是我们所期望的，因为它们都是福尔摩斯的故事，而“福尔摩斯”不是一个功能词，它不是“the”或“a”或“an”，所以没有被忽略，但“福尔摩斯”这些词可能不是我所指的。
- en: I say what are the important words even，though Holmes does show up the most。often
    it's not giving me a whole lot of，information here about what each of the。different
    Sherlock Holmes stories are，actually about and the reason why is。because Sherlock
    Holmes shows up in all，the stories and so it's not meaningful。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我想知道哪些词是重要的，尽管“福尔摩斯”出现得最频繁，但这并没有给我提供很多关于不同福尔摩斯故事实际内容的信息，原因是“福尔摩斯”出现在所有故事中，因此它没有意义。
- en: for me to say that like this story is，about Sherlock Holmes if I want to try。and
    figure out the different topics，across the corpus of documents what I。really want
    to know is you know what，words show up in this document that show。up less frequently
    in the other，documents for example and so to get at。
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我说这个故事是关于福尔摩斯的，我想知道在文档中出现的单词，与其他文档中出现的频率较低的单词是什么。因此，我真正想知道的是，哪些词在这篇文档中出现，而在其他文档中出现的频率较低。
- en: that idea we're going to introduce the，notion of inverse document frequency。inverse
    document frequency is a measure，of how common or rare a word happens to。be across
    an entire corpus of words and，mathematically it's usually calculated。like this
    as the logarithm of the total。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个概念下，我们引入逆文档频率的概念。逆文档频率是衡量一个单词在整个语料库中出现的普遍性或稀有性的指标，数学上通常是这样计算的：总数的对数。
- en: '![](img/2782ceb1249cc2bcb3d95101cf1913de_47.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2782ceb1249cc2bcb3d95101cf1913de_47.png)'
- en: number of documents divided by the，number of documents containing the word。so
    if a word like Holmes shows up in all，of the documents，well then total documents
    is a you know。however many documents there are a，number of documents containing
    Holmes is。going to be the same number so when you，divide these two together you'll
    get one。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 文档的数量除以包含该词的文档数量。如果像“福尔摩斯”这样的词出现在所有文档中，那么总文档数量就是文档总数，而包含“福尔摩斯”的文档数量也是一样。因此，当你将这两个数相除时，你会得到1。
- en: and the logarithm of 1 is just 0 and so，what we get is if Holmes shows up in
    all。of the documents it has an inverse，document frequency of 0 and you can。think
    now of inverse document frequency，as a measure，of you know how rare is the word
    that。shows up in this particular document，then if a word doesn't show up across。
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 1的对数就是0，因此，如果福尔摩斯出现在所有文档中，它的逆文档频率就是0。你可以把逆文档频率看作是一个度量，说明在这篇特定文档中，单词出现的稀有程度。如果一个单词没有出现在任何文档中。
- en: many documents at all this number is，going to be much higher and this then。gets
    us at our model known as tf-idf，which is a method for ranking what words。are important
    in a document by，multiplying these two ideas together。multiplied term frequency
    or TF by。
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果很多文档都出现，这个数字会高得多，这将导致我们得到一个称为TF-IDF的模型，这是一种通过将这两个概念结合在一起来对文档中的重要词进行排名的方法。将词频（TF）相乘。
- en: '![](img/2782ceb1249cc2bcb3d95101cf1913de_49.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2782ceb1249cc2bcb3d95101cf1913de_49.png)'
- en: inverse document frequency or I D F，where the idea here now is that how。important
    a word is depends on two，things it depends on how often it shows。up in the document
    using the heuristic，that if a word shows up more often it's。probably more important
    and we multiply，that by inverse document frequency IDF。
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 逆文档频率（IDF），这里的想法是，一个词的重要性取决于两个因素，它取决于在文档中出现的频率，使用启发式方法：如果一个词出现得越频繁，它可能就越重要，我们将其乘以逆文档频率IDF。
- en: because if the word is rarer but it，shows up in the document it's probably。more
    important than if the word shows up，across most or all of the documents。because
    then it's probably a less，important factor in what the different。topics across
    the different documents in，the corpus happened to be and so now。
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 因为如果一个词比较稀有，但出现在文档中，它可能比在大多数或所有文档中出现的词更重要，因为它可能在不同文档中话题的不同重要性较低。
- en: let's go ahead and apply this algorithm，on the Sherlock Holmes corpus and here。is
    tf-idf now what I'm doing is for each，of the documents for each word I'm。calculating
    its TF score term frequency，multiplied by the inverse document。frequency of that
    word not just looking，at the single value but multiplying。
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在福尔摩斯语料库上应用这个算法，这就是tf-idf，现在我正在计算每个文档中每个单词的TF分数（词频），乘以该词的逆文档频率，而不仅仅是看单个值，而是进行乘法运算。
- en: '![](img/2782ceb1249cc2bcb3d95101cf1913de_51.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2782ceb1249cc2bcb3d95101cf1913de_51.png)'
- en: these two values together in order to，compute the overall values and now if
    I。run tf-idf on the holmes corpus this is，going to try and get us a better。approximation
    for what's important in，each of the stories and it seems like。it's trying to extract
    here probably，like the names of characters that happen。
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 将这两个值结合起来，以计算总体值，现在如果我在福尔摩斯语料库上运行tf-idf，这将帮助我们更好地近似每个故事中的重要内容，似乎正在提取角色的名字。
- en: to be important in the story characters，that show up in this story that don't。show
    up in the other story and in，prioritizing the more important。characters that happen
    to show up more，often and so this then might be a better。analysis of what types
    of topics are。
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在故事中重要的角色，会在这个故事中出现，而不会在其他故事中出现，并且优先考虑那些出现频率更高的角色，这可能是一个更好的分析，关于话题的类型。
- en: '![](img/2782ceb1249cc2bcb3d95101cf1913de_53.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2782ceb1249cc2bcb3d95101cf1913de_53.png)'
- en: more or less important I also have，another corpus which has a corpus of all。![](img/2782ceb1249cc2bcb3d95101cf1913de_55.png)
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 不管重要性如何，我还有另一个语料库，其中包含所有的语料。![](img/2782ceb1249cc2bcb3d95101cf1913de_55.png)
- en: of the Federalist Papers from American，history if I go ahead and run tf-idf
    on。the Federalist Papers we can begin to，see what the important words in each
    of。the various different Federalist Papers，happen to be that in Federalist paper。number
    61 seems like it's a lot about，about the set。
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我对美国历史上的《联邦党人文集》运行tf-idf，我们可以开始看到各个《联邦党人文集》中重要的单词是什么，而在第61篇中，似乎是关于这一组内容。
- en: '![](img/2782ceb1249cc2bcb3d95101cf1913de_57.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2782ceb1249cc2bcb3d95101cf1913de_57.png)'
- en: and impeachments you can start to，extract what the important terms and。what
    the important words are just by，looking at what things show up across。and don't
    show up across many of the，documents but show up frequently enough。in certain
    of the documents and so this，can be a helpful tool for trying to。
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于弹劾，你可以通过观察在多个文档中出现的关键词，提取出重要的术语和单词，它们在某些文档中出现得足够频繁，而在其他文档中则不出现，这可以成为帮助工具。
- en: figure out this kind of topic modeling，figuring out what it is that a。particular
    document happens to be about，and so this then is starting to get us。into this
    world of semantics like what，it is that things actually mean when。we're talking
    about language now we're，not going to think about like the bag of。
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这种主题建模，弄清楚特定文档的主题，这使我们开始进入语义的世界，即在讨论语言时事物的实际含义，我们不会仅仅考虑词袋。
- en: '![](img/2782ceb1249cc2bcb3d95101cf1913de_59.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2782ceb1249cc2bcb3d95101cf1913de_59.png)'
- en: words when we just say treat a sample of，text just dressed a whole bunch of
    words。
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们只是处理一段文本时，我们处理的是一大堆单词。
