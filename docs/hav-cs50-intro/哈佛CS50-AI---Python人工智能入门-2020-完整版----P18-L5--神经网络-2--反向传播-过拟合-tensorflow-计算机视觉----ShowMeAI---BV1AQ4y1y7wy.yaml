- en: 哈佛CS50-AI ｜ Python人工智能入门(2020·完整版) - P18：L5- 神经网络 2 (反向传播，过拟合，tensorflow，计算机视觉)
    - ShowMeAI - BV1AQ4y1y7wy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 哈佛CS50-AI ｜ Python人工智能入门(2020·完整版) - P18：L5- 神经网络 2 (反向传播，过拟合，tensorflow，计算机视觉)
    - ShowMeAI - BV1AQ4y1y7wy
- en: actually be and so the strategy people，came up with was to say that if you know。it
    what the error or the loss ism on the，output node will then based on what。these
    weights are if one of these，weights is higher than another you can。calculate an
    estimate for how much the，error from this node was due to this。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，人们提出的策略是，如果你知道输出节点上的错误或损失是什么，那么基于这些权重，如果其中一个权重比另一个高，你可以计算出这个节点的错误在多大程度上是由于这个。
- en: part of the hidden node or this part of，the hidden layer or this part of the。hidden
    layer based on the values of，these weights in effect saying that。based on the
    error from the output I can，back propagate the error and figure out。an estimate
    for what the error is for，each of these nodes in the hidden layer。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏节点的部分，或者隐藏层的这一部分，或者隐藏层的这一部分，基于这些权重的值，实际上是在说，根据输出的错误，我可以反向传播错误，并弄清楚每个隐藏层节点的错误估计是什么。
- en: as well and there's some more calculus，here that we won't get into the details。of
    them but the idea of this algorithm，is known as back propagation it's an。algorithm
    for training a neural network，with multiple different hidden layers。and the idea
    for this the pseudocode for，it will again be if we want to run。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有一些微积分，我们不会详细讨论。这个算法的思想被称为反向传播，它是一个用于训练神经网络的算法，具有多个不同的隐藏层。对于这个算法的伪代码，如果我们想要运行。
- en: gradient descent with backpropagation，we'll start with a random choice of。weights
    as we did before and now we'll，go ahead and repeat the training process。again
    and again but what we're going to，do each time is now we're going to。calculate
    the error for the output layer，first we know the output and what it。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 使用带有反向传播的梯度下降，我们将从随机选择的权重开始，正如我们之前所做的，现在我们将继续重复训练过程。一次又一次，但每次我们要做的是现在我们将会。首先计算输出层的错误，我们知道输出是什么。
- en: should be and we know what we calculated，so we can figure out what the error。there
    is but then we're going to repeat，for every layer starting with the output。layer
    moving back into the hidden layer，then the hidden layer before that if。there are
    multiple hidden layers going，back all the way to the very first。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 应该是什么，我们知道我们计算了什么，因此我们可以弄清楚有什么错误。然后我们会对每一层进行重复，从输出层开始，向回移动到隐藏层，然后是之前的隐藏层。如果有多个隐藏层，将一直向回到最初的。
- en: hidden layer assuming they're multiple，we're going to propagate the error back。one
    layer whatever the error was from，the output figure out what the error。should
    be a layer before that based on，what the values of those weights are and。then
    we can update those weights so，graphically the way you might think。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层假设它们是多个，我们将会把错误向回传播。每一层，无论输出的错误是什么，弄清楚在那之前一层的错误应该是什么，基于这些权重的值。然后我们可以更新这些权重，所以，从图形上来看，你可能会想。
- en: about this is that we first start with，the output we know what the output。should
    be we know what output we，calculated and based on that we can。figure out alright
    how do we need to，update those weights back propagating。the error to these nodes
    and using that，we can figure out how we should update。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这一点，我们首先从输出开始，我们知道输出应该是什么，我们知道计算出的输出是什么，基于此我们可以。弄清楚我们需要如何更新这些权重，将错误反向传播到这些节点，并利用它，我们可以弄清楚我们应该如何更新。
- en: these weights and you might imagine if，there are multiple layers we could。repeat
    this process again and again to，begin to figure out how all of these。weights should
    be updated in this back，propagation algorithm is really the key。algorithm that
    makes a neural networks，possible it makes it possible to take。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这些权重，你可能想象一下，如果有多个层，我们可以。重复这个过程一次又一次，以开始弄清楚所有这些。权重在这个反向传播算法中应该如何更新，这确实是关键。算法使得神经网络成为可能，它使得我们能够进行。
- en: these multi-level structures and be able，to train those structures depending
    on。what the values of these weights are in，order to figure out how it is that
    we。should go about updating those weights，in order to create some function that
    is。able to minimize the total amount of，loss to figure out some good setting of。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些多层结构，能够训练这些结构，具体取决于这些权重的值，以弄清楚我们应该如何更新这些权重，从而创建一个能够最小化总损失的函数，找出一些好的设置。
- en: the weights that will take the inputs，and translate it into the output that
    we。expect and this works as we said not，just for a single hidden layer you can。imagine
    multiple hidden layers where，each hidden layer we just define however。many nodes
    we want where each of the，nodes in one layer we can connect to the。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 权重将输入转换为我们期望的输出，正如我们所说，这不仅适用于单个隐藏层，您可以想象多个隐藏层，在每个隐藏层中定义所需的节点数量，每个节点都可以连接到下一个层的节点。
- en: nodes in the next layer defining more，and more complex networks that are able。to
    model more and more complex types of，functions and so this type of network is。what
    we might call a deep neural network，part of a larger family of deep learning。algorithms
    if you've ever heard that，term and all deep learning is about is。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 定义越来越复杂的网络，能够建模越来越复杂类型的函数，因此这种类型的网络可以称为深度神经网络，属于深度学习算法的大家族，如果您听说过这个术语，所有深度学习关注的就是。
- en: it's using multiple layers to，be able to predict and be able to model。higher-level
    features inside of the，input to be able to figure out what the。output should be
    and so with deep neural，network is just a neural network that。does multiple of
    these hidden layers，where we start at the input calculate。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 它利用多个层来预测并建模输入中的高级特征，以确定输出应该是什么，因此深度神经网络就是一个具有多个隐藏层的神经网络，从输入开始计算。
- en: values for this layer then this layer，then this layer and then ultimately get。an
    output and this allows us to be able，to model more and more sophisticated。types
    of functions that each of these，layers can calculate something a little。bit different
    and we can combine that，information to figure out what the。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一层的值，然后是这一层，然后是这一层，最终得到输出，这使我们能够建模越来越复杂的函数，每一层可以计算一些略有不同的内容，我们可以结合这些信息来确定输出应该是什么。
- en: output should be of course as with any，situation of machine learning as we。begin
    to make our models more and more，complex to model more and more complex。functions
    and the risk we run is，something like overfitting and we talked。about overfitting
    last time in the，context of overfitting based on when we。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在任何机器学习的情况下，随着我们开始使模型变得越来越复杂，以建模越来越复杂的函数，我们面临的风险是过拟合，上次我们在过拟合的上下文中讨论了这一点。
- en: were training our models to be able to，learn some sort of decision boundary。where
    overfitting happens when we fit，too closely to the training data and as。a result
    we don't generalize well to，other situations as well and one of the。risks we run
    with a far more complex，neural network that has many many。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练模型时，试图学习某种决策边界，过拟合发生在我们对训练数据拟合得过于紧密，因此我们对其他情况的泛化效果较差，而我们在一个复杂的神经网络中面临的风险是。
- en: different nodes is that we might over，fit based on the input data we might。grow
    over reliant on certain nodes to，calculate things just purely based on。the input
    data that doesn't allow us to，generalize very well to the output and。there are
    a number of strategies for，dealing with overfitting but one of the。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的节点可能会因为输入数据而导致过拟合，我们可能过于依赖某些节点，仅仅基于输入数据进行计算，这不允许我们很好地泛化到输出，并且有很多策略可以应对过拟合。
- en: most popular in the context of neural，networks is a technique known as dropout。and
    what dropout does is it when we're，training the neural network what we'll。do and
    dropout is temporarily remove，units temporarily remove these。artificial neurons
    from our network but，chosen at random and the goal here is to。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络的背景下，最流行的技术被称为 dropout，dropout 的作用是在训练神经网络时，暂时移除某些单元，随机选择这些人工神经元，从而达到目的。
- en: prevent over reliance on certain units，that what generally happens in。overfitting
    is that we begin to over，rely on certain units inside the neural。network to be
    able to tell us how to，interpret the input data。what dropout we'll do is randomly
    remove，some of these units in order to reduce。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 防止对某些单元的过度依赖，过拟合通常发生在我们开始过于依赖神经网络内部的某些单元，以告诉我们如何解读输入数据。dropout 的作用是随机移除一些单元，以减少。
- en: the chance that we over rely on certain，units to make our neural network more。robust
    to be able to handle the，situations even when we just drop out。particular neurons
    entirely so the way，that might work is we have a network。like this and as we're
    training it when，we go about trying to update the weights。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能过度依赖某些单元，以使我们的神经网络更加健壮，以便能够处理情况，即使我们完全丢弃特定神经元。所以，这样的工作方式是我们有一个这样的网络，当我们训练它时，当我们更新权重时。
- en: the first time we'll just randomly pick，some percentage of the nodes to drop
    out。of the network it's as if those nodes，weights are so，ciated with those nodes
    aren't there at。all and will train in this way then the，next time we update the
    weights we'll。pick a different set and just go ahead，and train that way and then
    again。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次我们将随机选择一些百分比的节点从网络中丢弃，就好像那些节点的权重根本不存在一样，然后以这种方式进行训练，接下来当我们更新权重时，我们将选择另一组节点并继续训练。
- en: randomly choose and train with other，nodes that have been dropped out as well。and
    the goal of that is that after the，training process if you train by。dropping out
    random nodes inside of the，scenario Network you hopefully end up。with the network
    it's a little bit more，robust it doesn't rely too heavily on。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 随机选择并训练与其他被丢弃的节点一起，目标是，在训练过程中，如果通过随机丢弃网络内部的节点进行训练，希望最终得到一个更健壮的网络，不会过于依赖。
- en: any one particular node but more，generally learns how to approximate a。function
    in general so that then is a，look at some of these techniques that we。can use
    in order to implement a neural，network to get at the idea of taking。this input
    passing it through these，various different layers in order to。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 任何特定节点，但更普遍地说是学习如何近似一个函数，所以这就是我们可以使用的一些技术，以实现一个神经网络，从而理解将这个输入通过这些不同的层进行处理的思路。
- en: produce some sort of output and what，we'd like to do now is take those ideas。and
    put them into code and to do that，there are a number of different machine。learning
    libraries neural network，libraries that we can use that allow us，to get access
    to someone's。implementation of backpropagation and，all of these hidden layers
    and one of。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 生成某种输出，现在我们想做的是将这些想法付诸于代码，为此，有许多不同的机器学习库和神经网络库可以使用，这些库允许我们访问某些人的反向传播实现和所有这些隐藏层。
- en: the most popular developed by google is，known as tensorflow a library that we。can
    use for quickly creating neural，networks and modeling them and running。them on
    some sample data to see what the。![](img/0b60cfa2ad22087bb0426f088d1cbfc6_1.png)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的由谷歌开发的库被称为**tensorflow**，这是一个我们可以用来快速创建神经网络并对其进行建模和在一些样本数据上运行的库，以查看结果。
- en: output is going to be and before we，actually start writing code we'll go。ahead
    and take a look at tensor flows，playground which will be an opportunity。for us
    just to play around with this，idea of neural networks and different。layers just
    to get a sense for what it，is that we can do by taking advantage of。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将是什么，在我们实际上开始编写代码之前，我们将先查看张量流的**游乐场**，这将为我们提供一个机会，让我们玩一玩神经网络和不同层次的概念，以便更好地理解我们可以利用什么。
- en: neural networks so let's go ahead and go，into tensor flows playground which
    you。can go to by visiting that URL from，before and what we're going to do now
    is。we're going to try and learn the。![](img/0b60cfa2ad22087bb0426f088d1cbfc6_3.png)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络，所以我们继续进入张量流的**游乐场**，你可以通过访问之前的URL来进入，我们现在要做的是尝试学习。
- en: decision boundary for this particular，output I want to learn to separate the。![](img/0b60cfa2ad22087bb0426f088d1cbfc6_5.png)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个特定输出的**决策边界**，我想学习如何分离这些。
- en: orange points from the blue points and，I'd like to learn some sort of setting。of
    weights inside of a neural network，that will be able to separate those from。each
    other the features we have access，to our input data are the x value and。the y
    value so the two values along each，of the two axes and and what I'll do now。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从蓝点到橙点，我想要学习一种神经网络内部的权重设置，这将能够将它们彼此分离。我们可以访问的输入数据特征是x值和y值，即两个轴上的两个值，现在我将进行的操作是。
- en: is I can set particular parameters like，what activation function I would like
    to。use them and I'll just go ahead and，press play and see what happens and what。happens
    here is that you'll see that，just by using these two input features。the x value
    and the y value with no，hidden layers just take the input x and。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以设置特定的参数，比如我想使用什么激活函数，然后我会按下播放，看看会发生什么。发生的事情是，你会看到仅通过使用这两个输入特征，x值和y值，没有隐藏层，只是根据这些输入预测x和y坐标的输出。
- en: y values them and figure out what the，decision boundary is，our neural network
    learns pretty quickly。that in order to divide these two points，we should just
    use this line this line。acts as a decision boundary that，separates this group
    of points from that。group of points and it does it very well，you can see up here
    what the loss is the。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的神经网络很快学会了，为了将这两个点分开，我们应该使用这条线。这条线作为决策边界，将这一组点与另一组点分开，并且做得很好。你可以在这里看到损失是多少。
- en: training loss is zero meaning we were，able to perfectly model separating these。two
    points from each other inside of our，training data so this was a fairly。simple
    case of trying to apply a neural，network because the data is very clean。it's very
    nicely linearly separable we，could just draw a line that separates。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 训练损失为零，意味着我们能够完美地建模，将这两个点从我们的训练数据中分离开来。所以这是一个相对简单的案例，尝试应用神经网络，因为数据非常干净，线性可分性很好，我们可以画一条线来分开。
- en: all of those points from each other，let's now consider a more complex case。so
    I'll go ahead and pause the，simulation and we'll go ahead and look。at this data
    set here this data set is a，little bit more complex now in this data。set we still
    have blue and orange points，that we'd like to separate from each。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑一个更复杂的案例，先将模拟暂停，我们来看一下这个数据集，这个数据集稍微复杂一点。在这个数据集中，我们仍然有蓝色和橙色的点，我们希望将它们分开。
- en: other but there's no single line that we，can draw that is going to be able to。figure
    out how to separate the blue from，the orange because the blue is located。in these
    two quadrants and the orange is，located here and here it's a more。complex function
    to be able to learn so，let's see what happens if we just try。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 但是没有一条线能够将蓝色与橙色分开，因为蓝色位于这两个象限，而橙色位于这里和这里。这是一个更复杂的函数需要学习，所以让我们看看如果我们尝试一下这些点会发生什么。
- en: and predict based on those inputs the x，and y coordinates what the output should。be
    I'll press play and what you'll，notice is that we're not really able to。draw much
    of a conclusion that we're not，able to very clean they see how we。should divide
    the orange points from the，blue points and you don't see a very。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我会按下播放，你会注意到我们似乎无法得出很清晰的结论，无法非常清楚地看到我们应该如何将橙色点与蓝色点分开，你没有看到一个非常。
- en: clean separation there so it seems like，we don't have enough of sophistication。inside
    of our network to be able to，model something that is that complex we。need a better
    model for the centeral，network and I'll do that by adding a。hidden layer so now
    I have a hidden，layer that has two neurons inside of it。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这样看起来，我们的网络没有足够的复杂性来建模如此复杂的情况。我们需要一个更好的中心网络模型，我会通过添加一个隐藏层来实现这一点，现在我有一个包含两个神经元的隐藏层。
- en: so I have two inputs that then go to two，neurons inside of a hidden layer that。then
    go to our output and now I'll press，play and what you'll notice here is that。we're
    able to do slightly better we're，able to now say all right these points。are definitely
    blue these points are，definitely orange we're still struggling。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我有两个输入，然后传递给隐藏层中的两个神经元，再到我们的输出。现在我会按下播放，你会注意到我们能够做得稍微更好，我们能够说这些点绝对是蓝色的，这些点绝对是橙色的，但我们仍然在努力。
- en: a little bit with these points up here，though and what we can do is we can see。for
    each of these hidden neurons what is，it exactly that these hidden neurons are。doing
    each hidden neuron is learning its，own decision boundary and we can see。what that
    boundary is this first neuron，is learning our，this line that seems to separate
    some of。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到每个隐藏神经元到底在做什么。每个隐藏神经元都在学习自己的决策边界，我们可以看到这个边界。第一个神经元正在学习这条线，这似乎将一些y值分开。
- en: the blue points from the rest of the，points this other hidden neuron is。learning
    another line that seems to be，separating the orange points in the。lower right
    from the rest of the points，so that's why we're able to sort of。figure out these
    two areas in the bottom，region but we're still not able to。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个隐藏神经元正在学习另一条线，似乎正在将右下角的橙点与其他点分开，这就是我们能够确定底部区域这两个区域的原因，但我们仍然无法。
- en: perfectly classify all of the points so，let's go ahead and add another neuron。now
    we've got three neurons inside of，our hidden layer and see what we're able。to
    learn now all right well now we seem，to be doing a better job by learning。three
    different decision boundaries，which each of the three neurons inside。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，使用两个神经元和一个隐藏层，我们能够完美地对所有点进行分类，所以让我们继续添加另一个神经元，现在我们的隐藏层中有三个神经元，看看我们现在能够学习到什么，好吧，现在我们似乎通过学习三个不同的决策边界做得更好，每个神经元都有各自的决策边界。
- en: of our hidden layer we're able to much，better figure out how to separate these。blue
    points from the orange points and，you can see what each of these hidden。neurons
    is learning each one is learning，a slightly different decision boundary。and then
    we're combining those decision，boundaries together to figure out what。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的隐藏层中，我们能够更好地找出如何将这些蓝点与橙点分开，你可以看到每个隐藏神经元正在学习什么，每个神经元都在学习一个略微不同的决策边界，然后我们将这些决策边界组合在一起，以确定什么。
- en: the overall output should be I mean we，can try it one more time by adding a。fourth
    neuron there and try learning，that and it seems like now we can do。even better
    at trying to separate the，blue points from the orange points but。we were only
    able to do this by adding a，hidden layer by adding some layer that。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 整体输出应该是，我的意思是我们可以再试一次，添加一个第四个神经元，并尝试学习，现在我们似乎能更好地尝试将蓝点与橙点分开，但我们只有通过添加一个隐藏层来实现这一点。
- en: is learning some other boundaries and，combining those boundaries to determine。the
    output and the strength though the，size and thickness of these lines and。indicate
    how high these weights are how，important each of these inputs ISM for。making this
    sort of calculation and we，can do maybe one more simulation let's。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正在学习一些其他的边界，并结合这些边界来确定输出，线条的大小和厚度表示这些权重的高低，以及每个输入对进行这种计算的重要性，我们也许可以再做一次模拟。
- en: go ahead and try this on a data set that，looks like this go ahead and get rid
    of。the hidden layer here now we're trying，to separate the blue points from the。orange
    points where all the blue points，are located again inside of a circle。effectively
    so we're not going to be，able to learn a line notice I press play。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 继续尝试在一个看起来像这样的数据集上，去掉这里的隐藏层，现在我们尝试将蓝点与橙点分开，所有蓝点都有效地位于一个圆内，所以我们无法学习一条直线，注意我按下播放。
- en: and we're really not able to draw any，sort of classification at all because。there
    is no line that cleanly separates，the blue points from the orange points。so let's
    try to solve this by，introducing a hidden layer I'll go ahead。and press play and
    all right with two，neurons and a hidden layer we're able to。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用三个不同的决策边界，每个神经元都学习到了，而我们真的无法绘制任何分类，因为没有一条线可以干净地将蓝点与橙点分开，所以让我们尝试通过引入一个隐藏层来解决这个问题，我按下播放。
- en: do a little better because we，effectively learn two different decision。boundaries
    we learned this line here and，we learn this line on the right hand。side and right
    now we're just saying all，right well if it's in between we'll call。it blue and
    if it's outside we'll call，it or engine so not great but certainly。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 做得稍微好一些，因为我们有效地学习了两个不同的决策边界，我们在这里学习了这条线，并在右侧学习了这条线，现在我们只是说，好吧，如果它在中间，我们就称之为蓝点，如果在外面，我们就称之为橙点，所以不是很好，但肯定能分开蓝点与其他点。
- en: better than before we're learning one，decision boundary and another。and based
    on those we can figure out，what the output should be but let's now。go ahead and
    add a third neuron and see，what happens now I go ahead and train it。and now using
    three different decision，boundaries that are learned by each of。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 比之前好，我们学习了一个决策边界和另一个，基于这些我们可以弄清楚输出应该是什么，但现在我们继续添加第三个神经元，看看会发生什么，我现在去训练它。
- en: these hidden neurons we're able to much。![](img/0b60cfa2ad22087bb0426f088d1cbfc6_7.png)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够处理这些隐藏的神经元。![](img/0b60cfa2ad22087bb0426f088d1cbfc6_7.png)
- en: more accurately model this distinction，between blue points and orange points。we're
    able to figure out maybe with，these three decision boundaries。combining them together
    you can imagine，figuring out what the output should be。and how to make that sort
    of，classification and so the goal here is。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地对蓝点和橙点之间的区别建模，我们能够通过这三个决策边界的组合来推测，想象一下输出应该是什么，以及如何进行这种分类，因此这里的目标是。
- en: just to get a sense for having more，neurons in these hidden layers and。allows
    us to learn more structure in the，data allows us to figure out what the。relevant
    and important decision，boundaries are and then using this back。propagation algorithm
    we're able to，figure out what the values of these。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 只是为了让我们对这些隐藏层中的更多神经元有个概念，这让我们能学习到数据中的更多结构，找出相关和重要的决策边界，然后通过反向传播算法，我们能够确定这些权重的值。
- en: weights should be in order to Train this，network to be able to classify one。category
    of points away from another，category of points instead and this is。ultimately
    what we're going to be trying，to do whenever we're training a neural。network so
    let's go ahead and actually，see an example of this you'll recall。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以训练这个网络能够将一个类别的点与另一个类别的点区分开来，这正是我们在训练神经网络时最终要尝试做的事情，所以让我们实际上来看一个例子，你会记得。
- en: from last time that we had this，banknotes file that included information。about
    counterfeit banknotes as opposed，to authentic banknotes where I had like。four
    different values for each banknote，and then a categorization of whether。that banknote
    is considered to be，authentic or a counterfeit note and what。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 上次我们讨论的内容涉及到包含关于伪钞与真钞信息的钞票文件。我有四种不同面值的钞票，并且对每种钞票进行了分类，判断其是被认为是**真钞**还是**伪钞**。
- en: I wanted to do was based on that input，information figure out some function。that
    could calculate based on the input，information what category it belonged to。and
    what I've written here in bank notes，top pipe is a neural network that will。learn
    just that a network that learns，based on all of the input whether or not。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我想做的是根据输入信息推导出一个函数，该函数能够基于输入信息计算它属于哪个类别。我在钞票顶部写的神经网络就是为了学习这一点，它会根据所有输入来判断。
- en: we should categorize a banknote as，authentic or as counterfeit the first。step
    is the same as what we saw from，last time I'm really just reading the。data in
    and getting it into an，appropriate format and so this is where。more of the like
    writing Python code on，your own comes in in terms of。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该将钞票分类为**真钞**或**伪钞**，第一步与上次看到的相同，我实际上只是将数据读入并将其转换为适当的格式。这是编写Python代码的地方。
- en: manipulating the statum massaging the，data into a format that will be。understood
    by a machine learning library，like scikit-learn or like tensorflow and。so here
    I separate it into a training，and a testing set and now what I'm doing，network。here
    I'm using TF which stands for，tensor flow up above I said import，tensor flow as
    tftf。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 操作statum，调整数据格式以便机器学习库如scikit-learn或tensorflow能理解，因此我将其分为训练集和测试集，现在我正在使用的网络是TF，代表tensor
    flow。上面我提到导入tensor flow为tftf。
- en: an abbreviation that will often use we，don't need to write out tensorflow every。time
    we want to use anything inside of，the library I'm using TF dot Charis。Charis is
    an API a set of functions that，we can use in order to manipulate neural。networks
    inside of tensorflow，and it turns out there are other machine。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们常用的一个缩写，我们不需要每次都写出tensorflow。我在使用TF. Charis。Charis是一个API，一组函数，用于在tensorflow中操作神经网络，结果表明还有其他机器。
- en: learning libraries that also use the，Charis api but here i'm saying alright。go
    ahead and give me a model that is a，sequential model a sequential neural。network
    meaning one layer after another，and now I'm going to add to that model。what layers
    I want inside of my neural，network so here I'm saying model add go。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 学习库也使用Charis API，但我在这里说，好的，给我一个**顺序模型**，即一个层层相叠的**神经网络**，接下来我要在我的神经网络中添加我想要的层。
- en: ahead and add a dense layer and when we，say a dense layer we mean a layer that。is
    just each of the nodes inside of the，layer is going to be connected to each。of
    the nodes from the previous layer so，we have a densely connected layer this。layer
    is going to have eight units，inside of it so it's going to be a。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 继续添加一个密集层，当我们说密集层时，我们指的是该层中的每个节点都将与前一层中的每个节点相连，所以我们有一个密集连接的层，这一层将有八个单位在里面，因此它将是一个。
- en: hidden layer inside of our neural，network with eight different units eight。artificial
    neurons each of which might，learn something different and I just。sort of chose
    eight arbitrarily you，could choose a different number of。hidden layer hidden nodes
    inside of the，layer and if we as we saw before。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的神经网络中，有一个隐藏层，里面有八个不同的单位，八个人工神经元，每个神经元可能学习不同的东西，我只是随意选择了八个，你可以选择不同数量的隐藏层节点，如果我们之前看到的。
- en: depending on the number of units there，are inside of your hidden layer more。unit
    means you can learn more complex，functions so maybe you can more。accurately model
    the training data but，it comes with the cost more units means。more weights that
    you need to figure out，how to update so it might be more。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 根据隐藏层中单位的数量，更多的单位意味着你可以学习更复杂的函数，因此你可能可以更准确地建模训练数据，但这也带来了成本，更多的单位意味着你需要弄清楚如何更新更多的权重，所以可能会更复杂。
- en: expensive to do that calculation and you，also run the risk of overfitting on
    the。data if you have too many units and you，learn to just over fit on the training。data
    that's not good either so there is，a balance and there's often a testing。process
    where you'll train on some data，and maybe validate how well you're doing。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 进行这样的计算是昂贵的，如果你有太多的单位，可能会导致对数据的过拟合，如果你学习只是过度拟合训练数据，那也不好，所以这之间存在一个平衡，通常会有一个测试过程，在某些数据上进行训练，并可能验证你做得有多好。
- en: on a separate set of data often called a，validation set to see all right which。setting
    of parameters how many layers，should I have how many units should be。in each layer
    which one of those，performs the best on the validation set。so you can do some
    testing to figure out，what these hyper parameters so-called it。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在一组单独的数据上，通常称为验证集，以查看哪种参数设置，应该有多少层，每层应该有多少单位，哪一个在验证集上表现最好，因此你可以进行一些测试来弄清楚这些超参数。
- en: should be equal to next I specify what，the input shape is meaning alright what。is
    my input look like my input has four，values and so the input shape is just。four
    because we have four inputs and，then I specify what the activation。function is
    and the activation function，again we can choose there are a number。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 应该等于接下来我指定输入形状，这意味着我的输入是什么样的，我的输入有四个值，因此输入形状就是四，因为我们有四个输入，然后我指定激活函数是什么，激活函数，我们可以选择，有很多种。
- en: of different activation functions and，here I'm using raloo which you might，recall
    from earlier and。I'll add an output layer so I have my，hidden layer now I'm adding
    one more。layer that will just have one unit，because all I want to do is predict。something
    like counterfeit bill or，authentic build so I just need a single。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的激活函数，在这里我使用的是relu，可能你之前听说过。然后我将添加一个输出层，因此我现在有了我的隐藏层，再添加一层，这一层将只有一个单位，因为我想预测的只是像假钞或真实钞票这样的东西，所以我只需要一个。
- en: unit and the activation function I'm，going to use here is that sigmoid。activation
    function which again was that，s-shaped curve that just gave us like a。probability
    of what is the probability，that this is a counterfeit bill as。opposed to an authentic
    bill so that，then is the structure of my neural。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 单位和激活函数，我将使用的激活函数是sigmoid激活函数，它是那个S形曲线，它给了我们一个概率，表示这是一张假钞的概率，与真实钞票相比，因此这就是我的神经网络的结构。
- en: network a sequential neural network that，has one hidden layer with eight units。inside
    of it and then one output layer，that just has a single unit inside of it。and I
    can choose how many units there，are I can choose the activation function。then
    I'm going to compile this model，tensorflow gives you a choice of how you。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个顺序神经网络，包含一个有八个单位的隐藏层和一个仅有一个单位的输出层，我可以选择有多少个单位，可以选择激活函数，然后我要编译这个模型，tensorflow让你选择如何。
- en: would like to optimize the weights there，various different algorithms for doing。that
    what type of loss function you want，to use again many different options for。doing
    that and then how I want to，evaluate in my model well I care about。accuracy I
    care about like how many of，my points am I able to classify。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望优化权重，有多种不同的算法可以做到这一点，想要使用什么样的损失函数也有很多选项，接下来我希望如何评估我的模型，我关心准确性，我关心我能分类多少个点。
- en: correctly versus not correctly as，counterfeit or not counterfeit and I。would
    like it to report to me how，accurate my model was performing then。now that I've
    defined that model I call，model dot fit to say go ahead and train。the model train
    it on all the training，data plus all of the training labels so。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正确与不正确的分类，即伪钞与真钞。我希望它能告诉我我的模型表现得有多准确。现在我已经定义了模型，我调用model.dot fit来训练模型，使用所有训练数据以及所有训练标签。
- en: labels for each of those pieces of，training data and I'm saying run it for。20
    epochs meaning go ahead and go，through each of these training points 20。times
    effectively go through the data 20，times and and keep trying to update the。weights
    if I did it for more I could，train for even longer and maybe get a。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我为每一部分训练数据设置标签，并且我说运行20个周期，意味着对每一个训练点进行20次有效的遍历，并且不断尝试更新权重。如果我多做一些，我可以训练更长时间，或许会得到一个更好的结果。
- en: more accurate result but then after I，set it on all the data I'll go ahead and。just
    test it I'll evaluate my model，using model evaluate built in the tensor。flow that
    is just going to tell me how，well do I perform on the testing data so。ultimately
    this is just going to give me，some numbers that tell me how well we。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确的结果，但在我将其应用于所有数据后，我将继续进行测试。我会使用内置于TensorFlow的模型评估来评估我的模型，它会告诉我在测试数据上的表现如何。所以，**最终**这只是会给我一些数字，告诉我我们的表现如何。
- en: '![](img/0b60cfa2ad22087bb0426f088d1cbfc6_9.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b60cfa2ad22087bb0426f088d1cbfc6_9.png)'
- en: did in this particular case so now what，I'm going to do is go into bank notes。and
    go ahead and run bank notes top PI，and what's going to happen now is it's。going
    to read in all of that training，data it's going to generate a neural。network with
    a all my inputs my 8 hidden，layers our 8 hidden units inside my。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下我将进入银行票据，继续运行银行票据的顶级PI。接下来会发生的是，它将读取所有训练数据，并生成一个神经网络，包含我所有的输入和8个隐藏层的单位。
- en: layer and then an output unit and now，what it's doing is its training it's，training
    20 times。and each time you can see how my，data，it starts off the very first time
    not。very accurate they're better than random，something like 79 percent of the
    time。it's able to accurately classify one，bill from another but as I keep training。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 有一层，然后是一个输出单元，现在它正在进行训练，每次训练20次。你可以看到我的数据，第一次的时候并不是很准确，胜过随机的情况，大约有79%的时间能够准确区分一张钞票与另一张，但随着训练的进行。
- en: notice his accuracy value improves and，improves and improves until after I've，times。it
    looks like my accuracy is above 99%，on the on the training data and here's。where
    I tested it on a whole bunch of，testing data and it looks like in this。case I
    was also like ninety-nine point，eight percent accurate so just using。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到他的准确率不断提高，直到在经过多次训练后，我的准确率在训练数据上超过了99%。在这里，我在一堆测试数据上测试，结果显示我在这种情况下的准确率也是99.8%，所以仅凭这些。
- en: that I was able to generate a neural，network that can detect counterfeit。bills
    from authentic bills based on this。![](img/0b60cfa2ad22087bb0426f088d1cbfc6_11.png)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我能够生成一个神经网络，可以根据这个检测伪钞与真实钞票的能力。！[](img/0b60cfa2ad22087bb0426f088d1cbfc6_11.png)
- en: input data ninety-nine point eight，percent of the time at least based on。this
    particular testing data and I might，want to test it with more data as well。just
    to be confident about that but this，is really the value of using a machine。learning
    library like tensorflow and，there are others available for python。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据在至少99.8%的时间内能够准确分类，基于这组特定的测试数据，我可能还想用更多的数据进行测试，以增强信心，但这确实是使用像TensorFlow这样的机器学习库的价值，**还有其他可用于Python的库**。
- en: and other languages as well but all I，have to do is define the structure of。the
    network and define the data that I'm，going to pass into the network and then。tensorflow
    runs the back propagation，algorithm for learning what all of those。weights should
    be for figuring out how，to train this neural network to be able。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他语言，但我所需要做的就是定义网络的结构，并定义要输入到网络中的数据，然后 TensorFlow 会运行反向传播算法，以学习这些权重应该是什么，从而训练这个神经网络。
- en: to accurately as accurately as possible，figure out what the output values should。be
    there as well and so this then was a，look at what it is that neural networks。can
    do just using these sequences of，layer after layer after layer and you。can begin
    to imagine applying these to，much more general problems and one big。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 尽可能准确地确定输出值应该是什么，因此，这就是我们查看神经网络可以做什么的方式，利用一层一层的序列，你可以开始想象将这些应用于更一般的问题。
- en: problem in computing and artificial，intelligence more generally is the。problem
    of computer vision computer，vision is all about computational。methods for analyzing
    and understanding，images you might have pictures that you。want the computer to
    figure out how to，deal with how to process those images。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 计算和人工智能中的一个问题，尤其是计算机视觉的问题。计算机视觉涉及对图像进行分析和理解的计算方法。你可能有一些图片，希望计算机能够理解如何处理这些图像。
- en: and figure out how to produce some sort，of useful result out of this you've
    seen。this in the context of social media，websites that are able to look at a。photo
    that contains a whole bunch of，faces and it's able to figure out what's。a picture
    of whom and label those and，tag them with appropriate people this is。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 并找出如何从中产生某种有用的结果。你已经在社交媒体的上下文中见过这一点，社交媒体网站能够查看包含大量人脸的照片，并能够识别出每张照片中的人，并为他们贴上标签。
- en: becoming increasingly relevant as we，begin to discuss self-driving cars the。car
    these cars now have cameras and we，would like for the computer to have some。sort
    of algorithm that looks at the，image and figures out like what color is，the light
    what。cars are around us and in what direction，for example and so computer vision
    is。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们开始讨论自动驾驶汽车，这变得越来越相关。这些汽车现在配备了摄像头，我们希望计算机能够有某种算法，查看图像并判断例如交通灯的颜色、周围的汽车以及它们的方向。计算机视觉在这里就是关键。
- en: all about like taking an image and，figuring out what sort of computation。what
    sort of calculation we can do with，that image it's also relevant in the。context
    of something like handwriting，recognition this is what you're looking。at is an
    example of the EM NIST data set，it's a big data set just of handwritten。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都是关于获取图像并确定我们可以对图像进行何种计算，这在手写识别的上下文中也是相关的。这是你所看到的 EM NIST 数据集的一个例子，它是一个仅包含手写数字的大型数据集。
- en: digits that we could use to ideally try，and figure out how to predict given。someone's
    handwriting given a photo of a，digit that they have drawn can you。predict whether
    it's a 0 1 2 3 4 5 6 7 8，or 9 for example so this sort of。handwriting recognition
    is yet another，task that we might want to use computer。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用数字，理想情况下试图预测给定某人的手写或他们绘制的数字照片时，能否预测它是 0、1、2、3、4、5、6、7、8 或 9。例如，这种手写识别是我们可能想要使用计算机视觉的另一项任务。
- en: vision tasks and tools and to be able to，apply it towards this might be a task。that
    we might care about so how then can，we use neural networks to be able to。solve
    a problem like this well neural，networks rely upon some sort of input。where that
    input is just numerical data，we have a whole bunch of units where。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉任务和工具可能是我们关心的任务。那么，我们如何使用神经网络来解决这样的问题呢？神经网络依赖某种输入，该输入只是数值数据，我们有一堆单元。
- en: each one of them just represents some，sort of number and so in the context of。something
    like handwriting recognition，or in the context of just an image you。might imagine
    that an image is really，just a grid of pixels grid of dots where。each dot has
    some sort of color and in，the context is something like。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 它们每一个都表示某种数字，因此在手写识别或图像的上下文中，你可以想象图像实际上只是一个像素网格，每个点都有某种颜色。
- en: handwriting recognition you might，imagine that if you just fill in each of。these
    dots in a particular way you can，generate like a 2 or an 8 for example。based on
    which dots happen to be shaded，in and which dots are not and we can。represent
    each of these pixel values，just using numbers so for in particular。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在手写识别中，你可以想象，如果你以某种特定的方式填充这些点，你可以生成像2或8这样的数字，具体取决于哪些点被填充，哪些点没有。我们可以用数字来表示每个像素值。
- en: pixel for example 0 might represent，entirely black depending on how you're。representing
    color it's often common to，represent color values on a 0 to 255。range so that
    you can represent a color，using 8 bits for a particular value like。how much white
    is in the image so 0，might represent all black 255 might。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，像素0可能表示完全黑色，具体取决于你如何表示颜色。通常，颜色值被表示在0到255的范围内，这样你就可以使用8位表示特定值，比如图像中白色的多少。因此，0可能表示全黑，255可能表示全白。
- en: represent entirely white as a pixel and，somewhere in between might represent。some
    shade of gray for example but you，might imagine not just having a single。slider
    that determines how much white is，in the image but if you had a color。image you
    might imagine three different，numerical values a red green and blue。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这之间的某个值可能表示某种灰色的阴影。你可以想象，不仅仅有一个滑块来决定图像中白色的多少，如果你有一张彩色图像，你可以想象有三个不同的数值：红色、绿色和蓝色。
- en: value where the red value controls how，much red is in the image we have one。value
    for controlling how much green is，in the pixel and one value for how much。blue
    is in the pixel as well and，depending on how it is that you set。these values of
    red green and blue，you can get a different color and so any。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 红色值控制图像中红色的多少，我们有一个值控制像素中绿色的多少，还有一个值控制像素中蓝色的多少。根据你设置红、绿、蓝这些值的方式，你可以获得不同的颜色。
- en: pixel can really be represented in this，case by three numerical values a red。value
    green value and a blue value and，if you take a whole bunch of these。pixels assemble
    them together inside of，a grid of pixels then you really just。have a whole bunch
    of numerical values，that you can use in order to perform。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 像素实际上可以用三个数值来表示：红色值、绿色值和蓝色值。如果你把一堆这样的像素组合在一起，放在一个像素网格中，你就有了一大堆数值，可以用来进行操作。
- en: some sort of prediction task and so what，you might imagine doing is using the。same
    techniques we talked about before，just design a neural network with a lot。of inputs
    that for each of the pixel is，it we might have one or three different。inputs in
    the case of a color image a，different input that is just connected。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某种预测任务，你可能会想象使用之前讨论过的相同技术，设计一个有很多输入的神经网络，对于每个像素来说，我们可能在彩色图像的情况下有一个或三个不同的输入。
- en: to a deep neural network for example and，this deep neural network might take
    all。of the pixels inside of the image of，like what digit a person drew and the。output
    might be like ten neurons that，classify it as a 0 or a 1 or a 2 or a 3。or just
    tells us in some way what that，digit happens to be now there are a。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，把这些输入传递给一个深度神经网络，这个深度神经网络可能会处理图像中的所有像素，比如一个人画的数字，输出可能是十个神经元，它将其分类为0、1、2或3，或者以某种方式告诉我们这个数字是什么。
- en: couple of drawbacks to this approach the，first drawback to the approach is just。the
    size of this input array that we，have a whole bunch of inputs if we have。a big
    image there's a lot of different，channels we're looking at a lot of。inputs and
    therefore a lot of weights，that we have to calculate and a second。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有几个缺点，第一个缺点是输入数组的大小。如果我们有一个大图像，就会有很多不同的通道，我们要查看很多输入，因此需要计算很多权重。
- en: problem is the fact that by flattening，everything into just this structure of。all
    the pixel ISM we've lost access to a，lot of the information about the。structure
    of the image that's relevant，but really when a person looks at an。image they're
    looking at you know，particular features of an image they're。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，通过将所有内容压缩成这种像素结构，我们失去了许多关于图像结构的信息，这些信息是相关的。但实际上，当一个人查看图像时，他们是在观察图像的特定特征。
- en: looking at curves are looking at shapes，they're looking at what things can you。identify
    in different regions of the，image and maybe put those things。together in order
    to get a better，picture of what the overall image is。about and by just turning
    it into pixel，values for each of the pixels sure you。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 观察曲线或形状，看看你能在图像的不同区域识别出什么，可能将这些东西组合在一起，以便更好地了解整体图像的含义。通过将其转换为每个像素的像素值，你可以。
- en: might be able to learn that structure，but it might be challenging in order to。do
    so it might be helpful to take，advantage of the fact that you can use。properties
    of the image itself the fact，that it's structured in a particular way。to be able
    to improve the way that we，learn based on that image - so in order。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 可能能够学习到这种结构，但这可能会很具挑战性。为了做到这一点，利用图像本身的特性可能会很有帮助，事实上，它以特定方式结构化，以便改善我们基于该图像的学习方式——所以为了。
- en: to figure out how we can train our，neural networks to better be able to。deal
    with images we'll introduce a，couple of ideas a couple of algorithms。that we can
    apply that allow us to take，the image and extract some useful。information out
    of that image and the，first idea we'll introduce is the notion，of image。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弄清楚如何训练我们的神经网络更好地处理图像，我们将引入几个想法和算法，这些算法可以应用于让我们从图像中提取一些有用的信息。我们将介绍的第一个概念是图像的。
- en: evolution and what image convolution is，all about is it's about filtering an。image
    sort of extracting useful or，relevant features out of the image and。the way we
    do that is by applying a，particular filter that basically adds。the value for every
    pixel with the，values for all of the neighboring pixels。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图像卷积的演变是关于过滤图像，提取图像中有用或相关特征的。我们这样做的方法是通过应用一个特定的滤镜，基本上是将每个像素的值与所有邻近像素的值相加。
- en: to it according to some sort of kernel，matrix which we'll see in a moment is。going
    to allow us to weight these pixels，and in various different ways and the。goal
    of image convolution then is to，extract some sort of interesting or。useful features
    out of an image to be，able to take a pixel and based on its。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 根据某种卷积核矩阵对其进行处理，稍后我们将看到，这将允许我们以各种不同的方式对这些像素进行加权。因此，图像卷积的目标是从图像中提取一些有趣或有用的特征，以便能够根据其。
- en: neighboring pixels maybe predict some，sort of valuable information something。like
    taking a pixel and looking at its，neighboring pixels you might be able to。predict
    whether or not there's some sort，of curve inside the image or whether。it's forming
    the outline of a particular，line or a shape for example and that。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 邻近像素可能预测某种有价值的信息，比如取一个像素并查看其邻近像素，你可能能够预测图像内部是否存在某种曲线，或者它是否在形成特定线条或形状的轮廓，例如。
- en: might be useful if you're trying to use，all of these various different features。to
    combine them to say something，meaningful about an image as a whole so。how then
    does an image convolution work，well we start with a kernel matrix and。the kernel
    matrix looks something like，this and the idea of this is that given。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你试图使用所有这些不同的特性，结合它们来对整个图像做出有意义的陈述，这可能会很有用。那么，图像卷积是如何工作的呢？我们从一个卷积核矩阵开始，卷积核矩阵看起来像这样，基本理念是给定。
- en: a pixel there will be the middle pixel，we're going to multiply each of the。neighboring
    pixels by these values in，order to get some sort of result by。summing up all the
    numbers together so，if I take this kernel what you can think。of it's like a filter
    that I'm going to，apply to the image and let's say that I。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 中间像素将是一个像素，我们将通过这些值对每个邻近像素进行乘法运算，以便通过将所有数字加在一起得到某种结果。因此，如果我取这个卷积核，你可以将其视为我将应用于图像的滤镜。假设我。
- en: take this image this is a four by four，image well think of it as just a。black-and-white
    image where each one is，just a single pixel value so somewhere。between 0 and 255
    for example so we have，a whole bunch of individual pixel values。like this and
    what I'd like to do is，apply this kernel this filter so to。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以这个图像为例，这是一个四乘四的图像，可以将其视为仅包含单个像素值的黑白图像，例如在0到255之间。所以我们有一大堆单独的像素值，我想做的是应用这个卷积核这个滤镜。
- en: speak to this image and the way I'll do，that is all right the kernel is 3 by
    3。you can imagine a 5 by 5 kernel or a，larger kernel - and I'll take it and。just
    first apply it to the first 3 by 3，section of the image and what I'll do is。I'll
    take each of these pixel values，multiply it by its corresponding value。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 讲述这个图像，我将这样做，卷积核是 3x3。你可以想象一个 5x5 的卷积核或更大的卷积核，我会先将其应用于图像的第一个 3x3 部分，我将做的是将这些像素值与其对应的值相乘。
- en: in the filter matrix and add all of the，results together so here for example。I'll
    say 10 times 0 plus 20 times，negative 1 plus 30 times 0 so on and so。forth doing
    all of this calculation and，at the end if I take all these values，value in。colonel
    add the results together for，this particular set of nine pixels I get。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在过滤矩阵中，将所有结果加在一起。因此，在这里举个例子，我会说 10 乘以 0 加 20 乘以负 1 加 30 乘以 0，以此类推。完成所有这些计算后，最后如果我将这些值相加，对于这组特定的九个像素，我得到。
- en: the value of ten for example and then，what I'll do is I'll slide this three
    by。three grid effectively over slide the，colonel by one to look at the next three。by
    three section here I'm just sliding，it over by one pixel but you might。imagine
    a different stride length or，maybe I jumped by multiple pixels at a。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，值为 10，然后我会将这个三乘三的网格有效地滑动，滑动卷积核一单位以查看下一个三乘三的部分。在这里，我只是在滑动一个像素，但你可以想象不同的步长，或者可能一次跳过多个像素。
- en: time if you really wanted to you have，different options here but here I'm just。sliding
    over looking at the next 3x3，section and I'll do the same math twenty。times zero
    plus thirty times a negative，one plus forty times zero plus twenty。times a negative
    one so on and so forth，plus thirty times five and and what I。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你真的想要，你可以在这里有不同的选择，但我只是在滑动，查看下一个 3x3 部分，我会做同样的数学，20 乘以 0 加 30 乘以负 1 加 40 乘以
    0 加 20 乘以负 1，以此类推，加上 30 乘以 5，然后我。
- en: end up getting is the number twenty then，you can imagine shifting over to this。one
    doing the same thing，calculating like the number forty for，example and then doing
    the same thing。well，and so what we have now is what we'll，call like a feature
    map we have taken。this kernel applied it to each of these，various different regions
    and what we。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后得到的是数字二十，然后你可以想象把这个值移到这里。做同样的事情，计算比如数字四十，然后再做一次。现在我们得到的就是我们所称的特征图，我们将这个卷积核应用于这些不同区域，我们。
- en: get is some representation of like a，filtered version of that image and so to。give
    a more concrete example of why it，is that this kind of thing could be。useful let's
    take this kernel matrix for，example which is quite a famous one that。adds an eight
    in the middle and then all，of the neighboring pixels get a negative。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的是像图像的过滤版本的一些表示。因此，为了给出一个更具体的例子，说明为什么这种事情可能有用，我们以这个卷积核矩阵为例，这是一个非常著名的例子，它在中间加了八，然后所有相邻的像素得到负值。
- en: one and let's imagine we wanted to apply，that to a three by three part of an。image
    that looks like this where all of，the values are the same they're all xx。for instance
    well in this case if you do，twenty times eight and then subtract。twenty subtract
    twenty and subtract，twenty for each of the eight neighbors。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一的值。让我们想象一下，我们想把这个应用于图像的一个三乘三部分，图像看起来像这样，所有的值都是相同的，都是 xx。比如在这种情况下，如果你做 20 乘以
    8，然后减去 20，减去 20，再减去每个八个邻居的 20。
- en: well the result of that is you just get，that expression which comes out to be。zero
    you multiplied twenty by eight but，then you subtracted twenty eight times。according
    to that particular kernel the，result of all that is just zero so the。takeaway
    here is that when a lot of the，pixels are the same value we end up。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 那样的结果就是你得到的表达式为零。你乘以 20 和 8，但然后你减去了 20 乘以 8，根据那个特定的卷积核，所有这些结果就是零。因此，关键在于，当很多像素的值相同时，我们最终得到。
- en: getting a value close to zero if though，we add something like this 20s along。this
    first row then 50s in the second，row and 50s in the third row well then。when you
    do this because the same kind，of math twenty times negative one twenty。times negative
    one so on and so forth，then I'm going to higher value a value。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们加上类似这个 20s 的东西，得到的值接近零。第一行是 50s，第二行和第三行也是 50s。当你这样做时，因为同样的数学，二十乘以负一，二十乘以负一，以此类推，那么我会得到一个更高的值。
- en: like 90 in this particular case and so，the more general idea here is that by。applying
    this kernel negative ones eight，in the middle and then negative ones。what I get
    is when this middle value is，very different from the neighboring。values like 50
    is greater than these 20s，then you'll end up with a value higher。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种特定情况下大约是90，因此这里更一般的想法是，通过应用这个内核，负值、八在中间，然后是负值。当这个中间值与邻近值（比如50大于这些20）差异很大时，你最终会得到一个更高的值。
- en: than zero like if this number is higher，than its neighbors you end up getting
    a。bigger output but if this neighbor does，this this value is the same as all of。its
    neighbors then you get a lower，output something like zero and it turns。out that
    this sort of filter can，therefore be used in something like。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个数字大于零，假如这个数字高于其邻居，你最终会得到一个更大的输出，但如果这个邻居的值与所有邻居相同，那么你会得到一个较低的输出，比如零。结果是，这种过滤器可以用于某些东西。
- en: detecting edges in an image or I want to，detect like the boundaries between。various
    different objects inside of an，image I might use a filter like this。which is able
    to tell whether the value，of this pixel is different from the。values of the neighboring
    pixels if it's，like greater than the values of the。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 检测图像中的边缘，或者我想检测图像中不同对象之间的边界，我可能会使用这样的过滤器，它能够判断这个像素的值是否与邻近像素的值不同，如果它大于邻近像素的值。
- en: pixels that happen to surround it and so，we can use this in terms of image。filtering
    and so I'll show you an，example of that I have here in filter。dot PI a file that
    uses pythons image，library or PIL to do some image。filtering I go ahead and open
    an image，and then all I'm going to do is apply a。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 像素周围的情况，所以我们可以在图像过滤方面使用它。我会给你展示一个例子，我这里有一个在filter.dot PI文件中使用Python的图像库或PIL进行图像过滤的代码。我打开一张图像，然后我所要做的就是应用一个。
- en: kernel to that image it's gonna be a，three by three kernel same kind of。kernel
    we saw before and here is the，kernel this is just a list。representation of the
    same matrix that I，showed you a moment ago it's negative。one negative one negative
    one the second，row is negative one eight negative 1 and。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那张图像的内核，它将是一个三乘三的内核，和之前看到的那种内核相同。这是内核的列表表示，和我刚刚给你展示的矩阵相同，它是负一、负一、负一，第二行是负一、八、负一。
- en: the third row is all negative ones and。![](img/0b60cfa2ad22087bb0426f088d1cbfc6_13.png)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 第三行全是负值和。![](img/0b60cfa2ad22087bb0426f088d1cbfc6_13.png)
- en: then at the end I'm gonna go ahead and，show the filtered image so if for。example
    I go into convolution directory，and I open up an image like Bridge dot。PNG this
    is what an input image might，look like just an image of a bridge over。a river
    now I'm gonna go ahead and run。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然后最后我要展示过滤后的图像，所以如果我进入卷积目录，打开一张像Bridge.png这样的图像，这就是输入图像的样子，只是一张桥在河上的图像，现在我将继续运行。
- en: '![](img/0b60cfa2ad22087bb0426f088d1cbfc6_15.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b60cfa2ad22087bb0426f088d1cbfc6_15.png)'
- en: this filter program on the bridge and，what I get is this image here just by。taking
    the original image and applying，that filter to each three by three grid。I've extracted
    all of the boundaries all，of the edges inside the image that。separate one part
    of the image from，another so here I've got a，representation of boundaries between。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在桥上的这个过滤器程序，我得到的就是这张图像，只需通过将原始图像应用该过滤器到每个三乘三的网格。我提取了图像中分隔不同部分的所有边界和边缘。
- en: particular parts of the image and you，might imagine that if a machine learning。algorithm
    is trying to learn like what，an image is of a filter like this could。be pretty
    useful that maybe the the，the machine learning algorithm doesn't。care about all
    of the details of the，image it just cares about certain useful。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图像的特定部分，你可以想象如果一个机器学习算法试图学习图像是什么样的，这样的过滤器可能非常有用，因为也许这个机器学习算法不在乎图像的所有细节，它只关心某些有用的特征。
- en: features it cares about，particular shapes and are able to help，it determine
    that based on the image。this is going to be a bridge for example，and so this type
    of idea of image。convolution can allow us to apply，filters to images that allow
    us to。extract useful results out of those，images taking an image and extracting。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 它关心的特征，特定形状能够帮助它基于图像确定，这将是一个桥，例如，因此这种图像卷积的想法可以让我们将过滤器应用于图像，从而提取出有用的结果。
- en: its edges for example and you might，imagine many other filters that could be。applied
    to an image that are able to，extract particular values as well and a。filter might
    have separate kernels for，the read values the green values and the。blue values
    that are all summed together，at the end such that you could have。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，它的边缘，你可能会想象许多其他可以应用于图像的滤镜，它们能够提取特定的值，一个滤镜可能会有单独的卷积核，针对红色值、绿色值和蓝色值，最后将这些值相加，这样你就可以得到。
- en: particular filters looking for is there，red in this part of the image are there。green
    and other parts of the image you，can begin to assemble these relevant and。useful
    filters that are able to do these，calculations as well so that then was。the idea
    of image convolution applying，some sort of filter to an image to be。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 特定滤镜在寻找图像的某部分是否有红色，其他部分是否有绿色，你可以开始组合这些相关和有用的滤镜，能够进行这些计算。因此，这就是图像卷积的概念，将某种滤镜应用于图像。
- en: able to extract some useful features out，of that image but all the while these。images
    are still pretty big like there's，a lot of pixels involved in the image。and realistically
    speaking if you've got，a really big image that poses a couple。of problems one
    it means a lot of input，going into the neural network but two it。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 能够从图像中提取一些有用特征，但这些图像仍然相当大，图像中涉及很多像素。实际上，如果你有一个非常大的图像，这会带来几个问题，一个是输入到神经网络中的数据量很大，另一个是。
- en: also means that we really have to care，about what's in each particular pixel。whereas
    realistically we often isn't，like if you're looking at an image you。don't care
    whether it's something is in，one particular pixel versus the pixel。immediately
    to the right of it they're，pretty close together you really just。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这也意味着我们确实需要关注每个特定像素的内容。而实际上，我们通常并不在意，比如当你查看图像时，你并不关心某个东西是否在一个特定像素中，而是在它旁边的像素中，它们距离很近，你实际上只是在乎。
- en: care about whether there's a particular，feature in some region of the image
    and。maybe you don't care about exactly which，pixel it happens to be in and so
    this is。a technique we can use known as pooling，and what pooling is is it means
    reducing。![](img/0b60cfa2ad22087bb0426f088d1cbfc6_17.png)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 关注某个区域是否存在特定特征，也许你并不关心具体是哪一个像素。所以这是一种我们可以使用的技术，称为池化，池化意味着减少。
- en: the size of an input by sampling from，regions inside of the input so we're。gonna
    take a big image and turn it into，a smaller image by using pooling and in。particular
    one of the most popular types，of pooling is called max pooling and。what max pooling
    does is it pools just，by choosing the maximum value in a。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从输入中的区域进行采样来减小输入的大小，因此我们将把一张大图像变成一张小图像，使用池化，特别是最流行的池化类型之一被称为最大池化。最大池化的作用是通过选择某个区域的最大值来进行池化。
- en: particular region so for example let's，imagine I had this 4x4 image but I。wanted
    to reduce its dimension so I，wanted to make it a smaller image so。that I have
    fewer inputs to work with，well what I could do is I could apply。two by two max
    pool where the idea would，be，that I'm going to first look at this。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我有这个4x4的图像，但我想减少它的维度，想将其变为一张较小的图像，以便我有更少的输入可以使用，那么我可以应用2x2的最大池化，想法是，我首先查看这个。
- en: two-by-two region and say what if the，maximum value in that region well it's。the
    number 50 so we'll go ahead and just，use the number 50 and then we'll look at。this
    two-by-two region what is the，maximum value here 110 so that's gonna。be my value
    likewise here the maximum，value looks like 20 go ahead and put。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个2x2区域中，如果我们查看该区域的最大值，那就是数字50，所以我们将使用数字50，然后我们将查看这个2x2区域，最大值是110，所以这将是我的值，同样，这里的最大值看起来是20，我们就放。
- en: that there then for this last region the，maximum value is 40 so we'll go ahead。and
    use that and what I have now is a，smaller representation of the same。original
    image that I obtained just by，picking picking the maximum value from。each of these
    regions so again the，advantages here are now I only have to。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后的区域，最大值是40，所以我们将使用这个值，而现在我有的是同一原始图像的一个较小表示，这个表示是通过从这些区域中选择最大值获得的。因此，这里的优势在于现在我只需。
- en: deal with a two by two input instead of，a four by four and you could imagine。shrinking
    the size of an image even more，but in addition to that I'm now able to。make my
    analysis independent of whether，a particular value was in this pixel or。this pixel
    like I don't care if the 50，was here or here as long as it was。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 处理一个二乘二的输入，而不是四乘四的输入，你可以想象。进一步缩小图像的大小，但除此之外，我现在能够。使我的分析独立于是否，某个特定的值在这个像素或。这个像素中，我不在乎50是在这里还是在那里，只要它存在。
- en: generally in this region I'll still get，access to that value so it makes our。algorithms
    a little bit more robust as，well so that then is pooling taking the。size of the
    image reducing it a little，bit by just sampling from particular。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在这个区域，我仍然可以获取到那个值，这使得我们的。算法变得更加健壮，因此这就是池化，减少图像的。大小，通过从特定位置进行采样，稍微减少一点。
