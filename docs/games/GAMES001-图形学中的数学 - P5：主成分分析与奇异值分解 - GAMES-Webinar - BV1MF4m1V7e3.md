# GAMES001-图形学中的数学 - P5：主成分分析与奇异值分解 - GAMES-Webinar - BV1MF4m1V7e3

![](img/bb977eb198a22aec81d6d71d60cb6fd1_0.png)

行那就大家这个那我们就开始啊，首先呢祝大家愚人节快乐是吧哈哈4月1号，这也是节日，然后今天跟大家讲，这个奇异值分解与主成分分析哦，这一块，本来应该是这个线性代数里面的一部分内容。

但是呢就因为因为他这个非常的重要，然后在图形学里面应用也非常的广泛，所以呢就单独把它拿出来做一节课，然后给大家重新推一遍，然后以及看一些他的一些啊性质啊，以及应用之类的一些东西，好吧。

那我们AO那我们研究这个，不管是奇异值分解也好，还是主成分分析也好，其实我们主要研究的是什么问题呢，是研究我们是如何理解一个矩阵，那么通过前面几次课的这个介绍，大家应该也就接触到了很多。

这个用矩阵去表达这些啊，数据结构的例子，比如说一个矩阵可以表达一个三维的，或者是二维的线性变换，然后它还可以单独表示一张图片，也可以表示一些点云的几何数据，你把它组合到一起，它也是一个矩阵对吧。

那么我们如何去理解这些矩阵背后所代表的，这个更深层次的这些含义，啊也就是说它所表示的信息到底是什么，那我们就可以通过今天介绍的这个SBD，以及PC这样的技术，来去分析这个矩阵的一些特性。

那么我们首先介绍这个奇异值分解，也就是singular value decomposition，为了推导这个奇异值分析，我们先从这个实对称矩阵开始来进行分析，那么我们先证明两个两个定理。

第一个定理是实对称矩阵的特征值，它应该是一个实数，这个这个要怎么证明呢，对于任意一个实对称矩阵S，那么它一定存在这个特征值和特征向量，对不对，我们现在这个复述一下来讨论，那么它一定是存在的。

它一定存在一个拉姆达和对应的X，满足SX等于拉姆达X对吧，然后并且我们还做一个规划，就是这个X它乘以它的复转制，它结果应该是一，那么这个X和拉姆达就是一对，特征值和特征向量，它有可能是负数。

那么我们就可以对它求一个共轭对吧，辅呃负向量是可以共轭的，那么这个共轭的结果呢，就是sx store等于拉纳stx st，因为S本身是一个实对称矩阵，所以它的共轭就应该等于它本身对吧。

那么我可以在这个式子左边，同时乘以一个X转置，得到X转置XX等于X转置乘以lambda star x star，然后呢，因为这个拉姆达star是一个常数，所以我们可以把它提出来，提出来的结果。

那就是X转置乘以X到，那么我已经做了这个归一化了，所以它的结果是一，那所以这个结果就应该是lada store对吧，所以我们先对这个上面这个式子进一步共轭，得到这个结果，然后呢。

我们还可以对上面这个式子就对SX等于la s，这个式子求一个转置，得到X转置乘以X等于拉姆达X转置啊，这地方写错了，这地方应该是X转置，可能现在一些啊，啊对这个地方应该是X转置，因为S本身它是对称矩阵。

所以它的转置应该等于他自己对，所以有这个结果，然后呢我可以在这个等式两边，右边两边同时乘以一个X2，乘以一个X的这个共轭，那么可以得到这个X转置乘以sx st，等于后面这个结果，后面这个结果呢。

因为拉姆达本身是实数啊，不一定是实数，而是一个常数，所以我还是可以把拉姆达提出来，然后X转置乘以X2，它又是一，所以这个结果它就等于拉姆达，那么观察这两个结果，就可以发现说我左边其实是一样的。

都是X转置乘以X乘以X的共轭，那么它既可以等于拉姆达的共轭，也可以等于拉姆达，那么于是就可以得到结论，拉姆达其实等于拉姆达共轭，那也就证明了说，如果这个不对称矩阵存在一个特征值的话。

那么这个特征值一定是实数，这是我们的第一个结论，第二个结论是说，只对称矩阵不同特征值的特征向量，应该是相互正交的，这个怎么证明呢，我们可以取两个特征值以及对应的特征向量，就是X0和X1。

然后我可以去从下面这个等式，也是在这个拉姆达0X0转置，右边再乘以X1，那么这个式子左边拉姆达零乘以X0转置，它就可以回到这个实对称矩阵的这个形式，因为它是实对称矩阵的特征值，特征向量可以把X乘回来。

变成X0转置乘以X转置乘以X1，然后呢因为这个S本身是实对称矩阵，所以X转置等于S对吧，那么我就可以变成这个样子，然后X乘以X1，它又可以用特征值的性质给它展开，变成拉姆达一乘以X0转置乘以X1。

所以我又得到了这样一连一连串的等式，然后呢注意这个等式里面X0转置乘以X1，我可以把它提出来，然后呢只剩下这个X拉姆达零和拉姆达一对吧，那么如果拉姆达零和拉姆达一本身是不相等的。

那么一定这个等式成立的话，就一定有这个X0转置乘X等于零，换句话说就是，X0和X1应该是一个相互垂直的关系，也就是相互正交的关系，所以这就是我们证明了啊，这个实对称矩阵的第二个性质。

所以实对称矩阵它的特征值能够保证，一定是实数，并且不同特征值的特征向量一定是相互正交的，那我就可以保证说，任意一个实对称矩阵S，他一定能够写成这个q lambda q转置的形式。

中间这个希腊字母是大写的lambda，也就是写成后面这个矩阵相乘的形式，那么其中呢Q它是一个正交矩阵，满足Q转置乘以Q等于一个单位阵，已经有这样一个结论，这是我从我们刚才那一页PPT的结论得到的。

因为我们证明了说S矩阵所有特征值都是实数，并且呢这个不同泰勒值的特征向量相互正交的，那么S1定有这样的一个分解的形式，这个大家能理解吗，OK那有了这样一个分解形式之后，我们其实就对S这样一个实对称矩阵。

有了更清晰的一个一个了解，中间的这个拉姆达零到拉姆达N减一，这样一个N维的一个对一个对角矩阵，它就它就是S所有的这个特征值，那么整个的分解叫特征值，分解中间这个特征值有正有负。

就是当S当S的秩为R的时候，那么这个lambda里面只有R个非零，然后剩下的全都是零，同时如果拉姆达都是非负数的话，也就是所有的拉姆达大于或等于零的话，那么S就是一个半正定矩阵。

这个为什么叫它半正定矩阵呢，就是你可以干一件事情，就是你去求这个X转置，对于任意一个向量X求X转置乘以SX，这个这个值的大小，我们可以把这个S用这个特征值分解展开，变成Q拉姆达Q转置的形式写成这个样子。

然后呢你就会发现左边其实就是Q转置，X括号起来的转置，然后右边是Q转置乘以X对吧，所以我就可以写成后面这个形式，后面这个形式表达的是Q转置X，他在拉姆达这个内几下它的这个模长的平方。

其实也就是左边这个形式，那么左边这个形式，由于这个拉姆达本身，我能够保证它所有都是拉姆达，零到拉姆达N减一都是非负数，那么这个结果乘出来就一定是一个非负数，所以这是半正定矩阵非常重要的形式。

然后我们可以通过这个实对阵矩阵的特征值分，解把它证明出来，然后同时呢，对半正定矩阵我们还可以继续往下变形，就是中间的这个啊使对角矩阵LADA，我可以，把它写成一个sigma的平方的形式。

然后这个西格玛也是一个对角矩阵，然后它的取法非常简单，也就是西格玛零等于根号拉姆达零，西格玛N等于西格玛N减一，等于拉姆达根号拉姆达N减一，就这么一个对角矩阵，把这个拉姆达矩阵的每个对角项。

都给大家开根号得到sigma，那么一定有sigma的平方等于拉姆达，写成这个样子之后呢，那我们就可以对于S这个矩阵，实对称矩阵的特征值分解给它进一步开，给它写成这个two sigma的平方Q转置的形式。

然后写成这个样子之后呢，我还可以进一步在这个西格玛平方中间，给它插上一个Q转置乘以Q，因为我保证了这个Q矩阵是一个正交矩阵，那么Q转置乘以Q就是一个单位置，所以乘进去是不影响它的结果的。

乘进去之后你会发现Q西格玛Q转置这个矩阵，我把它定义成P的话，那么一定就有关系，这个S其实它直接可以等于P的平方对吧，所以这个P呢，它其实就是一个，类似于这个实对称矩阵的平方根的一个矩阵。

我们可以对任意一个实对称矩阵，如果它是半正定矩阵，也就是它的所有的特征值都是非负的话，那我们就可以定义这样一个P，这个P呢同样也是一个实对称矩阵，对吧，可以定义一个实对称矩阵，P满足P的平方等于X。

这是关于谁对称半正定矩阵的，另外一个非常有用的一个性质，然后同时呢，我们还可以去计算这个实对称矩阵的，它的trace trace是什么，Trace，就是把所有的这个对角线的元素给它加起来，对吧。

那S的trace等于Q乘以拉姆达，乘以Q转置的trace，然后这个trace计算又有一个非常有用的性质，就是trace ab等于trace ba对吧，这个大家如果学习线性代数的话，应该知道。

那我就可以把这个Q转置给它，移到左边来，变成求trace，Q转置乘以q lambda，然后Q转置Q呢就是单位阵，所以它就等于trace on a trace，拉姆达的话，由于拉姆达是单位阵。

所以trace拉姆达，也就等于所有的这个拉姆达的求和，那换言之，就这个实对称矩阵，它的G也就是它的trace，其实是等于它的所有特征值来进行一个求和，这就是关于实对称矩阵的一些性质，然后这个推导呢。

大家如果就是之前学习过线性代数，就是线性代数，是作为我们这门课的一个基础课程，就是大家如果呃，觉得看这些推导还是比较吃力的话，那么还是建议大家先回去，就是啊学一下这个线性代数对吧，然后因为线性代数。

毕竟还是一个非常基础的一个东西，就是呃它是很多东西的一个基石嘛，包括我们今天讲的这个啊，很多东西，OK吗，那关于实对称矩阵呢，我就讲到这里，就是借着它的性质，就以及它的推导，就大概提到这个地方。

然后有了这个基础之后，我们接下来就可以推到这个啊，这个奇异值分解，SVD分解，那么奇异值分解怎么导出来的呢，首先我们考虑一个任意的矩阵，A它是一个M乘以N的矩阵，它不一定是一个方阵。

然后它的质呢我认为是R就是给定的秩是R，然后我可以干一个操作，就是我去求这个A的转置乘以A把它定义成S，那么这个S矩阵它一定是一个对称的，半正定矩阵，这个是为什么呢，因为我可以。

因为我可以我可以求一下任意一个向量X哎，对任意的一个向量X求XXX转置，X转置乘以X乘以X，他应该就等于X啊，这个这个笔用的不是很熟练啊，不好意思，X转置乘以X再乘以X，它应该等于X转置乘以A。

转置乘以A乘以X对吧，那这个东西它应该就是ax的模长的平方，那么它一定是大于等于零，所以呢对于这样一个A转置乘以A的一个形式，它一定是一个对称的半正定矩阵，那已知它是对称半径力之后呢。

我就可以去利用我们之前的这个知识，去找到它的一组R个正交基，一组R个正交向量是它的这个特征向量，因为S本身是对称的，并且它是半正定的，那我一定可以找到这一组V和拉姆达，然后满足第一是他的特征值为。

拉姆达的特征向量，然后并且呢这个V它是相互正交的，OK那么有了这样一组V之后，我们可以把这个给他写开对吧，写开成A转置乘以A乘以vi，等于LDAIA乘以vi，然后呢同样的我们去看这个啊。

不是我们在这个这个结果前面这个结果上，左边再给它乘以一个vi转置，然后看这个结果，vi转置乘以A转置乘以A乘以VJ，那么根据刚才这个特征值的关系，我可以把这个后面这个东西写成，拉姆达J乘以VJ。

然后vi和VJ之间因为它是相互正交的，所以他就应该等于德尔塔IJ，所以他最后结果等于拉姆达J横一点开IG，那为什么要推这个东西呢，对这个形式推完之后，我们可以重新定义一个向量U，它满足这样的关系。

UI等于根号拉姆达I分之一乘以A再乘以VI，我们定义这样一组向量UI，那么根据上面推的这个关系，我们就一定有这个UI转置乘以U，J等于德尔塔IJ，对吧，就是利用上面导出来的这个关系。

那么换句话说从这个结果来看的话，UI本身它也构成了一组二个正交集，所以UI是一组正交基，然后vi也是一组正交基，并且UI和vi之间满足这样的关系，这个是由这个A转置A，这个矩阵是一个对称半正定矩阵。

然后利用我们上一页PPT的东西导出来的，那有了UI和vi之后呢，我们就可以使用这个史密斯正交化，把VI扩充成为这个RN空间的N个正交集，因为本身这个A矩阵它是一个值为R的矩阵，所以我只能定义R个vi。

然后我可以通过，但是我可以通过正交化，把这个vi扩扩充成为RN的N个正交集，然后同时呢，也可以把这个UI扩充成RM中的M个正交集，就通过这个史密斯正交化得到，那么扩充的积呢，首先它满足扩充了之后。

跟这个比如说vi扩充之后呢，他跟之前的这些向量都依然是正交的，同时UI扩充之后呢，跟之前项也都是相互正交的，并且呢我们之前R个向量满足了这个关系，同样对于扩充的向量也是满足的，为什么满足呢。

因为对于扩充的向量而言，这个它的对应的这个拉姆达J1定是零，对吧，因为因为我的质只有R，那我扩充值扩，那我除了这前R的特征值是大于零的之外，其他的应该全是零，所以这个LJ应该是零，所以左边应该等于零。

然后右边的A乘以VJ一样的，因为VJ是你扩充出来的，所以它结果也是零，OK吧，所以通过这个零，我可以认为这个根号拉姆达JUJ等于AVJ，然后你把这个关系和这个关系联系在一起看，发现他们俩其实是一个关系。

我把这个式子的，把这个UI定义式的根号兰姆达I乘到左边来，变成根号兰姆达IUI等于AVI，他就满足这样的关系对吧，那么这个关系我可以把UI和vi都给他，写成矩阵形式，就是定义两个矩阵。

一个V1个UV的每一列就是VIU的，每一列就是UI，那么上面这个关系就可以写成这样的矩阵形式，A乘以B等于U乘以西格玛，sigma一样的是一个对角矩阵，它的每一个对角圆就是这个根号拉姆达J。

所以这个av等于U西格玛，就是这个这个关系以及这个关系的一个，矩阵形式，那么到这里，我们离最终的这个SBD就只差一步之遥了，我们只需要把这个V矩阵给它乘一个转置，放到等号右边来。

就可以得到这样A的分解关系，A等于U西格玛B转置，其中呢U和V是两个正交矩阵，西格玛是一个对角矩阵，然后呢，其实我们刚才其实没有，假设这个A矩阵它是一个方阵，对不对。

我们只是说它是一个任意的M乘以N的矩阵，那么自然呢刚才在推的时候，这个U呢我是把它扩充成这个RM的基，所以一共是M个向量，所以它应该是一个M乘以M的矩阵，然后V呢是把它扩充成RN的G。

它一共是N乘以N的一个大小，那么中间的这个对角矩阵西格玛，其实它也并不是一个方阵，它是一个M乘以N的矩阵，但这不妨碍我去定义定义它的这个对角圆，就是它的对角圆，我还是他的第I行，第I列。

那么它的对角圆就是这个根号拉姆达I，也就是这个我们称之为A矩阵的奇异值，Single value，然后由于A矩阵本身它的秩是R，所以我只能找到R个不为零的这个奇异值，根号拉姆达I。

其他的所有对角元以及非非对角元都是零，这个就是任意矩阵的一个奇异值分解，如果这个推导，前面大家觉得呃就是还是太跳跃的话，就是还是推荐大家就是去系统的学习一下，这个线性代数的知识，然后呢我们今天其实是呃。

就是如果在大家真的不理解，前面的这些推导的话，也没有关系，只要理解这个形式就可以了，这个形式对于线性代数非常重要，因为它非常的朴实对吧，我是一个任意的一个矩阵，A1个不是方阵的都可以的一个矩阵。

并且也不一定是满质的，他都可以写成这样一个U，西格玛乘以V转置的形式，其中V和呃，U和V是两个正交矩阵，然后西格玛是一个类似于对角矩阵的，一个一个矩阵，这个就是一般矩阵A的奇异值分解。

如果大家实在不理解的话，就可以先我们先把这个形式记住，然后呢我们再看一下这个形式，它对于呃我们后续计算呢有哪些帮助，那么奇异值分解是线性代数中，非常重要的一个概念，它在图形学中的应用也是非常广泛的。

如果我们已经知道这个矩阵的任意一个，矩阵的奇异值分解，那我们就可以由这个奇异值分解的形式去推，这个矩阵的逆是什么，矩阵的秩是多少，以及这个矩阵的它的最小二乘，矩阵方程对应的最小二乘解是什么。

以及包括其他的分解，比如说极分解，它都可以通过这个极值分解得到的红R，以及还有很多很多的这个应用，那我们先看一个例子，比如说矩阵的逆与这个矩阵的极值分解，之间的关系，我们先假设一个可逆矩阵。

也就是一个满志的一个方阵A，然后去得到它的极值分解，A等于U西格玛比转置，那么通过这个形式我就可以推到这个A的逆，他应该写成什么样子，A的逆就应该等于U西格玛V转置，括号起来再加个逆。

然后这个利用这个括号的矩阵相乘的一个，need的展开式把它展开，那就可以展开成后面这个样子，然后又因为这个V矩阵和U矩阵是两个方阵，所以V转置的逆就应该是VU转U的力，就应该是U转至。

所以我就直接写到后面这个样子，然后sigma本身它又是一个单位矩阵，它不是单位，它是一个对角矩阵，那么这个对角矩阵的逆，也就是把它的这个对角线上的元素都给大家，求一个相反数。

它依然是一个对角矩阵放在中间，所以如果我已知A的SD分解的话，我是可以立马写出A的矩阵的逆，它的形式应该长成什么样子，这个样子你也可以把它理解成是这个AE，这个矩阵的，它的SVD分解的形式。

所以一旦你知道这个矩阵的奇异值分解，你自然就可以知道这个矩阵的逆，但是呢我们刚才说了，SVD它不只适用于A矩阵，是一个满志的方阵的情况，它是适用于A是任意的矩阵的情况，那比如说我们可以先这样假设。

我们假设A是一个列满秩的矩阵，但是呢它的行数是比列数要大的，也就是说它是一个长得像一个很长的，这样一个矩阵，那么如果我要去求解ax等于B的这个情况，那么这样一个矩阵方程。

它就是一个我们称之为超定的矩阵方程，因为我的未知数的个数就是X的，尾数就是N，但是我的方程个数呢是A的这个函数，也就是B的行数，它应该是M，但是我们刚才说了，行数大于列数的M要大于N的，所以A矩阵的呃。

AAX等于B这个矩阵方程，它的方程个数是要大于自由度个数的，那么意味着解其实它不一定存在的，那么这就是一个超定情况，那么为了求解这样一个超定的矩阵方程，首先它的解是不一定存在的。

那我可以取而代之去求一个什么呢，求一个最小二乘意义上的解，也就是求解这个一个X使得ax减B，这个我们称之为残差的东西，它的模长是最小的，比如说对于超定情况，它的真实解，它的严格解是不一定存在的。

但是我可以转而求其次去求一个X，使它满足这样的一个残差最小的情况，那么也可以认为这个X这个渠道最小的，这个X就是我想要的一个最好的解，那么这样一个X要怎么得到呢，我们需要求解这样的优化方程。

那么为了把这个问题理解的更清楚一点，我们可以利用这个SBD分解来看这件事情，我们把A通过SBD分解写开，那么X等于B就可以写成后面这个样子，然后写成后面这个样子之后呢，我可以统一的把U提出来。

然后后面这个B他前面其实什么都没有对吧，我可以假设它前面乘一个单位矩阵，然后因为U本身是一个正定额，是一个正交矩阵，那么也就可以写成这个U乘以U转置的形式，所以他应该是等于后面这个结果对吧。

U乘以U转置乘进去，它就是个单位，所以左边等于右边，那么我注意到一个结果，这个U本身是因为它是一个正定矩阵，它是一个正交矩阵，那么其实它是不会改变这个向量的模长的，对吧。

所以呢这个AX减B这个残差的平方，它其实就等于这个括号里面的，这个结果的残差的平方，那如果我进一步假定，我令X1撇等于V转置乘以X，B1撇等于U转置乘以B，我定义两个新的向量，那么就右边这个这个关系。

X等于B这个方程的残差的平方，它应该等于西格玛X1撇减B1撇的平方，那么通过这种方式，我就可以把原来的一个X等于B的问题，转换成右边的这样一个矩阵方程的问题，然后这个矩阵方程它有一个特别好的点。

就是这个西格玛矩阵现在是一个对角矩阵，它是一个非常好研究的东西，对吧，那我们就看一下，我们要问什么样的X，使得上面这个优化方程最小，那也就是问我们有什么样的X1撇的，后面这个结果最小，那什么样的X1撇。

会使西格玛X1撇减去B的模长的平方最小呢，你可以把西格玛乘以X1撇成塞，打开这个后面这个形式对吧，因为这个西格玛矩阵我们刚才说了啊，A矩阵它是一个列满秩的矩阵，所以它应该有N个不为零的。

这个特征是N个不为零的奇异值，然后以及剩下的应该全都是零，所以这个西格玛乘以X1撇，应该可以写成后面这个样子，写成四杠零乘以X01撇，这是第一项，一直到后面前面N项不为零的，然后后面所有项都是零。

那么自然如果你想让这个SIGMX1撇，减去B1撇，他这个残差的平方取到最小，那我就应该有关系，前面这N项应该对应的就是B的这N项，B的前N项，然后N项因为全是零，所以我不管怎么改变XC撇。

我都改变不了他是零的事实，那我就不管它了对吧，所以我只需要管前面的这N个方程，肯定是满足的，那么前面这N个方程满足，我就可以把X1撇写出来，可以写成X1撇，等于西格玛heat逆乘以B1撇。

为什么是西格玛height逆呢，因为这个啊西格玛矩阵，它本身不是一个方阵对吧，然后呢我的X1撇和B撇满足这个关系，这个关系我通过这样一个类似于西格玛矩阵，逆矩阵的一个矩阵，把它写成矩阵相乘的形式。

那么这个西格玛height逆这个矩阵它怎么定义呢，就是它是一个它的大小是这个sigma矩阵的转置，sigma矩阵本身是M乘以N的矩阵，那么sigma hat knee这个矩阵，它就是一个N乘M的矩阵。

然后同时呢，它的对角线对应的是原来sigma矩阵，对角线的一个导数，那么我定义了这样一个seek height，逆之后，把它乘以B1撇，得到了这个X1撇，也就是满足上面方程的X1撇，你只要把它的分量。

每一个每一个每一行写出来看一下，你就会发现它是满足跟上面那个形式等价的，所以我们定义了这个西格玛hit me之后呢，只是把上面这个形式，写成一个更好看的矩阵形式，OK那写成这个矩阵形式之后呢。

X1撇和B1撇在上一页PPT里定义了，它，分别是这个啊V转置乘以X和U转置乘以B，那我就给它乘回来，也可以得到最后的这个X的形式，X等于V乘以西格玛，HT e乘以UT乘以B对吧。

那这样又回到了一个X等于一个什么矩阵，乘以B的形式，本来我要求解ax等于B，如果这个A是一个可逆矩阵的话，那X自然应该等于A的逆乘以B对吧，然后我现在由于A本身不是一个方阵。

这个方程对应的方程是一个超定的方程，但是我通过最小，如果我假定我的解是满足最小，二乘以下的解的话，X依然可以写成这样一个矩阵相乘的形式，那么就可以把中间的这个矩阵呢，把它叫成原来矩阵A的一个尾逆。

就是他是一个假的逆，我们定义的这样一个形式，那么通过刚才的分析，我可以我们就可以说说，如果你X取到这样一个值的话，他是能够满足ax减去B它的模长是最小的，也就是原来ax等于B。

这个矩阵方程的残差是最小的，那另外呢我们也可以从另外一个思路去看，这个问题，就是在前面的课程中，星语给大家介绍这个矩阵求导相关的知识对吧，那么要优化这个X减去B模长的平方，这样一个目标。

我们也可以直接从矩阵求导的思路进行推导，也就是我定义这样一个优化目标是G，然后把它写开，就是后面这个结果，然后呢我去看X取到什么值的时候，能够满足这个G对X的导数等于零嗯，因为我本身这个函数是这个目标。

是关于X1个二次函数对吧，那么它取到零的时候，一定是他取到最值的时候，那什么时候才取到零呢，我就可以对G对X求偏导，然后这个偏导的求法就是啊，之前应该介绍过前面这一项。

X转置乘以A转置乘以A乘以X这一项，他的这个呃导数就是两倍的，A转置乘以A乘以X，然后后面这两项的导数，就是这一项是A转置乘以B，这项也是A转置乘以B，所以最后结果是减去二倍的A转置乘以B。

所以如果X取到最值的时候，它应该满足G对X的偏导数等于零，那么这个形式我就可以把它变成一个新的矩阵，方程，变成这个A转置乘以A，X等于A转置乘以B变成另外一个啊，另外一个矩阵方程的形式，那么这个时候呢。

A转置乘以A这个矩阵A是一个很长的向量，A转置它就是一个很扁的向量，A转置乘以A呢，它又回到了一个N乘N的矩阵，然后并且呢由于A矩阵是这个啊，是一个列满秩的矩阵，所以A转置乘以A呢，它是一个满秩的矩阵。

那么有了这个关系之后，我就可以直接对吧，求它的逆得到X等于A转置乘以A的逆乘，以A转置乘以B，对吧，我也可以通过矩阵求导直接导出来，说你X取到这个值的时候，它是满足这个残差最小的。

那么这个结果我们称之为A矩阵的左逆，为什么这么叫呢，因为你把这个中间的这个矩阵，存到A矩阵的左边，你就会发现它其实是一个单位证，是一个A转置A的逆乘，以A转置A的形式，所以它是单位阵。

但是呢你把这个矩阵乘以A到右边，A乘以A转置A整体的逆再乘以A转置，它就不知道是什么结果了，那就不一定是一个单位，所以你既然只能乘在A的左边，让它等于单位阵，那我就叫你A矩阵的左逆。

那么我们从两条思路都导出来，一个残差最小的X的结果，那么这两个结果是不是等价的，这是证明它们是等价的，怎么推呢，我直接看这个索尼，把A的SVD带SBD分解给他带进去，求这个A转置A的，它的逆是多少。

你看A转置A然后带进去就可以得到这个啊，变成这个V乘以西格玛，转置乘以西格玛乘以VT，然后呢西格玛转置乘以西格玛，两个对角矩阵相乘，我可以啊，两个类似于对角矩阵的一个矩阵乘起来。

我就可以变成一个真正的对角矩阵，变成一个拉姆矩阵，sigma本身是一个M乘以N的矩阵，那个拉姆达矩阵就是一个N乘N的矩阵，同时它的对角元素，第二个对角元素就是这个原来A矩阵的奇异值，第I个奇异值的平方。

西格玛I的平方，我可以定义这样一个拉姆达，那么A转置A的逆再乘以A转置呢，把一样带进去就可以变成这个V乘以london nee，再乘以sigma，转置乘以UT，然后这个拉姆达nee乘以sigma转置呢。

它是什么东西，拉姆达nee它是一个对角矩阵，那我就把它的对角线，每个元素给他求一个导数，然后左乘上这个sigma转置对吧，sigma转置对角元素还是西格玛，零到sea sigma n减一。

然后如果给它乘以一个对应的这个啊，奇异值的平方的导数的话，那它自然得到的结果就是西格玛分之一对吧，然后并且呢它的这个矩阵的这个形状，应该就是CMA转置的形状，它是一个N乘M的矩阵对吧。

那这样通过这样的方式，我就已经证明了，说中间这个矩阵它确实就是我们刚才定义的，Sigma hat ne，在我们刚才推了两个逆，一个是矩阵的左逆，以一个矩阵的尾逆，这两个逆结果是一样的，那换句话说。

就是如果这个矩阵是一个列满秩的矩阵的话，它的左翼和尾翼其实是一个东西，这是第一种情况，那还有另外一种情况是什么，就是这个A矩阵它是一个很扁的矩阵，它是一个行万支的矩阵，但是呢它的列数要比行数更多。

M小于N，那我去求解X等于B这样的一个矩阵呢，它就是一个确定的矩阵，也就是说它的方程的个数是A的，行数是M是要比未知数的个数，X的尾数N要小的，那我就有无穷多个X去满足X等于B，它是一个限定的矩阵方程。

那么同样的我可以在最小二乘的意义下去呃，去假定他的几这个最小二乘，那就不是让它的残差最小，因为它的残差是能取到零的，于是我们就可以转而让xx的模长是最小的，那就可以变成下面这样的一个，带约束的优化问题。

就是我在所有的满足AX减去B等于零，这样的呃X里面，挑一个X模长最小的一个结果，对吧，这是一个对应的一个最小二乘的结果，那么对于这样的结果，我同样的可以通过这个SVE分解。

去看一看它里面到底是发生了什么，对吧，我要我要求X等于B，也就是U西格玛V转置X等于B，那么做跟刚才一样的这个变量替换X1撇，把它定义成V1转置乘以X，B1撇定义成U转置乘以B。

那么这个U西格玛V转置X等于B，也就是等于U乘以U转至B，它就等于西格玛X1撇等于B1撇对，建成这样一个矩阵矩阵方程，那同样的这里的好处就是西格玛矩阵，现在它是一个对角矩阵，那么为了让他满足这个啊。

西格玛X1撇等于B撇的要求，那么自然所有的X1撇它要满足要求，每一行矩阵你可以单独拿出来，西格玛零乘以X01撇，它应该等于B01撇，然后一直到西格玛N减一乘以X减一撇，等于BM减一撇，一共M个方程。

这个方程满足，然后剩下的N减M个方程，也就是这个C矩阵后面全零的部分，对应的这个X1撇的这些值，就是从M到N加一的，M到N减一的N减M的值呢，它是可以任意取的对吧，因为我希望后面是一个纯零的矩阵。

那么我为了让这个X1X的模长最小再多一步，就是这个X的模长，它其实是等于X1撇的模长，为什么，因为V矩阵是一个正交矩阵，对吧，所以X1撇的模长它等于X转置乘以V，V转置乘以X就等于X转置乘以X。

也就是X的模长，所以如果我想让X的模长最小，也就是让X1撇的模长最小，那X1撇的什么模长最小呢，那自然是把后面这所有的值都给它取成零，这个时候X1撇的模长是最小的。

那么这个时候我可以类似的去写出这样一个，X1撇和B撇的关系，X1撇等于西格玛heat逆乘以B1撇，然后这个西格玛height逆它是这样一个矩阵，它的上半部分是一个N乘N的呀，一个一个多少。

一个M乘M的一个对角矩阵，然后对角元素是对应特征值的导数，而不是对应奇异值的导数，然后本身的这个西格玛hat逆矩阵呢，它的形状是跟sigma转置的形状是一样的，从原来扁的矩阵变成一个行的矩阵，对吧。

那你这样写成这样一个矩阵形式的话，就可以对应它的前N项是满足这个方程的，然后同时它的错误N减M项是零，这个时候呢，我能保证它的S1撇的模长是最小的，那么对应的X模长也就是最小的。

那就满足我方才刚才定义的这个优化问题，那么一样的，我把X1撇和B撇带回去，就可以得到X和B的一个关系，那么中间的这个矩阵呢一样的，它就是我们刚才定义的这个矩阵的伪逆形式，是一样的。

所以尽管我们刚才定义了一个完全不同的一个，呃优化问题，一个最小二乘的问题，但是我们发现这个X他最后的这个解的形式，依然是一个矩阵的伪逆乘，以B矩阵的形式要乘以B向量的形式。

那么呃我们刚才用了两种推导方法对吧，一种是通过SBD的方式得到矩阵的威力，还有一种是直接求这个优化问题，得到矩阵的阻力，那么在这个地方呢，如果你想直接去求解，这样的一个带约束的优化问题。

因为我要让这个X等于B，这是一个约束，求解这样一个带约束的优化问题的话，我们需要使用拉格朗日橙色，这个东西其实是优化里面非常重要的一个概念，我们会放到后面来讲，所以今天就呃我在这里给大家推。

就是你如何从直接从这个自由化的这个形式，导出来，这个X应该等于什么，我们就直接从这个矩阵的伪逆的形式去定义，矩阵的右翼怎么定义呢，其实也类似，就这个sigma heart nee这个形式。

我可以把它展开写成后面这个形式对吧，sigma转置，它是一个点的向量啊，它是一个长的向量，然后呢西格玛乘以sigma转置，西格玛本身是一个扁的向量，sigma乘以西格玛转置呢，它就会变成一个M乘N的。

M乘M的一个对角矩阵对吧，然后把它求逆，然后再乘以到sigma转置上，那我就应该得到这样一个长向量，然后长的矩阵，然后并且这个矩阵的对角元素是对应的，这个起义值的导数。

所以sigma如果满足这样的一个形式的话，那我们再去看这个啊V乘以西格玛，heat逆，再乘以U转置的这个形式呢，它就可以等于后面这个A的形式，这个推导过程就是你把我们上一页关于这个。

我们之前关于这个矩阵左翼的那个推导，把它倒过来，如果你sigma有这样一个关系的话，那A矩阵就会有这样一个关系，那么这样一个关系，这个这个形式把它定义成矩阵的右翼，它跟左逆是不一样的，为什么呢。

因为这个逆矩阵你把它乘在A矩阵的右边，能够保证乘出来的结果是一个单位矩阵，但是呢如果你把这个矩阵乘到A的矩阵的左边，它其实不能保证它是个单位，所以A转置乘以A乘以A转置的逆矩阵的形式，称为矩阵的右翼。

那么通过两个这个矩阵方程，其实定义了三个矩阵的三个逆，左逆右逆和伟力左逆，当矩阵这个就是列满秩的时候，左逆是一定存在的，那么左逆的形式是用来求解这个超定方程的，也就是你的方程个数。

比自由度的个数是要多的情况，如果矩阵是行慢支的，那么它的右翼一定存在，这个东西是用来求解这个欠定方程的，也就是你方程的个数比自由的个数要少，这是两个特殊情况，然后在不管在什么情况下。

这个A矩阵它甚至是一个不满值的矩阵都可以，它的尾翼是一定存在的，这个尾逆的定义方法，就是通过这个西格玛heat逆矩阵的这个形式，对吧，就是它的大小是西格玛矩阵的转置，然后它的对角线是西格玛本来的对角线。

给他求个导数，所以不管什么情况下，矩阵的伪逆是一定存在的，然后如果你的矩阵是列满制的话，尾逆等于左逆，然后矩阵是行满值的话，尾逆等于右逆，然后如果矩阵是满秩的话，这个尾翼它就是一个真正的力。

那么矩阵是一个方阵，是一个可逆矩阵的话，那么这个尾翼就是一个真正的力量，所以通过这个啊特征值分解，我们就可以获取关于这个矩阵A，任意的一个矩阵A，它求解矩阵方程，以及A矩阵本身的逆的很多的信息。

对这些信息对于我们理解这个问题，就是非常直观的一个理解，OK到这里大家有问题吧，大家有问题可以现在问，特征值的个数不超过质吗啊特征值的个数啊，首先这个东西叫奇异值，奇异值的个数等于矩阵的这个日。

就是你如果置为R的话，那么其实就是呃就是有R个，也就是说这个对角线上，对角线上它除了前二个值是有值的，不管是正啊，有正值，对西格玛是一定是正的，因为它是根号定义出来的，所以它一定是正的。

然后剩下的这些值就是零了，OK那看来没有问题，那我就继续了，OK我们刚才呃，刚才介绍的是这个SVD分解，与矩阵逆之间的关系，我们现在看一些关呃，跟图形学有关的一些东西是吧。

比如说我们可以通过SVD去理解一个几何变换，对于一个任意的一个线性变换，X1撇等于ax，我都可以把A矩阵A是一个33的矩阵对吧，它是一个线性变化，它是3×3的矩阵，把它写成这个特征值分解的形式。

写成这个奇异值分解的形式，由西格玛V转置乘以X，那么这样的三个矩阵相乘，再乘以X减X的形式呢，它其实就是做了三次的这个线性变换，对三次独立的线性变换，第一次是这个V转置乘以X，那么它对应的是什么。

是一个应该是一个旋转，因为V本身是一个正交矩阵对吧，然后呢XE然后你再乘以西格玛，那它就对应一个拉伸操作，然后再乘以一个U呢，它又会变成又是一个旋转，所以一个线性变换其实我可以通过SVD分解。

把它理解成两步旋转，加上一个拉伸，但是这个地方注意一个地方，就是呃我们在旋转变换的时候，介绍说所有的旋转矩阵，它其实以呃正交矩阵的要求要严格对吧，它除了是正交矩阵的要求之外。

还要求这个矩阵的行列式是正一，而不是一，所以呢为了就是如果你要做这个啊，这个几何变换的分解的话，你可以去调整这个sigma这个中间这个对角矩阵，这个拉伸矩阵的正负号，它某个元素的正负号。

来去调这个两个正交矩阵的它的行列式，然后调到让这个啊就是把它这个加一个负号，或者去加一个负号，对某某一行加一个负号各种形式，然后去调这个V矩阵的行列式，然后把它的行列式调成正一。

就能保证这个V矩阵它就里面不会有反射，这个反射最后反应到这个中间的拉伸上对吧，对于任意一个线性变换A来说，它其实是有可能包含这个反射，这个拉伸上来，所以啊有了SPD之后，其实对于一个任意的线性变换的话。

你也可以用这样的方式去理解它，就是它一定是包含了旋转和拉伸的，那么除了这个SP分解之外，我们其实还有另外一种直观的理解，这个啊这个变化的一个形式，那就是通过积分解。

polarity composition来看这个事情，对于任意的一个十方阵A，那么它的积分解定义成为A等于R乘以S，R是一个正交矩阵，然后S是一个半正定矩阵，然后通过SVE的角度。

我们其实也可以看到这个几分解是怎么出来的，其实非常直接，就是A等于U西格玛V转置，我只需要在中间乘以一个V转置乘以V，在U和CMA中间插一个V转置乘以V，因为V本身是一个正交矩阵。

所以它差乘出来这个结果是一个单位矩阵，那么走它插进去之后呢，前面这个部分U乘以V转置，是两个正交矩阵的几，那么它结果依然是一个正交矩阵啊，后面V乘以西格玛V转置，它就对应着一个半正定矩阵。

因为中间的这个西格玛，它一定是一个非负数对吧，然后同时这个矩阵是一个对称的矩阵，对吧，所以如果你已经有了SD分解，其实你可以非常方便的得到这个几分解，就是把U乘以V转至对应的就是它的旋转。

然后后面这个V乘以西格玛V转置，就是它的这个半正定矩阵S，然后同时呢你也可以换一个插法，就是在这个sigma和V转置，中间插一个U乘以U转置，然后把它写成后面这个形式，后面这个形式的话。

它就会变成前面是一个半正定矩阵，然后后面是一个正交矩阵对吧，所以但他的这个啊这个形式，这个旋转矩阵的正交矩阵的形式是没有变的，都是U乘以BT，OK那把它拆成这个样子之后呢，他其实对应着一个啊。

也是对应着这个三维的三维的变换，首先这个U乘以V转置呢一样的，因为你是正交矩阵，所以它对应着一个旋转，然后这个U西格玛U转置这个矩阵，或者是V西格玛V转置这个矩阵，它其实干点什么事呢。

它是你先把它转一个角度，然后拉伸一下再给他转回来对吧，因为你是你第一次旋转是UT，然后第二次旋转是U嘛，所以两次旋转其实是抵消的，所以做到最后呢，其实你是把它沿着某个不是啊，呃不一定与这个坐标轴对齐的。

那个方向沿着某一个方向把它拉伸了一下，所以一个半正定矩阵，其实对应的就是一个任意的一个拉伸，所以呢这个几分解用积分解去看，这个任意的线性变换的话，那么它对应的就是直接把他这个线性变换。

分解成两步一步旋转，一步拉伸，然后两种啊这个R乘以S或者是S乘以R，他们的这个呃两种分解的顺序都可以，然后他们对应的这个旋转矩阵是一样一样的，都是U乘以BT但是这个拉伸矩阵是不一样的。

从这个结果也可以看，大家也可以看到，我可以先旋转，然后拉伸，也可以先拉伸，然后旋转，那么这个积分解或者SVD分解，在图形学中有一个非常重要的应用，叫这个shape matching，形状匹配。

它形状匹配其实是一个算法，这个算法是干什么事呢，就是说如果我有一个任意的一个形变，Q到P，也就是说把这个模型上的一个任意点的Q，映射到了点P有这样一个变化，那么我应该如何去找到一个刚性的变换。

也就是一个旋转加上平移，对应的一个旋转矩阵R以及一个平行向量T，在这个刚性变换下，能够让这个模型，它跟我最终的这个任意的一个形变，尽可能的接近，这个算法就解决的是这样一个问题，就是我有任意的一个形变。

它里面可能有旋转，可能有拉伸，甚至可能有这个啊非线性的变化，但是我如何通过一个呃刚性的变化，一个旋转加上平移去变化点Q，让它尽可能的接近我最后的结果，P这样一个问题，他其实是跟这个SBD跟奇异值分解。

是非常相关的一个问题，那我们就下面来看一下这样一个问题，需要如何去求解它，那么对于这样一个问题呢，我们首先假设R是给定的，因为我最终是要同时优化R和T，那我就先假定R是已知的。

那么我现在只看TT什么时候，什么样的T会使得这个结果是最小的，那我们就可以对这个目标函数对T求导对吧，求导求出来就是后面这样一个一个向量对吧，这个就是一个简单的一个向量，向量的平方的一个求导。

求出来后面这样一个向量，然后呢等于零，然后可以把这个求和给大家写进去对吧，R是一个常数，所以可以提出来，然后呢T1共有N个点，所以T求和就是NT然后P呢，所以求和就是西格玛PI，所以T满足这样一个关系。

N乘以T等于pi的求和，减去减去这个R乘以QI的求和，那么我可以定义两个两个量，就是分别是这个形变前和形变后的这个质心，也就是把这个所有的PI给它加起来，然后除以N作为这个形变后的质心。

然后所以所有的QI加起来，除以N变成作为这个形变前的执行，定义两个量QC和PC，那么这个T就可以自动写为，T等于PC减去R乘以QC，这个结果说明的就是说，如果我的R已知的话，那么T直接取到这个值。

它就能够满足这个呃，目标函数是最小的，那么我就可以把这样的一个T给他带进去，我刚才的这个目标函数，把T的这个结果我带进去，那变成后面这个结果，后面这个结果呢，它就是只是跟R相关的一个结果。

它跟T就没有关系了，但是T已经被我约掉了，然后呢我再定义两个量，就是pi1撇，QI1撇，分别对应的是这个点P这地方少打一个I啊，pi减PC也就是点pi相对于G的呃质心的偏移。

然后QI1撇呢就是QI相对于质心的偏移，定义两个偏移量，那么R它对应的优化问题，就会变成下面这样一个优化问题，它就跟T没有关系，我们接下来的任务就是，怎么样去求解这样一个最优化问题。

那么求解这样一个最优化问题呢，我们把它展开来看，这样一个呃乘平方求和的形式，这个平方展开出来里面会有四项，然后这四项有两项是跟R没有关系的，第一项是这个pi1撇乘以PI1撇，转置乘以PI1撇。

这个东西它是我形变后的，这个我给定的形变后的这个点P，它的点pi的内积，那这个东西它是跟R没有关系的，然后最后一项它里面有一个R转置乘以R，由于R1定是一个正交矩阵，所以R转置乘以R呢一定是个单位阵。

所以后面最后这一项其实也跟呃，也跟这个选举人偶尔没有关系，然后跟R最有关系的是中间两项，然后中间两项呢其实都是什么，都是PI1撇和R乘以PI1撇的内积，对pi1撇转制成RQI1撇。

然后RQI1撇乘积的转置乘以PI1撇，都是这两个向量的内积，那么我为了让这个最后的目标函数最小，也就是让这个内几，然后求和它的结果是最大，所以转换成后面这个形式，然后后面这个形式呢我可以进一步做变换。

把它写成一个更好看的形式，怎么变化呢，就是把这个这个东西把它写成一个矩阵，trace的形式，然后注意到前面这个这一项，PI1撇，转置乘以R乘以PI1撇，这个东西它是一个标量对吧，那么一个标量。

我自然是可以把它写成trace的形式，因为一个标量的G就是这个标量本身，那为什么要把它写成trace的形式呢，是因为这个时候里面这个结果，因为我计算的是它的G，我就可以利用几G的这个运算性质。

A乘以B的G应该等于B乘以A的G，所以我就可以把这个GI1撇，转置这个东西乘到最后，然后同时宁愿华是一个一个长长长长线长矩阵，所以把R提到这个求和的外面来，最后变成R乘以啊，后面这个求和的形式，对吧。

这个求和形式他现在就跟R1点关系没有了，那我同样可以定义一个常数的矩阵，定义一个H而写成H等于后面这个东西，然后呢，我的目标就变成去最大化这个R乘以H的，Trace，这个就是我现在的优化目标。

然后这个HH矩阵是一个跟R无关的东西对吧，那么为了求解这个最优化形式，其实它对应的有一个定理，我们其实是利用这个定理来告诉说，R应该起到什么时候，会让他这个trace结果最小，这个定理它的形式是说。

对于任意的对称正定矩阵，M和正交矩阵B1定有M矩阵的trace，是大于等于B乘以M的trace，一个非常简洁的关于G的一个不等式，那么这个证明呢可能稍微有点复杂，对这块大家也也不需要理解。

其实就是你如果觉得你看的，你觉得你能看得懂的话，你可以尝试去呃尝试去看一下，然后如果你觉得你肯定看不懂的话，你就不需要在上面花时间对吧，只是有这样一个定理，就是如果你之后啊无意中碰到的话。

你就是再回头来看一看这个证明，然后这个证明了它整体的逻辑是什么，整体逻辑是说我把这个M矩阵，因为M矩阵是对称正定的矩阵，那么我就可以用最开始给大家介绍的，对称正定矩阵的那个平方根分解的那个形式。

就是我可以定义一个S矩阵，它也是对称正定矩阵，然后呢满足X的平方等于M，利用这个分解，然后把这个trace和trace bm，都写成更简单的形式，把trace bm写成trace b乘以X的平方。

然后呢利用trace信是把一个S乘到左边来，然后因为S本身是一个对称矩阵，所以可以写成trace x转置乘以B乘以S，然后呢我可以直接去计算trace m，也就是S转置乘以S的。

它的对角线应该是SI转置乘以SI，SI是S矩阵的第I列，那后面这个矩阵S转置乘以B乘以S，它的对角线，它的DX对角项应该是SI转置乘以B乘以SI，所以这两个矩阵的对角线，我都可以直接写成S和B的关系。

然后那自然的trace也就可以写成，两个对角项求和，那么写成这个形式之后呢，关键的这个不等号就在这个地方出来，也就是说对于每一项SI转置乘以B乘以SI，它一定是小于等于SI转置乘以SI的。

因为这个B转B矩阵它只是一个正交矩阵，它是不会改变这个SI矩阵的模长的，所以你SI去点乘上一个BSI，这个向量它一定是不会小于等于啊，它是一定小于等于SI转置乘以SI的，那于是也就自然得到了trace。

BM是小于等于trace m的，OK这个证明可能稍微有点难度，就是大家有有余力的话，可以去稍微仔细理解一下，没有语义就算了，就是有这么一个定理存在就行，那么有这样一个定理的话，我们就可以说。

如果我存在一个R矩阵，让这个R星存在一个R1个一个正交矩阵R星，它可以让R星H是一个对称正定矩阵的话，那么一定下有下面的关系，就是trace rh对于任意的矩阵R，任意的一个正交矩阵。

RRH的trace还可以写成后面这个结果，后面这个结果中间我插了一个R星，转置乘以R星，为什么要插呢，你就会发现前面这一项结合出来，它是一个正交矩阵，后面这一项结合出来，它是个对称矩阵。

而我们根据刚才的定理，一个正交矩阵乘以对称矩阵，它的trace是一定小于，等于这个对称矩阵本身的，也就是小于等于RCH的trace，所以如果可能在一个R星矩阵，可以让R星H是对称正定的矩阵的话。

那么这个R就一定是使得rh trace，最大的那个矩阵，这可能稍微有点绕，OK但是结论就是这个结论是清晰的，就是说什么情况下，偶尔矩阵会使得RH的trace最大呢，那就是一个R矩阵。

使使得RH这个矩阵是一个对称正定矩阵，那RA是什么时候对称正定呢，你就可以求，你就可以把H的这个SBD分解写出来，H等于U乘以sigma乘以B转置，然后如果这个R等于V乘以UT的话。

那么这个时候R乘以H等于V乘以，西格玛乘以VT，这个东西它就是一个对称正定矩阵对，因为西格西格玛里面每一项都是正，每一项都是非负的，然后V是VV和V转置，又是一个最后是一个对称的一个结果。

所以如果你R写成V乘以UT的话，那么RH就是对称正定矩阵，那么这个R就是使得我们之前给出的那个，trace最大的那个结果，OK当然这个地方需要有一个修一个小bug，就是我们刚才就是这个U乘V乘以UT。

这个矩阵它不一定是一个行列式为正义的矩阵，那么如果他的行列式是一的话，那么最终的这个对应的这个旋转矩阵，它其实是中间需要再乘一个一一，一的一个单位阵，把它乘出来，然后这个R才是我想要的这个旋转矩阵。

然后这个结果呢就是推导的话，可能又会有一些复杂的东西，这个就大家去看这个对应的这个这个链接吧，所以我们最后可以总结一下，形状匹配的这个算法，他说的是其实是对于任意的一个变化，对我们没有假定说这个变换。

它是一定是个线性变换，我可以说一个任意的变化，甚至是比如说在这个图上画，它就是两个图形，我只不过给他找了对应的点，我都可以对它进行匹配，匹配的方法就是我先去计算这些点，对于他各自之心的偏移。

然后把这个偏移组装成一个矩阵H，然后对H进行这个特征值分解，得到H等于U西格玛BT，然后呢，里面的这个U和VT，就可以用来组装这个最佳的旋转矩阵R，然后呢拿到旋转R，然后我就可以得到这个最佳的相对TT。

所以有了R和T，我就可以把两个图片或两个形状，给它对齐到一起，这个就是形状匹配算法，这个算法在图形学里面有非常多的应用，不管是在模拟里面也好，还是在几何处理里面也好，甚至在图片里面都会有应用。

就是大家呃应该之后就是可以碰到，然后我们作业里面会有一道题，就是让大家去实现一下，这个是matching这个算法，然后去看一下它的一个在碰撞处理中的应用，OK关于这部分大家有什么问题，这中间的这个推导。

如果就是太大家觉得有点难的话，就是呃没有没有关系，就是大家可以把这个结论给他记下来，这个算法流程还是一个非常经典的流程，是，OK没有问题的话，我们接下来再看另外一个例子。

就是使用这个特征值分解来做这个压缩的例子，那么使用特征值分解它做压缩的原理是什么呢，是说对于任意的一个图像，一个M乘N的图像，我都可以把它看成是一个大型的一个稠密矩阵，对吧，比如说我如果是啊。

五百五百一十二乘以512，512像素的图像，可以看成是一个512乘以，512的一个矩阵，然后这个矩阵我就可以直接做SD分解，然后呢这个SP分解我可以换一种写法，把它写成下面这种形式。

下面这个形式里面每加的一项其实是对应的，把这个西格玛这个矩阵的他的第I行，它的第I个对角项拿出来，然后对应U的第I列和V转置的第I行对吧，把它相当于把这个分块矩阵乘法写开，写成下面这个形式。

然后下面这个形式呢，它是一个很多个矩阵相加的形式，加上这些每一个矩阵，就这个UI乘以vi转置，这个矩阵都是对应一个值为一的矩阵，对吧，因为我是两个向量叉乘出来的，它是一个值为一的矩阵。

然后同时它对前面有个系数西格玛I，那这个结果就是说一个A矩阵可以写成这么些，这么些个矩阵连加的形式，那么我们对它进行压缩的时候呢，其实是干一件什么事呢，就是把这个求和给它截断。

我们只保留前面几个矩阵的求和，但是后面后面的这些很多矩阵的求和，就给它扔掉了，为什么可以这么干呢，就是我们可以看一个结果，就比如说对于这样一张512，乘以512的图像。

我们可以去看它对应的这个特征值的分布对吧，我们可以把它所有512个特征值，全求出来，然后去看它这个特征值的大小，你就会发现大部分的特征值，就是最开始的那几个特征值是很明显的，是很大的。

但是这个特征值会立马衰减，衰减到这个地方，你看十的零次方就是一衰减到一这个规模，然后再进一步抓线，再进一步衰减，它会衰减的越来越小，这个代表什么含义呢，就是说前几个特征值最大的那几个特征值。

其实它就占了整个图片很重要的一个比重啊，后面那些小的特征值，因为它那个前面那个系数很小，所以他就没有那么重要了，那于是呢我如果想去压缩这个图像的话，我其实只需要在特征值这个地方，给他来一个阶段对吧。

比如说在对于这个图来说，我可以看大概什么几十的这个位置，给他来一个阶段，后面这些值，因为我知道它的值很小，所以对于前面这个求和来说，它的影响就很小，所以呢我只需要保留，前面几个和西格玛比较大的情况。

就可以对这个图片进行压缩了啊，这个标题可能答的有问题啊，这个不是特征值，这个是奇异值，这个我之后改一下，啊这个地方也写错了，这应该是奇异值额，那么我们可以看一下这个压缩的结果，假设我们只保留一个奇异值。

也就是这个图里面最大的这个值，只保留一个的话，那么整个图片呢它就变成这样，一个一个一个样子，这个样子它对应的就是一个秩为一的矩阵对吧，你可以看到它里面有很多的横竖，他其实对应的就是一个质唯一的一个矩阵。

那这个时候基本上图片你是看不出来，他原来的那个样子对吧，这是因为我们压缩太过了，但是如果我们把这个数字，比如说从一加到四，那这个时候你就能大概看出来，这个地方有一个人形了对吧，这个时候呢。

整个图片它是由四个是唯一的矩阵加起来，那如果我把这个特征值的个数，进一步把这个棋子的个数进一步往上加，加到比如16，这个时候这个人就已经比较清晰了，但是你可以看到中间的很多细节都是糊的。

这个时候我使用的这个内存，其实只占到了我之前图片的3。15%，为什么会少呢，因为对原来的图片，我只需要存512×512，这么多个像素的值，但是现在呢我只需要存16个值为一的矩阵，每一个值为一的矩阵。

我只需要存512×2，这么多个像素就可以对吧，然后一共16个，所以它的压缩率就是百分之百能到3%，但这个时候这个图片呢，你就能大概看出来是个什么样的图片，如果你进一步，比如说我用十六十四个特征值。

64个奇异值的话，这时候我使用压缩之后的这个图片，大小是原来的啊，12 12。5%，然后这个时候呢，你可以看到这个人像就已经很清晰了，但是这个图片呢在这些啊光滑的地方，它有很大的很多的这些噪点。

这些噪点就是我丢掉的那些呃，特征值里面表达的东西，OK那么这样一个压缩过程，其实最近呢就是关于这个什么大语言模型啊，还有之类的一些东西的，它其实非常有用的，这里面有涉及到一个非常重要的技术。

叫laura laurrank adaptation of large language models，他说了一件什么事情呢，就是对于一个大语言模型，它里面有很多很多的参数。

如果我想find to这些参数的话，我就要去改这些参数嘛，比如说我中间有一个很大的一个线性层，一个W而且D乘以K的很大的一个矩阵，那么这个矩阵我如果直接想改它的话，那我是不是得。

就是把整个大语言模型给他存下来，然后去改，这是我们每一次跑一遍FORWORD，都需要去ford这个东西，然后反向传播也是一样的，那个时候整个训练的成本就很高了，那么LAURA干件什么事呢。

就是说我在改的时候呢，我其实只需要改增量对吧，我就是看我啊find to的时候呢，改这个W正呃，相对它改了多少，然后相对它的部分呢，我就可以一个类似于这个，我们刚才介绍这个SBD压缩的一个形式。

我用两个字很小的矩阵就置为R的矩阵，这个R是很小的一个值，两个就很小的人给他乘出来，它可以成为一个形状跟W1样的矩阵，然后把这个矩阵跟W加起来，作为我最后的结果，那么这个时候呢我最后多出来的。

我需要训练的这这呃这些参数呢，它相对于原来的W矩阵，就是一个小规模的东西对吧，就是下面这个压缩率这个表达的东西，就是我可以用，比如说我可以用类似于什么10%，这样的参数个数，然后去翻译成整个大的模型。

这个时候我训练的经济性就会很高了，但是同时呢因为我们知道这个W矩阵本身，它包含的信息，就可以利用SVD分解进行一个压缩，所以呢这个信息呢他也没有丢失太多对吧，所以利用SD分解。

你就可以构造像LAURA这样的方法去帮助你，fighting大模型对吧，当然劳尔只是一个这个地方介绍，只是一个劳尔的一个基本思想，他自己也有很多其他的这些技术细节，其实就是大家去啊。

如果对这个东西感兴趣的话，可以进一步了解，呃然后SPD，我们刚才就是介绍了一堆这个SBD对吧，那么这里面涉及到一个问题，就是说SVD它其实是一个可微运算，也就是说如果我把A矩阵做了SD分解。

那我怎么知道这个分解之后的结果，跟A之间的导数关系呢，也就是这个西格玛矩阵对A矩阵的导数是多少，如果我把这个SVD作为神经网络的一个模块，那么在优化需要反向传播，对不对。

那么我已知这个A矩阵的这个这个偏差，德尔塔A是多少，在反向传播的时候，我怎么去推这个西格玛和R它的偏差是多少，所以在这些情况下，你可能是需要知道这个SBD的导数是多少的，那么呢这个问题本身它呃啊。

也不是特别简单对吧，我们这地方啊给一个结果，就是关于这个啊奇异值的导数啊，不好意思，就是这个PPT里面，后面所有的特征值应该是都错了，应该是奇异值啊，不好意思呃，比如说这地方给个例子。

我们就给大家看一下，说如果我知道一个矩阵F，它的奇异值分解是U西格玛VT，我怎么求他的这些奇异值，关于F本身的导数怎么来求呢，我就可以利用这个，F本身呢它一个一个德尔塔就是它的一个围绕。

或者说就是它的一个微分，我们看这个德尔塔F它等于多少，那么利用矩阵求导的链式法则，你可以把它拆开成这三项，然后呢，这三项里面你可以把德尔塔西格玛拿出来，这个东西是我们想要的对吧。

就是你在这个等式左边乘以一个，右转至右边乘一个V，那么就可以得出来，德尔塔西格玛等于后面这个东西，然后后面这个东西呢它里面有一点，你会发现这个U转置乘以德尔塔U，以及V转置乘以德尔塔V是两个反对称矩阵。

为什么这个东西，其实在上一节给大家讲这个旋转的求导的时候，提到了这个事情对吧，就是旋转求导里面，如果你有啊U是一个正交矩阵的话，那么U转置乘以U等于一个单位，你把这个等式左右两边同时求微分。

那么就可以推出来，U转置乘以德塔U其实它是一个反对称矩阵，那么反对称矩阵的对角线其实是零对吧，因为反对称矩阵的对角线by by definition，根据定义它就是零，所以上面这个式子。

因为我知道西格玛是一个对称，而西格玛是个对角矩阵，那我只需要两边同时求一个它的对角项，就可以得到这个啊，西格玛的德尔塔，西格玛等于后面这个东西，它的一个对角线的部分，那么这个等式告诉我们。

其实就是这个西格玛和F之间的导数关系，OK吧这个就是给大家做一个示例，就是如果你后面需要这个sigma，关于F的导数的话，可以看看这个推法，OK那关于歧义是分解的这些呃，这个理论部分我们就讲到这里。

然后关于实现的话，一般来说SBD算法是一个比较耗时的算法，这个它是跟这个矩阵规模是很密切相关的，然后好在呢SD现有的这些实现是比较齐全的，然后比如说如果你需要做小矩阵的SVD分解。

比如说是一个3×3的矩阵，或者是2×2的矩阵的话，有非常好的这些加速算法是这个指令集，指令集及优化的，这个SPD，SPD算法，能够很快的帮助你做这个33矩阵的CD分解，然后对于大规模的稀疏的矩阵的话。

像这些线性代数库，基本上都会提供给你这样的接口来优化，来来实现这个优化好的SPD算法，所以就是大家大部分时间就只需要调包，就可以了，基本上算法都有很好的实现，但是一定要注意SP本身的效率问题。

它是比较耗时的，OK这就是关于SVD部分的介绍，关于这部分大家有什么问题吗，这些内容本科讲线性代数，确实，因为我们这课是这个基于本科线性代数之上的，所以就是还是得大家先把这个本科的线性代数，学明白了。

然后再来看这个课程，因为那个东西是基础，张量分解跟这个东西是不是有点像，张量分解啥张量分解跟哪个东西有点像，给SPT分解，我可能没太懂你的意思，张量分解，张量分解是啥，关于张量的话。

就是大家用到的时候就可以再去学这个东西，对graphic来说就啊不是特别的那个必要，就是当有些有些东西特特定的领域很很有用，但是不是所有东西都需要从张亮那个角度去做，有教材推荐吗。

呃线性代数的教材就是最好的，应该就是那个MIT的那个课吧，MIP的那个课应该是应该是公认大家最好的，线性代数的这个课，然后他那个对应的有讲义，如果你觉得课上的东西不是人会懂的话，可以从那个地方开始补起。

对那个老爷子编的那个线性代数，那个课讲的非常好，讲的时候我都懂，讲完就忘了没事，这个东西就是今天讲了也不肯定不能期望，大概就是一下子全懂了，就是之后如果碰到的话，能用就可以。

就是比如说你知道这东西大概呃，跟什么东西有关，然后就是你可以自己去查相应的资料，或者看一下这个现在这个课程录播啥的，就是能够重新捡起来就可以，毕竟数学这个东西就是对于讲某些概念的时候，它是非常必须的。

比如说如果后面大家如果上图形学的课程，用到了这个谁matching算法的话，那么SBD这一块就是必须的，但是如果你用不到那些东西的话，那他就不是必须的，所以这个课本身它也不是一个，虽然不是一个必修课程。

不是说你非得学完零一才能去看101，而是说就是呃，如果你发现看到后面的课程里面，有哪一块的数学不是特别清楚的话，你可以回头来看一下啊，这个课把它作为一个一个一个资料，然后去看他，然后自己也可以有方法。

就是有目标去找一些其他的教程来看，OK那关于SVD我们就讲到这里，然后接下来给大家介绍一个这个呃主成分分析，pca principal component analysis部分的东西。

这个东西它跟SBD其实是密切相关的对吧，所以很多推导呢如果大家熟悉SBD的，就是就基本上差不多，那么SBPCA这个东西，它是用来做这个降维表达的，什么叫降维表达呢，就是我有一个呃，比如说这个图里面画的。

我有一个二维的一个一些数据，就是每个点每个点它都是一个二维的点，它代表一个数据点，那我有这样一堆的一个二维的数据集，我其实可以通过降维表达，把它降维成一个一维的数据，怎么降维呢，就是我给他找一条线。

比如说对于这个图里面，我找一条这个倾斜的这个直线，然后把所有的这个点投影到这个直线上，那么在这个直线，这个这些点在这些直线上的分布，其实就很大程度上，能够反映这个数据集本身的分布，对吧。

但是原来这个数据集是二维的，我现在给它投入到这个直线上，它就变成了一维的数据，所以这个就是数据降维，数据降维能够帮助我们用更小的数据维度，去理解高维的数据集，对吧，那这里面关键自然就是。

我如何去找这个投影的方向对吧，我们可以一点点来看，比如说假设我有一个二维的数据集，N维数据集，我假设只用一个点去近似它，那么自然这个点就应该是，就是这个数据集的一个平均点对吧，这个数据集的一个平均值。

我把它记为M就是这个数据集本身的零阶近似，那么这个数据的一阶近似是什么呢，一阶近似就是跟刚才图里画的一样，就是我找一条直线，然后把整个数据投影到这条直线上去，看这个数据集在这个直线上的分布。

那么就是这个数据集的一阶近似，那么为了找这个一阶近似呢，我们就要去最小化这个直线啊，去找这个直线的方法方向，使得我的数据投影到这个直线上，能够最小化它跟这个直线的距离，那么这样的话这个点啊。

这个这个方向才能最好的代表这个数据集对吧，那么我可以推导一下，如果我定义了一个任意一条直线，它经过M这个点，并且它的方向是V，那么可以写成这样一条一个一个直线对吧，X等于M加上tb。

然后呢我把所有的数据点投影到这个直线上，那就把这个啊把这个点xi减去直线上的M点，然后再点乘B，这个是之前讲计算几何的时候会讲到，那么可以求到这个点在垂直方向，距离这个直线的距离。

这应该是XI减去本身模长的平方，减去投影的平方对吧，这是勾股定理呃，那么我去最小化这些点距离这个直线的距离，因为xi减去M还是一个跟直线的方向，没有关系的东西，所以呢我最小化DI的平方。

也就是最大化这个xi转置xi1撇的平方对吧，那么这个XY1撇呢，利用这个投影关系，可以把它写成下面这样一个形式，这个形式中间的这个矩阵，我们就称之为方差矩阵S，也就是说，如果我要找一个直线方向去最小化。

所有数据点在这个直线方向上的距离，勾引到这个直线的距离，那么转而我应该是去最大化这个结果，最大化这条这些数据点在这条直线上的方差，那么为什么有这样的结果，大家可以看下面这个图里看，就是对这样一个数据集。

我如果找了一个方向，是这样一个方向，那么你把所有点投影到这个直线方向去的话，他这些点就会分布的比较集中，如果我找的是一个绿色的这个方向的话，那么这个方向显然是能够，更好的代表这个数据集的。

那么对应的结果就是，这些点在这个直线上的分布是比较分散的对吧，所以说你去找一个方向，最小化这个variance最小化这个距离，其实也就是最大化，这些点投影到这个直线之后的variance。

那么最大化这个结果最大化，V转置乘以SVS真定成这个样子，怎么最大化呢，我们可以看S的啊，这个特征值分解对吧，S是一个半正定矩阵，我可以把它写成Q乘以拉姆达，乘以Q转置的形式，它写成后面这个形式。

然后同时呢，我可以把认为这个拉姆达，它有一个大小的培训关系，拉姆达零是最大的，然后拉姆达N减一是最小的，然后因为S本身是半正定的，所以他所有的拉姆达都大于等于零，对吧，那我要找一个方向。

V转置乘以SV最大，那么把S的这个呃，特征值分解的这个形式代入进去，你可以写成后面这个样子，那后面这个样子呢把Q转置V写到一起，你就会发现这个目标他就是去最大化一个。

Q转置V这个向量的转置乘以一个对角矩阵，再乘以这个向量本身，那么这个结果什么时候能取到最大呢，那自然是这个V，Q转置乘以V等于1000的时候，那么这个时候V转至后面，这个结果能够取到最大值，拉姆达零。

对吧，因为拉姆达本身是一个对角矩阵，然后它的最大值是拉姆达里，最小值是拉姆达N减一，所以如果你两边乘以一个向量，两边乘以一个任意一个向量，Q转至B要转置，然后右边的乘一个Q转置V的话，它取到最大的时候。

应该是这个Q转置V，能够撑到这个最大的这个人值上，也就是让Q转置B等于1000的时候，那么Q转至V等于1000，对应的是什么呢，对应的也就是V等于Q0，也是V是Q矩阵的第一列。

也就是这个S矩阵对应特征值是兰姆达零的，特征向量，这个时候能够得到S的微转支撑，以SB是最大的，所以说如果我要找一个主方向，作为整个数据集的一阶近似，那么这个主方向就应该是S矩阵的最大。

特征值的对应的那个特征向量的方向，好吧，那我们找到一阶的近似之后呢，我们可以继续下去，比如说如果要找到啊，两个方向，也就是把它投影到一个平面上，把一个N维的数据集降到二维的话。

那我们就还需要确定剩下的一个分量，那么这个分量怎么确定呢，你就可以找这个这个这些数据点，在投影完第一个分量之后，还剩下的值，继续再把它投影到另外一个分量P上去，啊不对不对，说错了，应该是P是我的啊。

P是我的第一个主分量，说错了，不好意思，P是我的，刚才求出来的这个第一个主分量，TP等于S的最大特征值和特征，最大值对应的特征向量，然后呢把XI减去M他已经投影到P上的部分，减去考虑剩下的部分。

然后对于剩下的部分去找一个，找一个主分量出来，继续把剩下的部分找一个一阶近似出来，那么这就是一个类似于刚才那个东西的过程，刚才求一阶一阶近似的过程，然后你可以求一下这个矩阵，SI两撇乘以XI两撇的转置。

这个矩阵它变成什么样子，它会变成S减去拉姆达零乘以P乘以P转置，为什么是这个结果呢，给大家尝试一下，就是利用这个啊，P是S矩阵对应的最大的特征值的特征向量，利用这个性质。

把这个xi两撇乘以XY两撇转至这个东西展开，你就可以得到这个结果，那么你想换成这个结果之后呢，因为S本身它是写成Q乘以拉姆达Q转置，它可以写成后面这个结果对吧，然后你把拉姆达零乘以P乘以P转置。

也就是拉姆达零乘以Q0，Q0转置这一项拿掉之后，剩下的再有一项最大的，那其实就是后面这个结果了，就是他第二大的特征值了，对吧，所以说在你找完一阶分量近似之后，再找一个方向作为他的第二个分量。

作为它的二阶近似的时候呢，那这个时候你找的第二个分量，这应该是它对应第二大特征值的特征向量，那于是呢这个过程可以一直继续下去，总结起来CCA算法是一个什么样的流程呢，是说对于由X零X1直到XK减一。

这K个N维的数据点组成的数据集，然后如果我要找一个他的MV的近似，那我就找他的这个S矩阵，S矩阵定义成这个样子，它的前M个特征向量Q零一直到QM减一，也就是前M个前M大的特征值。

对应的特征向量Q0到QM减一，这个方向相互正交，当成了MV的子空间，这个MV的子空间，如果你把这个数据集投影到这个MV的子空间上，那么这个MV子空间里面的东西，就构成了这个数据集的地位表达。

这个就是PCA算法，对吧，那它有哪些应用呢，比如说它可以用来求这个点云的反向，比如说如果有任意的一个点云，它本身是没有法向信息的，然后我们取表面上一个点，我要怎么知道它的法向信息呢。

那我就可以把这个点周围的这些点拿出来，把它作为一个数据集，作为一个三维的数据集，然后去找他的第一个主分量和第二个主分量，那么这两个主分量它应该对应的是什么，对应的是我这个三维数据集。

它的一个二维的一个特征对吧，那么这个点云它表示的几何体，它的二维特征自然就应该是这个这个面的，这个点的法，这个表面的信息对吧，因为表面是一个二维的东西，所以如果你把这个点周围的这些点拿出来。

做PCA的话，那么得到的前两个方向，应该就是这个表面的两个切线方向，那么你拿到切线方向之后，再给他做一下叉乘，就可以得到这个表面的法向，法向方向，所以PC可以直接用来求点云的法向，但是这个地方注意的是。

就是这些只能告诉你一个无符号的方向，就是没有办法告诉你，这个法向应该朝外还是朝内，那么这个时候呢去确定这个法向的正负号，就需要别的算法，需要你去别的算法去判断，说这个这个方向应该朝哪个方向才是朝向外的。

好吧，我再看一集，再看一个例子，就是一个I跟face的例子，I can face，是一个早期的，一个做人脸识别的项目，就比如说我们这里有400张人脸的数据对吧，那么对应的就是我有400张。

112×92的图片，所以还是一个相对来说比较大的一个东西，我可以每一张图片我认为它是一个数据点，那么每一个数据点，它对应的就是112×92为，所以是一个非常大的一个空间，一个空间。

那么对应的是一个400个，112×92维的数据点，这么多个数据点我可以做做对他进行做PCA，然后呢我可以提取前40个主分量，PC的主分量，那么这40个分量呢，你把它挖出来就会是这个样子对吧。

就是看不出来是什么东西，但是没有关系，如果你懂这么多，400张图片，都给它投影到这个40维的空间里面去，的时候呢，你看它从它们对应的这些结果，你就会发现每一张脸，它其实就都比较像之前的这些图片呢。

当然有些结果是很很奇怪的，比如说中间这些结果能明显看到它是呃，它是有问题的，但是它整体上来说，它跟原数据集的分布是很像的对吧，那么这个东西就是一个原数据集的降维表达，每一个数据点原来是112×92位。

有这么多位，但是后来呢，因为我是把它投影到了一个40位的一，个子空间里面，所以每一个数据点只有40位，是吧，所以我大大降低了这个数据的这个额，数据的这个存存储上的这个开销。

但是呢它整体的分布呢跟原数据就是比较接近，那同时呢在这个模拟里面，也有使用这个PCA的方法，就是这个model reduction的方法，这个方法说的是什么呢。

是说如果我有一个整一个完全的simulation，一个模型的simulation，它这个模型是可能有很多个自由度的，因为你的mesh有很多的节点，所以自由度可能很多，如果你要模拟它的话。

这个它的开销是相对来说比较大的，那我就可以预先跑很多的这些simulation，把每一条simulation当成是一个数据集，当成是一个数据点，然后呢考虑对整个simulation进行这个PCA降维。

降维的好处就是降维之后的好处，就是我不再需要去模拟整个的模型，我只需要去模拟PC降维之后，那个子空间里面的自由度，所以我模拟的自由度会大大降低，但是由于我的PCA是原来数据集的一个近似。

所以我模拟出来的结果呢，又会是像之前模拟的一个结果，所以说这个PC这个东西它是可以用到模拟中去，帮助我做模拟加速的，这个如果大家之后上后面的模拟课程，应该是能看到这个看到这个应用的。

但是呢PCA它也是有明显的局限性的，它的局限性在什么地方，就是我其实假定这个数据，它是有这种线性的分布特征的，那是这个假设，对不对呢，对于大多数的数据来说，其实它都是不对的，比如说左边这个结果。

左边这个结果是一个三维的一个数据集对吧，每个点都是一个三维的数据点，然后它的分布呢长成图中这个样子，你可以看到整个数据集的分布，它其实是一个类似于一个圆形的分布，如果你硬要说的话呢。

它是一个类似于一个一维的数据集对吧，因为你所有点它是分布在一个圆圈上的，但是如果你去给他做PCA的话，你就只能得到一个二维的一个近似对吧，因为这个，如果我认为这些所有数据点，分布在一个圆上的话。

那么自然你的前两个主分项应该就是，这个圆的切线方向，对吧，它并不能很好的去model这个数据，分布在这样一个曲线上的这样一个特征，这个是PC也做不了的，那同时呢比如说还有像我们这种人体的数据。

不管你是从captured也好呢，还是说生成的也好，对人体这样的数据呢，它自然它不会是一个线性程度很高的数据，所以如果我有一个大的人体的数据集，你想用这个PC进行降阶的话，其实它也是比较困难。

但也不是说不能用，只是说它的效果可能没有那么好，那么为了解决这个问题呢，其实是有对应的方法的，其中一个方法，比如这个kernel pc的方法，它是什么意思呢，cannot pc直观上理解。

其实就是把原来的一个数据集，通过某个非线性的变换，映射到另外一个空间里去，比如说图中我其实有两类点，一类是绿色的点，一类是蓝色的点，他们应该是分开的，但是如果你用PCA进行分析的话。

你会发现两个特征是重合在一起的，但是如果你把它做了一次映射之后呢，这两个数据点这两个数据蓝色和绿色的东西，连蓝色和绿色两个部分就能够分开，这是由于我中间做了一次非线性映射导致的，那除此之外呢。

如果你想对一个非线性的数据集做降阶的话，现在更常用的方法就是使用这种啊，encoder decoder的结构，就是我直接上神经网络去做这个非线性的映射，把一个大规模的一个也不是大规模的吧。

一个高维的一个向量，通过这种神经网络层，映射到一个低位的latent code里面去，移植到一个引空间里面去，然后再通过decoder恢复到这个这个高维的，这个数据。

通过这样一个encoder decoder编码器，解码器的结构去做这个数据的降维，那么它的效果呢，自然是要比这个单纯的PC要好很多的，所以PC的局限性也可以通过这些方法来记。

那么这个地方我细节我们就不介绍了，大家有兴趣的话，可以去看对应的一些文献什么之类的，OK这个就是我们今天介绍了，关于SBD和PC的部分的知识，这里列出了一些参考文献，大家有兴趣的话可以去呃。

可以去看一看这个gilbert strange，就是那个MIT的那位老教授，这个后面这个链接，就是他上这个线性代数课程的这个链接，OK啊行，那我们今天就讲到这里，大家关于之前的东西有什么样的问题。



![](img/bb977eb198a22aec81d6d71d60cb6fd1_2.png)