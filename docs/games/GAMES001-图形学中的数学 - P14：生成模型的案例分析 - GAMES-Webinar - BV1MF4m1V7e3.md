# GAMES001-图形学中的数学 - P14：生成模型的案例分析 - GAMES-Webinar - BV1MF4m1V7e3

![](img/816f05c1547221c722334871f47a15fb_0.png)

大家好啊，我们今天要给大家介绍，是这个深度学习当中的一些呃数学知识，那么在之前的课程中呢，我们呃给大家介绍多了很多像这个概率论呀，线性代数啊这些基础性的呃内容，那么随着这个图形学的发展呢。

以及这个深度学习技术的发展，那么后来的很多的这些图形学技术呢，它其实是啊都是深度学习相关的技术，那么深度学习呃，深度学习这个领域也是在不断的发展中的，所以说我们最后一节课来关注。

一些相对来说比较前沿的部分，就是如何去啊，利用数学去为深度学习更好的提供这个建模，那么总的来说，如果我们要去了解深度学习中，可能用到的这些数学知识的话，比如说大家可以去网上找，有一些非常好的教程。

这些教程呢包含了这个里面，可能生学习里面可能用到的数学的方方面面，比如说像这个微积分，线性代数啊，概率论这些知识，那么这些部分呢，其实我们在之前的课程以及课程中，已经跟大家介绍过了。

所以其实今天呢我们的重点也不在这个地方，我们也没有精力说，就是这一节课就是把把大家跟大家从头开始，就是来看一看深度学习中到底有哪些数学，我们不可能做到这种非常全面的介绍对吧。

以及说深度学习其实是一个在飞速发展的社区，比如说这里展示了这个CPR去年统计的这些，他接收的和已发表的，你看到这个增长的势头是非常，所以社会学领域本身也在一个快速的发展。

它所需要的数学工具呢也是这个不断的呃，日新月异的，所以我今天的想法呢，更多的是说，我们就从这个以几个这个案例分析的角度，我们举几个点来给大家看一下，说比如说像概率论这样的一些基础的数学。

它是如何和这些啊，我们今天看到了各种各样的比较fancy的这些啊，大模型的结果，它们之间是如何建立联系的，那但是不可否认的是，这个这两点之间的联系，它这个gap是非常的大的。

比如说我们这里有一张这个命图对吧，我们先告诉大家这个如何计算，1+1=2对吧，如何去做一些基础的运算，然后我们来看看例题，就是一个非常复杂的这样一个啊求这个极限啊，什么各种各样的东西。

求极限积分这些东西对吧，那我们在学习了这些基础的概率论呀，线性代数啊，优化基础这些课程之后，然后去了解这个最新的一些网络前沿，包括像这个transformer啊，NERF也好。

defection model也好，那么这两者之间还是有一个比较大的gap的对吧，这个不可否认，那我们今天呢也不可能就说一节课，就把这个gap给它填上了对吧，所以啊我们今天呢我的一个主要思路，就是说。

大家就是理解一些这个推导的关键部分就好了，这部分呢我都用红色标出来了，那至于其中的其他的这个数学细节的话，大家可以啊，就是下去之后自己再推一推对吧，但是这些而关键的这些证明的呃逻辑啊，然后推导的步骤啊。

我都用红色标出来来帮助大家这个理解，那我们在介绍啊这些东西之前呢，首先要给这个深度学习有一个基本的引入对吧，那我们这节课也不是一个啊，深度学习的入门课程对吧，所以我们就以一些非常难易的观点来看一看。

深度学习是什么东西，比如说我们可以借助这个拟合的想法对吧，我们之前呃在这个差值拟合中有这样一个例子，我们要对一堆散点XY，然后要给他做这个拟合，比如说在图里画，就是一个最简单的一个最小二乘拟合对吧。

我们首先是假设这个Y和X之间满足某个分布，如如果是线性的话，那它就满足一个啊，比如说一次的话就满足这样一个一次方程，其中呢这个方呃，这个一次函数的呃斜率和截距CA1CA2，它是我的待定的系数。

我需要去通过优化啊，这个直线与这个我实际观测到的这些数据之间，的这个误差，这些差距来得到最佳的这个西塔值，我们可以用这样一个简单的模型去理解，深度学习到底是个什么东西。

比如说如果我们不去关呃关注这个神经网络，它内部具体的这个架构的细节，比如说像CNN啊，就是performer啊，还是RN等等，我们就可以把它当成是一个，充满了待定参数的函数FC和X。

然后深度学习的过程就是，我针对我具体的问题去定义一个损失函数，损失函数L，然后优化这个参数theta，那么这个问题就是一个经典的一个，无约束的优化问题对吧，那么对于这样的无约束优化问题。

深度学习一般采用的是随机梯度下降法，去优化它，随机梯度下降法，顾名思义，就是在梯度下降之上加了一点随机性对吧，这个是帮助这个神经网络能够更好的去逃离，这个安检来设计的，那这部分我们也今天不具体展开。

那关键在哪，关键在于说因为你是随机梯度下降，所以我肯定得有梯度对吧，于是我就必须要能够计算这个损失函数，相对于未知参数C它的这样一个梯度，那么有了梯度之后，我就可以跑随机梯度下降算法。

我就可以优化得到最终西塔，那我就学习过程就完成了，所以今天我们啊对，深度学习有这样一个非常难以付的理解，然后我们就可以看一下，说这个当下最火热的，比如说像这个VAE也好。

diffusion model也好，这样的一些网络，他到底去如何设计这个损失函数，来去formulate它的问题，那么这其中涉及到的这个数学知识，主要就是这个概率论的知识，那么生成模型是什么对吧。

比如说我们这里展示了这啊，四个非常呃火热的这个生成模型，大力三是生成图片的，SA是生成视频的，mesh是生成这个呃mesh和这个贴图的啊，SONO是生成这个音乐的对吧。

那么这些生成模型它的效果非常的炫酷，但是如果我们究其本质的话，我们可以用一种概率论的方法去理解，他们到底是在干一件什么事情，怎么理解呢，我们对吧，我们先从已知的哦，也不是已知的。

大家比较熟悉的这个网络开始对吧，我们大家比较熟悉的网络是哪一种呢，一般来说是这种啊，分类器对吧，假设我有很多的这个猫的图片，狗的图片，鸟的图片，然后喂给一个网络，然后让他去学。

然后他就能告诉我说以后我给你一张图片，这个哪些是猫，哪些是狗对吧，那么这样一个过程相当于什么呢，如果我们把图片对吧，我们把图片就是把它写成X，那么一张N乘N的图片呢，它对应就是一个N乘N的这样一个向量。

那么所有的狗的图片呢，它其实对应着一种概率分布，PX这个PX告诉你了，说呃到底有多大概率，这张图片是狗，假设说这个PX是存在的，OK吧，那么我有了这个PS之后呢，我的网络要干件什么事呢。

我的网络就要告诉区别开，这个狗的概率和猫的概率到底区别在什么地方，我知道这地方画条线，然后告诉你说这条线右边就是猫，这条线左边就是狗，那么我们这个分类任务就完成了对吧，这是一个非常啊非常朴素的理解。

那么生成人物要干的是些什么事呢，生成人说，我要使用我的网络，去尽可能的拟合这样一个概率，比如说我拟合一个猫的概率，那假设我的拟合成功了，那我们接下来就可以通过sample这个网络的概率。

这我已经知道概率了，我就可以在这个概率里进行sample，那么sample出来的所有图像，它都应该是猫的图像了对吧，并且由于我这个分布应该来说是个连续的分布，所以我就可以生成一些实际数据集里没有的。

这些猫了，那么这样我的一个生成任务就完成了对吧，所以如果我要生成一个什么东西，那它的关键就是我要去拿网络，去拟合这个东西的概率分布，OK那如果把它理解成这样一个问题的话。

我们自然就会问自己两个更具体的问题，第一个问题是说，假设我的网络能够表达这个概率分布了，那我们怎么去衡量网络表达概率分布，与实际猫的分布之间的差距呢，这地方困难在哪，困难在于说。

我们其实根本不知道这个PX具体长什么样子，我们有的只有从数据集因拿到了一堆sample，这个sample，影视的表达了这个概率分布长什么样子，第二点是说我的网络到底要如何去表达这个啊，这个概率，对吧。

你可以想象说这个X维度是非常高的对吧，我们是如果是一个512×512，甚至都不是一个很高清的图片对吧，512像素乘以512像素，是一个不是那么高清的图片，那么它的X维度就已经非常恐怖了。

并且这个C塔一般来说维度也很高对吧，那我到底要设计一个怎样的函数来去表达这个，我想要的这个概率呢，所以我们自然就会问这两个问题，那我们一个一个来看，首先第一个问题是说，如何去衡量两个分布之间的相似程度。

那么这一个过程，大家一般来说用的都是这个kl散度，它的定义式就可以写在这个地方对吧，PQ是两个分部，我可以写成这样一个积分的形式，那么这两个积分，由于我中间有个乘了一个PX对吧，那么根据概论的基础知识。

我就可以把它写成是相对这个P分布的，一个期望对吧，这个就是kl散度，那么对这个kl散度而言，如果P和Q相等，这两个分布一样，那么自然这地方P除以Q，那就是一对吧，那log1就是零。

所以kl散度就呲呃就就就是零了，那如果P不等于Q的时候，这两分布之间有差距的时候呢，那么可以证明后面这个积分式，它一定是大于零的，这个我们就呃不具体推了，大家可以记住这个结论，就是说如果对kl散度而言。

它是一定一定是个非负的东西，如果P等于Q的时候呢，它就等于零，如果P不等于Q的时候，它就一定大于零，那同时呢kl散度它也它虽然满足这样一个啊，看起来是P和P和Q的这样一个距离，度量的一个东西。

但它其实并不是对称的对吧，你把P和Q交换，那么后面这个积分式自然肯定长的不一样对吧，你后面这个期望在不是对P求了，就是对Q求了，那这两东西就不太应该不太可能相等了对吧，他就不相等，所以kl散度呢。

它不是一个对称的一个距离的度量，但它有个好处是什么呢，好处是说这个kl散度呢，它在度量这个距离的时候，它其实集中度量的是这个P不为零的区域，因为我这地方乘了一个啊PX对吧，如果你在某个地方PX等于零。

那么这个积分式他就没有贡献，我就根本不care那个地方的Q到底长什么样子，那换句话来说，我们可以怎么形象来理解，假设这个P是猫的分布对吧，Q是我一个网络分布，那么去度量二者之间的距离，二者之间的差距。

我其实根本不关心GX等于零的部分，也就是我不关心那些什么猫的啊，那些狗的图片啊，杯子的图片，我只关心那些实际上可能是猫的图片地方，我这个网络到底给我输出了一个什么样的东西。

所以这个是kl散度非常好的一点，那同时呢最好散度它也有一些额，也有一个非常好的一个啊，另外的一个理解，如果把啊我们要如果我们要最小化kl散度，并且假设这个P呢是我的已知分布，Q是我要拟合的分布。

那么这个时候最小化这个最小散度呢，其实也就是最大化后面这个负的这一项对吧，就是负的这个被P取去望的long q，那么最大化这个东西我可以把写成期望的形式，然后呢这个期望就可以写成采样的形式。

那么这个采样呢是后面对后面这个额log q，要求求求和对吧，那我把它放进去变成这个连城，那也就是说如果我最大化它，那其实也就是最大化后面这个连乘，那么这个连乘代表什么东西呢。

这个连乘代表其实也就是我采样了一堆点，从P里面采用了一点xi，然后我去计算每个点上的，这个我的模型的概率Q然后把它乘在一起，我要让这个概率最大，那么这个如果他能够最大的话，那Q和P就最接近了。

那么这个对应着什么，也就对应着这个最大自然估计，最大或最大自然估计是什么意思，也就是我希望我采样的这些点上，我的模型给出的概率是要尽可能高的，这样的话，我的这个呃Q的分布才能跟P的分布是对上的，对吧。

所以说最小化观测与模型之间的kl散度，那其实等价于最大化观测点上的模型概率，也就是这个最大自然估计，所以kl散度有这样一个非常好的一个对应关系，OK那么到这里我们就解决了。

就是说如何去度量两个分布之间的差，距的问题了，那好，那剩下一个问题就是我们到底该如何去表达，这个额模型的概率，也就是如何去构造这个派THETX，那我们首先介绍一个啊，现在相对来说比较啊经典的一个方法了。

就是这个变分自编码器的一个方法，Variational auto encoder，也叫VAE模型，那么VE模型在构造这个网络的这个，概率分布的时候呢，它其实借助了一个已知分布的隐变量Z。

这个Z他遵从的是这样一个高维的，一个高斯分布，一个标准正态分布对吧，均值是零，方差是一，那么这样一个标准正态分布，那写出来公式呢就是下面这个样子对吧，高斯分布这个大家应该也比较熟悉了。

那么这个Z的维数是要远远小于这个，我的这个图片的维度X的，我们借助这样一个隐变量来尝试，为这个额I西塔X来写出一个表达式出来，OK那么到这里我们就需要回顾一下了，因为我们引入了一个额外的一个随机变量。

Z对吧，那我们就要看一下这个Z和X之间，它可以组合出哪些公式呢，那首先就是P和额X和Z之间，它是有个联合概率的对吧，P x z，那么这个联合概率它可以写成条件概率的乘积，对吧，我可以先用PZ。

然后乘以一个x condition z对啊，这样一个呃条件概率，这两个乘起来结果等于总概率对吧，我也可以啊，先有个PX，然后再乘以上这个z ctr上X的概率，然后得到总概率。

然后并且呢对于这样一个联合的概率，我还可以求它对应的这个啊marginal的概率分布，也就是边缘概率分布怎么做呢，比如说对PXZ，如果我对所有的Z求积分或者是求和的话，那么这样一个积分结果。

对应的就是这个X的分布了，那同理对于啊X求积分，则就是Z的这个分布了对吧，从这个下图上，你可以理解为说，我们这里有个呃二维的一个分布，然后你把它在额Z上进行压缩，就是在Z上求这个积分。

那得到就会是S分布，在X上求积分，得到就会是这个Z的分布了，OK我们做这样一个简单的回顾，那好，那我们有了这样一个我们定义的这样一个，已知分布的一个Z之后呢，我们要如何去表达X呢。

我们就可以引入两个条件概率对吧，第一个是给定Z，然后X相对于Z的概率是多大，也就是p x condition z，那么这个概念呢它对应的就是自变分，自编自编码器，一个里面一个叫这个解码器的东西对吧。

也就是哦不不好意思，说错了，解码器的东西，那就是给定一个X，然后去问你应该如何得到X呃，给定一个Z如何去得到X，那同时还有另外一个反向的一个概率，就是condition on x z的概率是多大。

那它对应的是这个啊，encoder编码器，比如说我有了这个X之后，那么这个Z相对于X的概率会是多大，那么变分自编码器它的核心也就是这两个条件，概率，OK那我们假设已经有了啊。

我们我假设我们比如说我们知道如何去表达，这个X相对Z的概率了，那我们其实可以直接写出来，我们想要的这个派西塔X它的具体形式对吧，那他就是一个我们刚才用那个刚才的那个条件，概率啊，什么那些公式往里套对吧。

他应该写成这个样子，那么这个样子里面，PCTXZ是我的网络要表示的概率，PZ呢是我给印的概率，就是一个标准正态分布，这两项我都知道对吧，我们可以改写成后面这个样子。

那我们如果去直接优化这个plus x和PX，接下来这个kl散度，是不是我就可以训练出来一个生成模型，能够模仿这个PX呢，就我们假设这样一个问题，但非常不幸的是，我们不可以，为什么呢。

问题在于这个我们这地方需要对Z求一个均值，求一个期望，那么这个期望有一个什么问题呢，是说我们到底要踩多少个Z，才能把这个期望给他准确的求出来，因为自己是一个标准正态分布。

然后并且呢我们其实是在干点什么事情，我们其实是希望我们的这个X的分布，PX是要被映射到这个PZ的分布上的，对我们有两个方向的映射嘛对吧，一个是encoder，一个是decoder，对我们有两个条件概率。

我们希望这个S和Z之间呢，它有这样一个双向的映射关系，那这这这就代表说，这个Z其实他承担了，描述整个数据集分布的功能，那如果你要采样它的话，那你可以想象出我们需要非常非常多的采样。

才能求出来这个均值对吧，那这个东西它就是一个基本上不可算的东西，所以就算数额是个数学形式上给出来很漂亮，就是我们只要优化这个kl散度就可以了，但是呢他实际是不可操作的，所以说变变分自编码器。

为什么要还要引入另外一个方向的条件，概率就是因为我们其实希望我们对一个X，我们希望他能够找到相对应的Z，大概是什么东西，这样的话我们如果在涉及到采样的时候呢，我们就可以减少我们的采样个数。

就不再需要对整个全空间进行采样了，这样的话能够帮助我们的这个训练，那么这个东西就是借助这个编码器，encoder来实现的，那么在数学上，这个变分自编码器到底是如何干这件事情的呢。

我们可以用两个神经网络来分别表述，表示我们这两个方向的条件概率，PC塔XZ是decoder的网络，它描述的是我给另一个Z之后，那么这个对应的这个X的概率是长什么样子呢，可以写成下面这样一个形式。

它可以写成一个高斯分布，这个高斯分布的均值是我这个decoder解码器，这个网络的输出，换句话说，这个decoder，它是一个输入一个低维的一个向量Z，然后输出一个非常高维的向量X。

他作为我这个啊条件概率的均值，然后方差是有我给定的，比如说我们就取一这部分对应的是这个解码器，那编码器呢告诉我说我给定一个X，那么这个Z它跟X之间的，应该是一个什么关系呢，它应该也是一个呃正态分布。

然后呢，这个正态分布的均值和方差，都可以由网络来输出，都可以用这个编码器来输出，什么意思，就是我这个编码器干的事情，就是我会给你一个很长维度的X，然后你给我输出两个东西，一个是这个分布的均值。

一个是这个分布的方差，然后呢我用这个东西我就可以去采样自己了嘶，那么这个东西描述的是一个我给定个X之后，我怎么去得到Z对吧，那好，那我们现在就有两条路径，来表示X和Z的联合分布对吧。

我们首先PXZ这个分布呢，可以写成我给定Z的分布，这个Z我是已知的，它是一个啊，我给定了一个值，就是给定了一个分布，比如说就是一个标准正态分布，然后PCTX肯定是上Z，就是一个网络表示的正态分布。

然后同时我还可以反着来，就是我先有PX，这个是我的数据集分布对吧，我虽然不知道它具体形式是什么，但是呢他反正我可以用采样来表示它，然后再乘上一个encoder的分布，q five啊。

最终也可以得到这个S和Z的联合分布，那好，我们现在有两个方向来得到同一个联合分布，那我们的任务是什么，我们就可以去优化这两种表示方法，它们之间的这个cal divergence调散度了，对吧。

那如果这个飘散度能够优化成功的话，那代表了我有X和Z的双向映射关系了对吧，那接下来我要得到PX怎么办，我就可以用这个上面这条路对吧，我就可以三破一个Z，然后再用这个decoder这个网络的概率分布。

我就可以拿到X的分布了对吧，这就非常好了，OK那我们下面就来看一下这个kl散度，它可以拆开写成什么样子，叫我们直接按定义拆开，然后呢，这两个这两个Q这个地方，我可以用这个条件概率给他展开对吧。

展开之后呢，我可以把它进行整理，怎么整理呢，就是所有涉及到PX的地方对吧，PX乘以一坨东西的一个积分，它可以写成相对于这个P这个分布的期望对吧，然后呢以及后面这个log项。

其中我也把这个PX给它单独拿出来，然后把QYPXZ这东西扔到后面去，那么这样的话，上面这个式子就可以拆成下面这两项，那么这两项我们可以注意到什么，对于第一项而言，我们后面有一个期望是。

Z相对于Z在这个q five里面采样，然后求log p x的均值，那么自然这个log p x它跟自己没有任何关系，对不对，所以这个均值的结果也就是log p x，那好那我就变成了X在P里面采样。

然后求log p x的均值，那这个东西它跟我的网络是没有任何关系的，跟我的参数没有任何关系，跟Z也没有任何关系，所以第一项我是可以直接不要它的对吧，我们最小化它没有意义，它是一个固定的值。

那么对于第二项而言对吧，我们要优化这个东西，前面第一个X在P中采样求均值的操作，我们就可以直接替换为，在数据集里面做均匀采样对吧，我从数据集里面拿到一堆X，那么这个X也就是X在P里面的采样。

那么拿到一个采样之后，比如说我们拿到一个采样点xi之后呢，我就要计算后面这一项，那这后面这一项对应着什么东西，也就是我们的损失函数了，对吧，所以说到这个地方我们推出了这个呃AE。

也就是这个变分变分自编码器啊，变化变换自动变码器，它的这个loss函数到底长什么样子对吧，就是对于数据集中的每个xi，我都要求一个这个L，然后呢，我在这个L上把它的梯度，往这个网络参数里面回传。

求它相对于网络参数的这个梯度，我就可以让网络训练了对吧，那这个形式呢，它现在看起来还是一个比较啊模糊的形式对吧，我们并不知道它具体代表什么含义，那么非常非常巧的是，这个东西它其实有一个非常直观的理解。

也就是这个证据下界的一个理解方式，Evidence lower bound，他怎么理解呢，比如说我们把这个式子写到上面，然后我们去考察下面这个资料过程对吧，我们从log p x开始推起这个东西。

可以用这个联合分布写开对吧，写成父母这个样子，然后呢，我也可以在里面同意一个QZ除以一个QZ，把它写成这个样子，然后写成这个样子之后呢，去干点什么事呢，数Z把它单独拿出来对吧。

那我这个积分式呢它其实也就是啊对这个呃，对这个Z相对于做Z这个采样，然后求剩下部分的一个均值对吧，跟之前是一样的，那么这个均值呢它是一定就是你先求均值，然后再取log。

它是一定大于等于先求log再取均值的，为什么这个东西是什么，这就是情商不等式对吧，因为log它是一个凹函数，所以我先做一堆的这个加和取平均的操作，然后再取log，它是一定大于等于。

我先取log再取这个平均的操作的，这个就是琴声不等式对吧，所以说我们证明了什么东西，然后面这个东西你对应上面这个结果，那他们就差一个负号对吧，就是log的上下分母他交换了一下而已，对吧。

那我们就把这个东西定义成这个evidence，lower bound e o b o简写成ELBO，它表示什么含义呢，我们刚才说我们要最小化这个L这个损失函数，其实也就是最大化这个EOBO。

因为他们之间只差一个负号对吧，那么最大化这个东西，其实也就是最大化log p x的下界，因为j log PS始终大于等于这个EOBOELBO，那么最大化这个elbow，其实也就是最大化PX的下界。

那最大化PX的下界，最大化PX其实也就对应着最大似然估计对吧，因为X是我采样值嘛对吧，所以说呃我们在最小化这个loss，其实也就是在做这个最大自然，估计跟我们刚才的那个最开始的理解是一样的。

OK所以这是另外一种理解这个loss函数的方式，还有一种就除了这种之外呢，就是我们刚才最开始这种理解对吧，我们就是求这个XZ联合分布的两种表达方式，之间的这个ko divergence。

OK那我们下面就具体看一看，这个损失函数可以写开成什么样子对吧，我们把它下面这个联合分布用另外一条路径，就是PZ乘以PC塔这个形式写开，然后把两项拆出来，那么注意第一项呢，我就直接给他写成期望的形式。

后面这一项Z啊，在q five里面求期望，然后求期望的东西是log q five除以P，那么这个式子也就是对应着我们最开始的那个，co divergence的一个定义对吧，那我就直接写成这个。

QF与PZ之间的co divergence，所以这个loss可以写开成这两项，我们分别来看这两项代表什么含义，先来看第一项，第一项里面是关键一项就是这个log p c大对吧。

那么PC大我们其实是有定义的，在最开始时候我们说了，说这个PC大其实是个正态分布，它的均值对应的是网络的输出，那于是呢如果我对他取flog，那利用这个高斯分布的那个表达式。

那应该最核心的一项就是这个二分之1XI，减去DC它的平方，然后加上一个常数C，那么这个结果它其实对应着什么呢，啊对着什么xi是我的数据集的这个输入对吧，我的数据集的这个采样DC tz。

我这个encoder输出的结果，那其实也就是我要生成的东西，我要最小化数据集和我生成动机，之间的一个L2距离，那么这个就很好理解对吧，我当然希望我生成结构与数据集足够接近了，所以第一项就干这个事情了。

那好那么第一项这个地方log里面求出来了，剩下还剩下一项，Z需要在q five里面采样来求期望对吧，那么这个地方注意到这个Z的分布，是我们给定的这个编码器的分布，它也是一个正态。

它的这个均值和方差是由网络来输出的，那么与最开始我们在哪个地方介绍呢，与这个地方介绍了，这个自己需要在PZ里面取这个均值不同，这个地方，由于这个PZ相当于是全空间的一个概念。

所以Z要取很多很多的sample，但是在这个地方这个Z的分布是我网络的输出，就我这个encoder的输出，我是需要喂给它一个数据集里面的点xi，然后他给我吐出一个均值一个方差。

然后用这个分布来在里面采用Z的，相当于这个Z和X之间和特定的X之间，就有一个相对来说一个对应关系的，那么这个时候我再去踩Z的话，我就不是在整个空间来踩了，我可以理解成什么。

我可以理解成是在这个数据集点和I附近，去起来这个Z了，那这个时候这个Z需要的采样的个数，就很大大减少了，甚至我们可以怎么办呢，我们把这个theta f不由网络输出，我们就直接给它一个很小的值。

那么这样的话V也是可以训练的，我们当然我们也可以用网络来输出它，但是我们可以认为它输出的结果，应该来说也是比较小的，那么这个时候呢，这个Z他就不需要很多很多的采样，来求第一项的均值了，那么最简单情况下。

我们就可以只取一个Z，只采一个Z，我们可以给定一个xi，给定一个数据集上的点，相当于我们的ground truth，然后由这个编码器吐出一个高斯分布，然后在这个高斯分布里面采一个ZI。

然后再把这个CI代入到这个decoder里面去，得到一张图片出来，然后把这个图片和我的ground truth，算这个l two north，这个就是a loss的第一项。

OK那么这个lost的D项它有一个问题在哪呢，不知道大家看出来没有，就是这个ZI它是要在这个q five，里面进行采样的对吧，代表这个ZI跟QFI是有关系的，那我这个L1求梯度的时候。

那它同时要对Z到对theta求以及对F求，那是L1去求F的梯度，这里面隔着一个ZIDI，它是一个采样出来的东西，那我怎么去反传这个梯度呢，怎么去求这个梯度呢，它其实并不直接对吧。

所以我们这里需要引入一个重参数化的一个，Trick，这个区trick其实也很好理解，就是说，由于我的ZI是在这样一个正态分布里面采的，它的这个具体的这个分布的形式，我不知道。

那么我可以把这个分布变成一个标准正态分布，我可以从标准正态分布里面猜一猜，一个阿西诺I，然后把这个ABCOI经过乘以一个CNAFI，再加上EFEI之后得到一个ZI，那么这个ZI。

它这个结果和我直接在上面，这个相对来说比较复杂的分布里面，采样的结果是一样的，为什么你可以直接理解成，我们其实就是把高斯把这个复杂的高斯变换啊，高斯分布进行了一下，这个平移和压缩的这样一个变换。

把它变成了一个标准正态对吧，那我就只需要在标准状态里面踩一下，然后再把它放放回去，乘以一个值，然后加一个值放回去，就可以得到这个ZI的这个采样结构，和它上面采样结果是一样的。

那这个东西就是一个重参数化的过程，那么写成这个样子之后呢，你就会看到ZI它跟这个EF和西格玛FI，它就是一个算术关系了，而这个ABCI呢它是一个从标准正态中的采样，它跟我的网络参数没有任何关系。

那我就可以啊，就不需要再向他传梯度了，所以通过这种方法，我就可以把这个梯度正确的传到这个参数five，也就是这个encoder的这个分数里面去，这个是lost的第一部分，那么还有第二部分对吧。

第二部分我们是算这个QYZ，抗跌上xi与PZ之间的差距，那么PZ是我已经给定的一个标准正态分布，那第二项，其实我就是希望我最小化这个QY的分布，与这个标准正态之间的差距。

那么这个东西我们可以怎么去理解它呢，前面说了说，我们第一项是说，尽可能的让这个网络的生成结果，和数据集里面的xi要尽可能的像对吧，那么假设没有后面这个部分，它只有第一项的话。

那么这个时候其实整个网络它退化成了，不叫这个variational auto encoder，它其实就叫auto encoder，它是个确定过程，它就没有随机性了，对吧。

那这个时候网络去学这样一个没有后面一项的，这个loss时候呢，它很有可能就是我就只能offer fit到网额，数据集里面那些点，就是数据集里面那些xi呢，我能很好的给你decode出来一个很像的图片。

但是如果你要我生成的话，我就没有办法去生成它了，生成那些数据集里面没有的这些啊这些采样了，那么有了第二项之后呢，由于我我希望这个Z，他尽可能和标准正态之间相，那也就是说Z他的这个西格玛。

他的这个Z分布的这个方差，它不要尽可能的小对吧，他要跟这个N01尽可能的像，那么这个Z他就有了一定的这个随机的能力，有了随机的能力，他就有泛化的能力对吧，就告诉你说。

你这个Z呢不能严格的是从这个xi出来的，它要从X2出来之后呢，我还要再进行它，对它进行一些偏移，这样来保证我的网络能够学到正确的，整个数据集的分布，而不是offer fit到数据集那些点上去是吧。

所以这个是第二项干的事情，那么这个第二项怎么计算它呢，我们可以需要一个定理，就是对于两个正态分布而言对吧，一个均值mu1和方差sim1，一个均值mu2方差sim2，那么这两个高位正态分布之间的。

发的这个care divergence可以有严格的数学表达式的，这个怎么求呢，就是你把高斯分布的那个指数的那个形式，带进那个LL散度的这个定义里面去，然后一通报算算出来，就是后面这个结果。

所以说两个标准正呃，两个高两个正态分布之间呢，它的k o w verges是有这个严格的这个呃，数学的B世界的，那么对于我们的问题而言，q five是一个由神经网络表示的正态分布。

PZ是我给定的这个标准正态，那于是他们两个之间的这个啊kl散度，我就可以直接得到它的这个闭式解，也就是后面这一项对吧，那么这一项也就是我的这个损失函数的第二项，把这两项加起来。

这个东西就是最终这个variational auto encoder，还需要训练的时候的这个损失函数，对吧，那么在训练的时候呢，我就是对于数据集里面的每个c xi，首先经过编码器得到一个均值和一个方差。

这个东西对应到一个高斯分布，然后我在这个高斯分布里面怎样得到一个ZI，然后这个ZI呢经过编码器DC塔得到一个输出，这样我就可以把这个输出拿去算这个loss对吧，然后中间的这些啊EFI呀。

sigma fi也代入进去，可以得到一个loss，那这个loss呢它就可以对这个theta和phi，正确的给它求梯度了对吧，这地方都是显示关系对吧，e theta虽然我不知道具体形式。

但它肯定是一个红素对吧，e five呢一样的，sigma f一样的，就我只需要利用这个链式法则，我就可以把这个L的梯度给它，相L相对于theta和phi的梯度给他算出来，那这时候我就可以开始训练了。

那生成的时候怎么办呢，生成的时候我就不需要这个encoder了，我只需要这个decoder，那我就直接在这个PZ，也就是N01里面随机采样一个Z，然后再经过这个decoder之后呢。

你就可以生成得到这个最终的这个输出结果了，那么这个结果呢，由于我是一个呃随机采样的过程，所以它可以生成一些这个数据基地，没有的，但是呢又是符合数据集分布的一些数据啊，这个时候他就有了这个生存能力了。

那么这里我们就展示了一些，这个VE生成人脸的一些结果，大家可以去想象说这些人脸呢，它其实在这个现实生活中并不存在对吧，但他有个很明显的问题是什么，就是这些人脸都非常的糊对吧。

那么这个弧呢是一个VAE不得不面对的问题，就是由于我中间引入了这个嗯，Z是一个正态分布，我需要对Z进行采样，然后并且我希望这个Z的方差不要太小，所以导致了说这个你生成结果就会有一些糊，那么。

还有另外一个就是最当下大家用的最多的，也是这个效果最好的生成模型，称之为扩散模型，Diffusion model，去看它的生成结果的时候，你会发现它生成结果是非常的这个清晰的对吧，你可以看到生成的人脸。

他是非常的这个拟真的，就是如果你不仔细看，甚至你仔细看的话，你很难去发现这个人脸到底有哪些，有不对的地方，它效果是非常好的，那么扩散模型它可以理解为某一种，就是作为一种理解方法。

可以认为他是一个高级版的这个VAE，他的很多的这个呃这个formulation也好，推导也好，跟VA是相通的，所以我们接下来就看一下这个cos的模型，它到底是怎么去建模这个生成问题的。

怎么去定义它的这个损失函数的，好，那我们首先看我们如何从VAE，把它变到扩散模型呢，在VA里面，我们最终得到的结果其实是，我们构建了一个数据集分布与隐空间分布，PZ之间的一个双向的映射关系对吧。

数据集分布是我未知的，影空间分布是我已知的，然后呢，我用一个encoder，一个decoder构建了两个方向的条件概率，这样我就知道了这个X和Z之间的对应关系，这个是啊这个VAE干的事情。

那么扩散模型呢他其实把这个问题做成了多步，也可以列为色散模型，就是多步的VAE，他引入了一系列的隐变量X0，一直到X大T呃，X呃就已出一，引入了一系列的隐变量，从X0到X大题。

其中的X0对应的是我们这个生成结果，或者是数据集的输入，然后呢剩下的所有这一系列的这些额随机变量，xi x t它都是这个隐变量，都是上面的这个Z，然后呢从X0到XT，它是一个随机性逐步增加的过程。

每一步相当于是都构建了一个，隐式的映射关系啊，构建了一个映射关，双向的映射关系，然后XT和XT减一之间，它们差的就是一个一个噪声多一点，一个噪声少一点，那么通过这种多步的方法扩散模型呢。

它相当于是多个VAE在里面起作用，我们就不再是直接一步，从一个已知的隐空间分布到一个数据集分布，而是我们从一个完全随机的XT出发，然后经过很多很多步之后，才到达这个数据集的分布，X0，由于我做了很多步。

所以呢这个扩散模型，它的这个表达能力要更强一些，那么他这个生成结果也会更好，质量会更高一些，OK那我们接下来就尝试去推导这个生成模型啊，扩散模型的这个数学，数学部分的一些一些一些一些东西了。

那么在开始推导之前呢，就是由于扩散模型，它这个数学是非常复杂的对吧，这个扩散模型的这个这个部分的东西，应该是我们之前呃，呃这个课程里面介绍数学的最难的部分呢，我觉得那么为了把这个事情讲清楚。

就是希望大家就是还是，就是能够把这个大的脉络搞清楚就可以了，然后具体推导可以自己再下去推，那么我们首先做一个约定，就是说在后面所有出现的这些公式里面，Q表示的概率都是我已知的概率，它是不需要我去学的。

它是我定义好的，它可以没有这个具体的数学形式，比如说我是数据集的分布的话，那就是一个采样表示的，可以没有数学形式，或者呢那也可以是我使用一些条件概率，比如说这个贝叶斯公式推出来的对吧。

就不管它的形式复不复杂，但其实它是已知的，就是我一定可以算出来的，或者我可以通过某种方式表达出来的，那我就不需要去绝对部分的概率了，那所有p theta表示的概率呢，由于它下边有个西塔。

它表示都是神经网络表示的概率对吧，这个东西是我们要学的东西，OK那么有了这样一个呃，有一个约定之后，我们接下来就开始推这个扩散模型，做散模型有两个过程，一个是前向过程，一个是逆向过程。

我们先来看前项过程，扩散模型的前向过程，说的是我从一张图片出发对比，从一张狗的图片出发，它是清晰的，没有任何噪声的，然后呢，我对这张这幅图片不断的去增加高斯噪声对吧，就像下面这张图来展示的一样。

我对这个狗呢不断的给他加这些啊纯噪声，然后直到你慢慢认不出来它是一只狗，直到它变成一个完全的纯噪声，最后一张图表示就是一个完全的一个啊，标准正态分布了，那么这中间的哪一步是怎么做的呢，DT步的这个结果。

XT它只与你上一步加造的这个结果有关，如果你已经到了到，比如说到了这个T减一这一步，你有一张加了一些噪声的这个狗图片了，那么你怎么继续加噪呢，这个过程是一个啊高斯分布，它长成这个样子。

也就是说如果我给定T减一，然后XT的分布呢对应一个高斯分布，它的均值是根号一减贝塔T乘以XT减一，它的方差的平方是贝塔T，贝塔T是我每一步给定的常数，我们可以认为这个贝塔T呢，一般给是远远小于一的。

就是我们每一步加的这个噪声，它都是一个啊很小的噪声，因为你如果贝塔T等于零的话对吧，贝塔T等于零，方差等于零，然后前面就是X减一，就是XT等于XT减一，代表就是我完全没有加噪声对吧。

如果贝塔T等于一的话，那么前面这一项均值就是零，后面这一项就是一，代表就是一个完全高斯的噪声，所以贝塔T在0~1之间呢，并且是一个接近零的东西，零的一个常数它是不需要学的，这代表了我往这个XT减一。

里面加了一个很小的噪声，那么这样一个定义的一个逐步的一个过程，就是我从零开始，每一步都呃，相，每一步相对于上一步都是一个呃，稍微加了一点噪声的高斯分布，那么这个东西，它对应的是一个叫马尔科夫链的过程。

对吧，就是马尔科夫链，它最核心的东西是什么，就是我D替补的结果，它只与XT减一有关，而跟前面的所有结果都没有关系，对吧就是我如果到了，比如说到了这个DT步，我要我要知道DT不。

我其实只需要知道T减一就可以了，而前面T减二，T减三一直到零，这些东西我都可以完全不管它，它跟我最终生成T没有关系，OK吧，所以这个就是马尔科夫链一个很重要的一个啊，他那么这样的过程我们就定义为。

这个马尔科夫链，它啊非常重要特性就是它的无后效性，也就是这个s st值与XT减一有关，OK那么这个马尔科夫链呢我们可以把它写呃，用重参数化给它重新改写一下，怎么改写呢，就是你这个地方不是一个高斯分布吗。

对吧，那么这个高斯分布呢，我可以把它改写成一个标准高森布的形式对吧，就是我们刚才介绍的这个跟VAE里面一样，我们有个重参数化的过程，我们的XT可以写成这个等式关系。

其中呢阿尔法T等于一减贝塔T只是一个啊，只是一个变量的替换而已，关键在这个地方，app on t减一，他是个标准正态，如果你从标准正态里面采一个XNT减一，然后把它代入这个算式里面，得到XT。

那么这个ST，就等价于在上面这个高分布里面去采了对吧，这两东西是等价的，OK那么写成这个形式呢是方便我推导的，我们可以继续把这一步整塞成下面这个形式，对吧，我们X7-1。

还可以继续把它写成X7-2的一个依赖关系，写成这个样子，那么写成这个样子之后呢，我们可以利用一个定理，最近你说的是什么，说的是如果我有两个高斯分布，正态分布，那么并且呢这两个分布是相互独立的。

那我们来两个随机变量X1和X2，然后把它加起来的，得到这个随机变量Y，它其实也满足一个高斯分布，并且这个高斯分布的均值等于两个啊，等于缪一加缪二，方差是根号西格玛一方加西玛二方对吧，那简单来说。

那就是两个独立的高斯分布，加起来算还是一个高斯分布，那对于我们的问题而言意味着什么，你在这个地方我们XT写成这样个样子，后面的X上T减二和X系统T减一，他就是两个独立的高斯变量对吧，高斯分布对吧。

那我们这两个高斯分布对应的随机变量，其实它是可以合在一起变成一个高斯分布的，那就是F系统T减二，前面乘以这一坨加上正统T减一，前面乘以这一坨，他其实可以写为一个随机变量XSM鲍尔T减二。

然后前面乘以这一坨，对吧，我们可以写成这样一个形式，那写成这样一个形式之后呢，那XT就直接与XT减二，建立了这个直接的关系，对吧，XT可以直接由XT减二得到，它唯一加的就是这样一个高斯噪声。

那么这样的过程显然是我们可以去啊，不断进行加取的对吧，我们X7-2还可以展开成X7-3，那么最终的结果是得到什么结论呢，就是XTXT和X0之间，它是可以有这样一个与列的关系的，对吧。

X3上T还是一个标准正态分布，只不过呢他前面乘了这个系数，写成这个样子，其中呢阿尔法t bar等于阿尔法一，一直乘到阿尔法T，那换句话来说，这句话意味着什么呢，就是说我们虽然这个前向过程。

定义的是逐步进行的一个马尔科夫过程，但是呢我们可以直接从X0只加一步噪声，得到任意一部XT的结果，这个性质对于这个前项，就是对于我们后面推导是非常有用的，这也是呃这个呃这个扩散模型。

它前过程中最重要的一个结论，OK好，我们接下来再看反向过程，反向过程说的是什么，说的是我们从完全的噪声XT出发对吧，就是个标准正态的一个分布XT出发，逐步减小噪声，恢复到X0的过程，那么如何恢复呢。

在每一步我们实际上是用神经网络来拟合，这个反向的概率，PC它披萨是XT减一，肯定上XT它和正向过程是正好相反的，并且和这个VAE一样，我们假设这个PC大是一个正态分布。

这个正态分布的均值是由网络来输出的，是我给定这个XT的状态以及T额是第多少步，然后最后一个均值，然后呢，方差西格玛T1般来说是我提前给定的一个值，对吧，我们可以给一个小的一个方差。

OK那么这个就是反向过程，OK那现在前降过程有了反向过程也有了，我们其实就有了什么呢，我们其实就有了隐变量和我们希望的这个，最终数据集的分布之间的一个双向映射关系，这个和VAE是一模一样的对吧。

在越狱中，我们有隐变量和这个X的数据集的分布，然后我们最终优化的是这样一个，elbow的一个损失函数，它写成这个样子对吧，那对于扩散模型而言，我们可以直接替换进去，那么这里面的隐变量Z就不再是一个变量。

而是一系列的变量，就是除了X0之外的所有这些加的噪声的结果，哎都是因变量，就是X一一直到X2T都把它当成因变量，然后最终的这个生成结果X0，它对应就是数据集的分布，也就是上面公式里面这个X。

所以我们只需要进行这样一个变量替换，我们就最终写出了这样一个呃，这样一个扩散模型的，它的损失函数的形式，它本质上和这个VAE没有任何区别，只不过我们的这个已变量呢现在变多了，变成X1直到X2T了是吧。

你可以检查一下，说这个啊公式是不是一一对应的，应该没有问题，OK那我们接下来的任务是什么，就理论上而言，如果你把这个loss给他算清楚了，你就可以跟上面VE一样去训练了对吧。

那么接下来的任务就是去怎么把这个loss，给它展开算清楚，OK那我们首先观察一下这个log的呃，这个lost的形式里面的，后面的这一系列的这个大的概率呢，它是可以用条件概率的公式。

拆开成每一步的概率对吧，可以拆开成上面啊，上面是这个X1肯定上X0，肯定X2肯定上X1，然后呢一直是X大，T肯定X减减一，等价于上面这个Q那P的话呢差不多，只不过方向是反的，我现在X0肯定上X1。

然后X1直接上H2，注意他这个数字越大，代表它的噪声是越多的啊，那最后呢一直到这个X大T减一，肯定上XT哦，PT还是说写错了，应该是XT，然后额以及最后再乘上一个X大T的分布。

这个分布呢就是一个标准正态分布了，所以说你去观察这个结果，你会发现每一项条件概率，其实我们之前都定义过了对吧，对于分子而言，每一项都是一个已知的一个正态分布，分布而言呢。

每一项都是我给定形式的一个标准正态分额，一个正态分布，只不过他的这个均值是有网络输出的，那么棘手的东西在哪，棘手的东西在前面对吧，齐射东西在于前面，我要对X一一直到XT这么多个变量求期望。

那我们你再去采样的话，显然是不可能的对吧，这个采采样空间是非常非常大的，所以我们接下来的任务呢，就是去减小这个期望的采样次数，那我们回顾一下在VAE里面，我们是如何去减小隐变量的采样次数呢。

我们做法是引入编码器，然后呢去找到每个数据集点xi对应的Z，这样的话我们只要在这个自己的附近去采，那么这个方差小，我的磁场次数就死亡次数就小对吧，那么对于后面这个问题而言。

我们是我们可以考虑一个类似的过程是什么呢，我们可以在后面这些概率里面引入它，相对于生成结果X0的概率，然后呢那么这样一个做法，他其实就3号类似于VAE我们的做法对吧，我们就是在构建一个反向的一个概率。

AE说法是我们用encoder，我们用编码器来找这个自己抗定上xi的概率，那么在扩散模型里面，我们为了这个求解的方便，为了这个期望它的采样个数，的采样次数给它降下去，我们把这个后面这些条件概率呢。

我试着给它指定上，让X0X0，也就是我最终生成结果，或者是我数据集的输入对吧，如果我肯定上了这个东西之后，我就希望说我最后这个公式推出来，他这个需要的这个采样次数是少的，那怎么去推呢。

我们可以从这个Q入手对吧，QXT肯定上XT减一，为什么，因为这个XT到XT只与XT减一有关对吧，因为前向过程是一个马尔科夫过程，X系只与X减一有关，那跟X0其实没有关系，所以我们把可以把它直接接上X0。

这两个概率是一样的，然后呢有了这样一个写法之后呢，我就可以用这个贝叶斯公式对吧，把它改写成后面这样一个形式，那么这个形式里面可以注意到，就是所有的每一项概率都都抗击上了昂X0，对吧。

我的每一项概率都相对的是这个X0的概率，那么我期望我这个公式推出来之后呢，那这个variance就足够小了，这是我们期望好吧，那我们去看一看，说如果真的写成这个样子之后呢，他是不是会真的想。

OK那我们任务呢就是把下面这个公式，把这个Q的形式带入到上面去，那么第二到上面试去的时候，有一个技巧，就是说你看到后面这一项，它其实是一个什么XT除以XT减一这个东西，如果你在上面做连乘的时候呢。

它是可以相消的对吧，分子分母一直在那相消的，所以呢后面部分大概率是可以完全被消掉的，那么剩下的部分呢你再把它单独提出来，然后再给它这个稍微总结一下，你就可以得到log里面，最终这一大坨东西。

对应的是下面这一大坨东西对吧，这个没有办法啊，这就是diffusion model，它就是有这么复杂，所以我们的公式写出来就是有这么复杂，这个这个没有办法避免对吧，但是我我已经尽可能让大家能够理解。

这些公式到底是什么含义了，OK那么呃那么好，那我们就是上面这个形式，可以写成下面这样的连乘的形式，那这个连乘的形式呢也因为在log里嘛，那最终我就可以把它放到log y，把它变成求和，OK吧。

然后把它变成求和Z，然后求和写开之后呢，注意到一个事情，就是说比如说像第一项，第一项里面这个东西，由于它只跟X0和X1有关，所以我前面这个所有对X1和XT进行sample，然后取均值的一个操作呢。

其实根本不需要做后面的值，因为它跟X22X3这些值都没有关系，所以我其实只需要SX1相对于X0这个概率，我就可以算这项的期望对吧，那对第二项一样的，由于第二项里面只涉及到XT和X0。

所以我只要求采样一个X大T，我可以得到后面项的均值了，然后呢对于后面这个连加和的形式呢，它里面涉及到X0XT和XT减一，那我们这个需要的这个取均值的操作呢，就只需要针对这个随机变量。

XT减一和XT就可以对吧，对于其他的那些随机变量呢，对他取均值没有意义对吧，因为这个后面等式跟他没有关系，所以我就变成这样一个求和式，然后呢这个求和式第一项我不动。

第二项呢我可以把它写成个kl散度的形式对吧，这个形式是直接定义的，那后面这一项呢它其实也可以写成kl散度，为什么你只需要把前面这一个概率给它，拆开成两项。

第一项是QXT减1condition on xt以及X0，对应的也就是这个log分子上的一个概率，然后再乘以一项QXTCONTE上X0对吧，这两个条件概率的乘积等于最后总概率，那么其中的第一项。

把他的这个期望和后面结合在一起，那其实也就是这个kl散度的一个形式对吧，只不过我还剩下一项，就是我XT还要对X0求均值，所以这个蓝色的式子是可以变到下面，这个蓝色的式子对吧。

大家可以再就是下去仔细推导一下，这个肯定是对的，那么最终的这个L也就是我实际用的这个啊，实际推导出来的这个呃，扩散模型的这个损失函数的形式对吧，它有很多的项，那我们还是一项一项的来看。

那么第一项它是一个从X0，它是一个抗跌上X0的X1的概率，我要在这个东西上求它的均值，后面这一项西theta是一个我给定的，我已知的已知形式的一个正态分布对吧，那么log它是可以直接求的，没问题。

然后呢对这个啊X1取希望，那我只需要随机采一个X1就可以了，为什么这个地方只需要踩一个呢，那就跟VAE里面道理是一样的，因为我这个X1是condition x0的，我的X1只是从X0加了一个很小的噪声。

方差，很小的噪声之后得到的一个结果，所以我要采X1的时候呢，我其实只需要采一个值，就能足够好的去估计这个啊这个均值了对吧，这是第一项，那么第二项呢，注意到它跟这个网络的参数没有任何的关系。

它其实是希望说我这个加造过程加到最后呢，我希望他是跟我的这个XT的实际分布，也就是一个标准正态分布足够接近的，那么这个东西只要你的这个T这个步数，一般来说取得足够多，或者贝塔T取得足够大。

那么这个结果他就是能够满足的，那并且它跟网络没有关系，所以我就可以直接裂项征掉了，那么关键的呢就是下面这一坨东西对吧，那么下面这个东西怎么算呢，哦sorry啊，就下面这套东西。

然后并且呢下面这个东西它是一个求和对吧，是对所有的LT进行求和，然后我们还可以继续把它再简化下去，怎么简化呢，就是在前项过程中，我们是可以直接由X0得到XT的对吧，只是利用我们最开始推出来那个性质。

那么这个求和式呢，它其实就可以被改写成一个期望的形式了，对吧，就是你这个期望是针对什么进行的，针对T进行的，就是我T随机采一个二到T之间的，一个二道大题里面的一个整数，然后算后面这头东西的期望。

它其实就等于什么，等于这个N分之一乘以，后面这坨东西的求和对吧，所以这个求和是可以被替换成一个期望的，那替换成这个期望有什么好处呢，就是我这个我如果随机踩一个T，这个XT的结果。

我是可以直接一步加造得到的，我不需要跑完整个前项的过程，那这就方便了我去做这个训练对吧，那么实际在做的时候呢，我就可以均匀的踩一个T出来，然后只取其中的一项LT拿到这一项，把它作为我最终的概率。

那么在期望意义下，它等价于我求这个最终的这个求和的概率啊，求和的这个呃，去最小化这个求和的这个损失函数对吧，所以不要退到这个怎么损长这个形式，跳到最后呢，我们其实只需要关心这个LT就可以了。

这个是我们最终需要额再进一步关心的东西，那好，那这个东西我们就里面包含了一个kl，kl散度对吧，那我们就看看这个KOS怎么写，一项这个QXT减一横推上XTX0，这个概率它是一个什么概率。

它是一个XT减部XT减一这一步，那同时condition on，这个最终额最开始的这个加造的，这个完全没有加噪的这个图片，也就是数据集的X0，以及他加噪到下一步的XT这两个东西，给定之后。

问XT减一的分布应该长成什么样子，对这是一个很奇怪的分布，但是呢他这个这个分布的具体函数，这是具体函数形式，其实我是可以给出来的，那么怎么给呢，我就用贝叶斯对吧，贝叶斯之后呢。

我就给大家展开成后面这个形式，后面这一项你注意一下，XT减一肯定上XT减，XT肯定上啊，XT减一以及X0，它是其实也就等价于XT肯定上XT减一对吧，因为因为马尔可夫性嘛。

那么这个概率它是一个已知的正态分布对吧，这个是我们之前定义好的一个正态分布，就是单步加造的一个概率分布，然后后面这个QXT减1con提上X0，这个是西是什么，这个就是我加T减一步的噪声之后的结果。

它的概率分布是什么，那其实我们也推过了对吧，就是利用那个啊前沿过程的那个好的性质，我们这一步其实也就对应一个高斯分布，那么底下分母这个分布呢，哎也是一个高斯分布，所以右边的这三项全都是正态分布。

那么你如果实际的把正态分布的形式带进去，然后就是一通报算，你就会发现这个结果它最后出来的，对应左边这个概率呢，它也是一个正态分布，只不过这个正态分布的均值和方差，可能会是一个相对来说比较复杂的形式。

这方我就直接给出来了，这个大家要推的话，可以去尝试一通报算，看看是不是这个结果，可嗯，OK所以说这个K代表真的，第一项是一个我已知的这样一个高斯分布，那么第二项是什么。

第二项是一个我给定形式的网络的高分布对吧，那么这两个之间的ko divergence，我就可以直接用两个高斯分布的ko divergence，去给出显示解了，那么回顾一下啊。

对那么回顾一下这个显示界长什么样子，长成这个样子对吧，那么并且如果我们去只关注这两个高斯分布，它们的均值，而忽略方差的时候，因为方差大概率也不是大概率的，就是我给定了对吧。

在我们方向模型里面处理问题的时候，方差都是我给定的，我从来没有让网络是输出方差，所以我们就只关心均值，那么对于只关心只关于均值的时候呢，这个ko divergence它关键项就是中间这一项。

它其实就是对应着均值的L2距离平方，对吧，就是L2norm l2norm的平方是吧，U1减去UU2模长的平方，所以啊两个正态分布，如果你只关注它的均值的话。

那么他的这个k o w version有一个很好的解释，就是它对吧，那么如果把这个性质直接用到这里面来，那我们就可以把LT简化成什么，后面这个K的vision se我就不要了，我们最终就只要这个啊。

Q的均值和PC大值均值两个之间的差距，这就可以了，Q的均值是一个我知道的东西，而PC大均值是一个网络输出的东西对吧，那这个时候这个形式就非常的简单，那唯一的问题前面又有一个期望，这个期望怎么办呢。

还是一样的，我这是从X0到X1的这样一个条件概率，那么这个条件概率呢，我是可以直接写出来下面这样一个形式了，那我们就可以直接在这个形势下做一次采样，然后作为这个LT的估计值，然后就可以得到后面的这个啊。

然后然后带进去算这个后面这个差是多少，他就可以得到最终的这个损失函数了对吧，那并且呢这个损失函数呢，我是可以进行梯度回传的对吧，ax s on t是一个高斯分布，它跟网络参数没有关系。

那剩下的这个XT呢，它是这个呃带进去对吧，那这个mu我是可以得到的，mu得到之后呢，唯一的问题就是缪theta，这是个网络输出，那我只需要对L对mc ta求梯度就可以了，这个用链式法则就很快得出来了。

因此呢到这里，其实我们就已经可以去训练一个负载模型了，那么训练它呢，如果你去看或者模型的算法的话，你会发现它跟我们这个形式，还是有一点小小的区别的，这个区别就是，我们还要再进行最后一次参数变换。

怎么参数变化呢，就是new我们现在写成是XT和X0的函数对吧，因为它是一个condition，然后XT和X0的条件概率的均值，那我们这个XT呢，它是可以用这个式子展开的对吧，直接写成跟X0的关系。

那我们可以把这个式子里面X0反解出来，用XT来表示，然后把它带入到这个缪的形式里面去消去X0，然后就可以得到mu，相对于XT以及ab希统T的一个关系，这个代表什么含义，这个代表这个XT减一。

现在呢它可以看成是XT减一的均值，它可以看成是XT减去了后面这坨东西，而这个东西对应的是一个纯高斯的一个噪声，那好那我们对于缪theta而言，我们把它改写为下面这个样子。

这个样子其实也就是仿照上面的这个定义，我们把mu theta改写为一个absoon theta的一个函数，我们的网络最终并不是直接输出MUSETA，而是直接输出这个app on theta。

然后把这个ABC用西塔带入到这个形式里面去，得到这个UC塔，然后再把这个MC塔跟上面这个mu进行，这个算这个l to norm，然后得到最终的这个损失函数，那么这么写了之后呢。

它其实只是一个就是我们计算上的一个方便，就是说最终我们要求的l to norm，不再是两个mu之间的l to norm了，可以写成是这个两个ab西隆之间的l to norm，其中的第一个还是系统T。

它是一个纯粹的高斯噪声，它是我散播出来的啊，不送T是我散布出来的，然后呢我三不出来一个X用T之后呢，我可以把它加到X0上，得到这个XT的值，然后把这个XT和T作为网络的输入。

让网络生成出来一个噪声app on西塔，然后我希望这个网络生成的噪声，和app on t是足够接近的，那么这个东西就是这个我们一通计算之后，化简之后得到的这个啊扩散模型，最终的这个损失函数了。

尽管这个模型这个最终的式子看着简单，但你可以想象，就是我们前面经历了这一通艰苦卓绝的推导，对吧，他这个推导形式非常复杂的，但是呢又究其本质，这一通推导出来的loss函数。

和最开始的这个损失函数的形式是等价的，而这个形式是什么，其实也就是一个elbow损失函数的形式，跟VAE里面是一样的对吧，所以扩散模型的出发点其实并不难理解对吧，并不困难，但是呢中间推导很复杂，并且呢。

最终的这个最终得到这个损失函数的形式呢，他反而又化简出来一个很简单的形式，OK吧，那我们这时候就可以完全理解这个啊，这个这个这个扩散模型，它的训练和这个采样的算法了对吧。

训练的时候你从数据集里面拿到一个S0对吧，然后我uniform3回一个T出来对吧，这个对应的是那个求和那个loss求和变成期望，然后呢我生成一个噪声，一个标准正态的噪声很像。

就是对应上面这个公式里面的XTIMT，然后呢我计算这个啊loss对吧，就这个LT，它是这个abs on t与ABF，son theta之间的这个l two norm的平方，然后把它对theta求梯度。

我就可以拿到gradient对吧，拿到gradient之后呢，我就可以把它放进这个啊，所以就有三样算法里面让它去收敛了对吧，所以这个就是他的训练过程，那生成的时候呢，我就从一个完全随机的噪声开始。

然后走上面这个公式对吧，就是我每一次呢随机采呃，让网络生成出来一个噪声，这里面就没有采样了啊对吧，我们先生成下一个噪声，然后带进去，然后得到这个降噪之后的一个均值，得到均值之后呢。

由于我的这个后呃前向过程呃呃不对，后项过程它依然是一个概率分布嘛对吧，所以我还有它是个高斯分布，我只有均值，后面再加上一个方差西格玛T，这个西格玛T是我给定的，通过这样一个操作之后呢。

我们可以得到它降噪一步的XT减一对吧，然后你从大T1直做到一，那最后就可以得到最终的生成的结果X0，这个就是扩散模型的一个整合的过程，OK所以啊到这里我们就从VAE开始讲起，然后一路讲到这个扩散模型。

然后把最终的这个算法给他搞明白了，大家可以看到，这里面其实用到的数学是非常多啊，另外这个概论的知识还是挺多的，推导还是比较复杂的，那么我呃，就是我尽量的把这个红色部分的标出来，给大家理解。

就是看看大家能不能就是大概理解这个推导的，这个关键点，然后呢如果想具体啊计算里面的细节的话，可以自己再去把这些啊其他的部分给他补上，那么扩散模型本身呢，它现在也是一个就是啊，一个蓬勃发展的一个领域对吧。

那么关于他的理解呢，也不仅限于今天我们介绍的，就是基于VAE去理解这个扩散模型对吧，我们还有其实还有另外一些理解方法，比如说基于这个啊朗转换过程S啊，这个随机微分数，随随机微分方程的一些理解理解方法。

那么你换了一种不同的理解方法之后，他就会告诉你一些啊，新的一些这个可以优化它的方法对吧，比如说像这个我们今天讲的这个，叫这个DDPM对吧，我们还有这个后面的改进叫DDIM，还有一些其他的一些呃改进方改。

针对这个扩散模型的改进，他可能不太能从这个，我们今天基于via e的理解出发对吧，他可能需要另外一套，这个另外一套这个随机分方程啊，朗转方程啊，还有这个微分函数啊，这种东西去进行了解。

那么那部分的知识呢可能就啊更为复杂的对吧，那就不再是基础的这个概率论了，它是一个随机微分方程领域内的一些知识了，所以我们今天就更不会展开了对吧，所以我们今天只是用这两个例子去给大家一个。

一个简单的一个一个就是也不是简单吧，就是一个非常小的见微知著的一个，一个一个例子，然后大家可以呃了解到就是前面我们学习的，比如像概率论这样的知识，如何跟后面的这个前沿的部分，它是联系在一起的。

所以学好数学还是非常重要的，OK那我们这就是我今天的啊参考资料了，然后啊这个PPT啊什么之类的也会之后发到网上。



![](img/816f05c1547221c722334871f47a15fb_2.png)