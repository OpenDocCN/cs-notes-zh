# „ÄêÂèåËØ≠Â≠óÂπï+ËµÑÊñô‰∏ãËΩΩ„ÄëCMU 15-721 ÔΩú Êï∞ÊçÆÂ∫ìÁ≥ªÁªüËøõÈò∂(2020¬∑ÂÆåÊï¥Áâà) - P10ÔºöL10- ÊÅ¢Â§çÂçèËÆÆ - ShowMeAI - BV1wv411w7Ko

„ÄÇ

![](img/5bfd10e92f11ecac17826e17d9bc8016_1.png)

üéºAll rightÔºå so„ÄÇToday we're talking about recovery protocols and so there'll be some you know in the introduction class„ÄÇ

 we talk about areasÔºå we talk about how to we want to restore the database after crash and in the AMory database„ÄÇ

 we're going to do the exact same thing right obviously that if the databaseÔºåüòäÔºåYou know„ÄÇ

 if database shuts down or crashesÔºå we want to be able to restart the system and put the database back into the correct state such that we guarantee the asset admissity and durability guarantees it would have in asset system right so aacity means that we can't have any partial transactions„ÄÇ

 consistency means that we don't put the database in like a funky state where like data could be actually incorrect and then durability just means that we write changes to the database we tell committed a transaction„ÄÇ

 The outside world heres that that transaction is committed when we come back then all our changes are still there„ÄÇ

So every recovery algorithm is going to have two parts„ÄÇ

 there's the part we're going to do at runtime where as we process transactions and they update the database„ÄÇ

 we're going to take some extra steps to record some extra information so that if we crash and come back„ÄÇ

 we can look at extra extra information and figure out what was going on in the system to put us back in the correct state„ÄÇ

 and then there's the recovery protocol you actually do after the crash or after a failure„ÄÇ

 where you look at all the information you've collected that you generate doing the normal processing and then you figure out what do I need to do to go back„ÄÇ

So we'll talk a little bit about when you spent time talking about in memorymory database recovery or logging protocols and recovery protocols„ÄÇ

 the paper I had you guys read was not in an memoryory database„ÄÇ

 and I'll explain why I signed it there's actually not a lot not many papers about inmemory database log recovery When you look back at the early ones from the 1980s„ÄÇ

 they all make this big assumption that the log or the database itself will be backed by what is called nonvolunt memory or persistent memory„ÄÇ

Back in the 1980sÔºå this just meant battery back of DRA„ÄÇ

 so you had a little battery on the motherboard that could be triggered anytime you recognize that the power was going to go down and you use the battery to actually gracely shut down the memory„ÄÇ

 write it out the disk„ÄÇ and so when you came backÔºå all your contents or DRAM was still there„ÄÇüò°„ÄÇ

So battery back DRA has been around for a long time„ÄÇ

 it's not widely used because it takes up space on the motherboard„ÄÇ

 they're great until you try to use them and they don't work because you have to make sure the battery is always functioning correctly so you don't see that too often in the while you certainly can't go to Amazon and get an AWS instance that has this„ÄÇ

üò°ÔºåSo the other kind of persistent memory though instead of bang battery back to DRAM is to use what is called true nonvault memory or true persistent memory„ÄÇ

 And so for years and years and yearsÔºå every time I taught this classÔºå I would say„ÄÇ

 oh real nonvault memory is coming out one year from nowÔºå one year from nowÔºå and finally last year„ÄÇ

 Intel stepped up and released true nonvault memory called the opt team and again„ÄÇ

 the idea that goes into a dim slot like you normally would with DRA„ÄÇ

 but this has magic inside of it that saysÔºå you know if if I lose powerÔºå I still maintain everything„ÄÇ

 it's going to be slightly slower than DRAÔºå but it's gonna be way faster than than SSDs„ÄÇ

 So we're not going to talk about and theM stuff or persistent memory right now will cover this at the end of the semester yes„ÄÇ

The question is it really expensive„ÄÇ What's thatÔºå What you say oh„ÄÇ

Yes and no's not like it's not like outlandishÔºå it's like maybe a couple thousand for like 128 gigs or something like that„ÄÇ

 I don't know the exact prices I don't think until publishes them„ÄÇ

 you can maybe altogether get from OEMÔºå but yeah it's more expensive in the DRAM more expensive than SSDs„ÄÇ

 but it has certain properties that you can't get out an SSD„ÄÇ

You can also get it in a form factor that goes down on the PCX„ÄÇ

 so that looks like a flash drive even though it's the special basically it's not NFlash„ÄÇ

 it's not DRAÔºå it's a special storage medium called phase change memory„ÄÇ

 you can get it in other form factors and that one though the prices are close to what SSDs are„ÄÇ

 but for the dim stuff it's more pricey„ÄÇwe'll cover this later in the semester the point I'm have to make is like if you go read the early papers on logging protocols for MMory databases„ÄÇ

 they all assume thisÔºå but back then they didn even have it„ÄÇ

 so we still have to design our protocols using relying on SSDs or spinning disk hard drives„ÄÇ

So the good thing thoughÔºå for us in an M database is that our recovery protocol is going to be slightly easier than what we have to do in a discordant system„ÄÇ

 and this is just due to the fact that since the primary storage location of the database is in memory„ÄÇ

 when we crashÔºå or there's a failureÔºå that memory is wiped„ÄÇSo when we come back„ÄÇ

 all we need to do is load in the last checkpoint and replay the log from disk„ÄÇ

 and we don't have to worry about any dirty pages from transactions that may be lingering around on disk that we had to reverse„ÄÇ

 as we would in a discordient system„ÄÇSo we don't have to track dirty pages„ÄÇRight as we run„ÄÇ

 like if a transaction makes some changes and then it doesn't commit and we crash„ÄÇ

 all those memory pages get wiped„ÄÇ So that's fine„ÄÇ All„ÄÇ

 So this means now we don't have to story in undo records to reverse these things because there's nothing to reverse„ÄÇ

 The only thing we need to write out the disk is just redo information„ÄÇ

The other big difference is going to be that we're not going to log any changes that we make to indexes„ÄÇ

So in a disk based systemÔºå as I update my table and that causes updates to the indexes„ÄÇ

 I'm also going to create log records that record the changes to the index„ÄÇüò°ÔºåBut in my database„ÄÇ

 I don't do any of thatÔºå and so when I crashÔºå I have to load the database back in from the checkpointt file anyway„ÄÇ

 so the disk is way more expensive than doing the computation„ÄÇ

 so I'm just going to rebuild the indexes on the fly as I load the checkpoint in„ÄÇüò°„ÄÇ

And in a dis based systemÔºå you don't do that„ÄÇSo we still have to deal with the fact that dis is gonna to be slow„ÄÇ

 And so we still want to take advantage of all the optimizations we would do in in a discordance system„ÄÇ

 like group commitÔºå batchshing thingsÔºå doing speculative locked releasesÔºå things like that„ÄÇ

 All those optimizations are still going to apply for us hereÔºå even though we're in memory„ÄÇ

So today I'm gonna talk about first about different types of logging schemes„ÄÇ And again„ÄÇ

 the paper you had you guys read was for a disk orient systemÔºå but it's relying on multiverging„ÄÇ

 So I think that's really interesting„ÄÇ and we'll see how that works„ÄÇ

 And then we'll talk about checkpoint protocols„ÄÇ And these two here are what you need for to guarantee your database will actually this is all you really need to guarantee your database is durable„ÄÇ

 This is a way to speed things up after a crash„ÄÇ and this is a way to speed things up if you're going to do a restart and you know that it's gonna be scheduled at a certain time„ÄÇ

 So this one you have to have„ÄÇ This one is an optimization to make this go faster and this is an optimization to avoid having to do all of this okay„ÄÇ

„ÅØ„ÅÑ„ÄÇSo at a high levelÔºå there's two types of logging schemes„ÄÇüò°„ÄÇ

It physical logging and logical loggingÔºå physical logging is like taking a diff as you wouldn't and get„ÄÇ

 the idea is that we're going to record the changes that we're making to a record at the byte level„ÄÇ

üò°ÔºåIf you think of also in NBCCÔºå if you're using a Delta storeÔºå I'm just recording„ÄÇ

 here's the column that I modifiedÔºå and here's the new value„ÄÇRight„ÄÇSo then after a crash„ÄÇ

 or essentially you just going to replay these log records and reapply the changes to the columns„ÄÇ

The other approach is do logical loggingÔºå and this is where instead of storing again the low level bytes were modifying within a tuple„ÄÇ

We're it going to store the high level operation that the application invoked requested on the data system to make that changeÔºü

So it would be like recording the actual SQL statements that they would send us„ÄÇ

 the answer update delete queries„ÄÇSo the idea here is if I have a database„ÄÇ

 but the table has 1000 tuplesÔºå bigÔºå maybe a billion tuples„ÄÇ

If I'm doing physical logging with one query that needs to update all 1 billion tuples„ÄÇ

 then I need 1 billion log records that correspond to every single tu that I've modified„ÄÇ

 but if I'm doing logical loggingÔºå then I only need to record that single update statement and that's enough for my system have to restart to replay that update and put back in the correct state„ÄÇ

üò°ÔºåSo logical logging is going to be much smaller log files and potentially much faster to commit at runtime„ÄÇ

 but the recovery processes could potentially be more expensive because there's no magic I can do to make this update go faster during recovery so if I have to update a billion twoupples and it takes an hour the first time I run it after I crash and restart and replay this update query„ÄÇ

 it might take an hour again„ÄÇSo it might delayed the amount of time it takes for me for my database to come back online„ÄÇ

So for this reasonÔºå those systems are going to choose physical logging„ÄÇ

 and this is only used in certain specialized systems„ÄÇOkay„ÄÇNowÔºå the question is„ÄÇ

 when do we actually flush the log records that a transaction generatesÔºü

So the first approach is to do all at onceÔºå meaning as a transaction runs and they're making changes„ÄÇ

 they're executing queries that update the database„ÄÇ

 doesn't matter whether it's physical logging or logical logging„ÄÇ

 I'm just going to record in memory in this log buffer for my transaction here's all the log records„ÄÇ

 and only when the transaction says go ahead and commit if we pass our validation„ÄÇ

 then we hand out those log records to some our logger thread who then flushes it out to disk„ÄÇ

Of courseÔºå we have to wait until all those law recordss are written to the disk before we can tell the outside world that our transaction committed„ÄÇ

So it could beÔºå you knowÔºå if I've updated a billion2 of a billion wall records at the rate Wait for those 1 billion log records to get rid to disk„ÄÇ

The other approach is do incremental flushing„ÄÇAnd this is as transactions run and they'reccumulating these log records„ÄÇ

 when their local log buffer gets fullÔºå they can hand that off to some writer thread who can then start writing that out the disk and then they get a new log buffer to start filling up with new log records„ÄÇ

 so that means that if there's a crash now in the log„ÄÇ

 there may be log records from transactions that did not commit yet„ÄÇ

Because we're allowing things to get written out before everything is finished„ÄÇ

So in this approach here for all at onceÔºå this makes recovery easier because I know if I crash and come back„ÄÇ

 I'm almost never going to see log records from transactions that did not commit„ÄÇüò°„ÄÇ

I may see a transaction did commit and they start writing out all their log records„ÄÇ

 but we crash before we write the rest of it and the commit message„ÄÇ

 in that case I certainly have to undo itÔºå but most of the times I'll have all the log records„ÄÇüò°„ÄÇ

In the case of incremental flushingÔºå I had to do some extra work to go figure out„ÄÇ

 here's a bunch of transactions that that have been finished„ÄÇ

 Let me go ahead and make sure I don't reapply their changes„ÄÇ

So this is not that big of a deal because againÔºå we don't write out any dirty pages„ÄÇ

 We're reloading from the checkpoint anyway„ÄÇ it's just we have to douce extra processing in the log to make sure we skip things that shouldn't committed„ÄÇ

 Yes the thing if a transaction is let say searching 10 billion yes and„ÄÇ

Inserted them and sorry you changed them„ÄÇ And then it did something„ÄÇ Yes„ÄÇ

 and those 5 billion things got stored„ÄÇ YesÔºå and last 5 billion things did not get stored„ÄÇ correct„ÄÇ

 and then it like crashed also YesÔºå so now how do you undo these last 5 billion things because again„ÄÇ

 there's nothing to undo„ÄÇ Nobody updated But updated what updated in memory pages„ÄÇ

 we crash that memory is gone„ÄÇSo when we come back onlineÔºå we're loading the checkpoint„ÄÇ

 there's no dirty pages„ÄÇRight„ÄÇIt's just in this case here„ÄÇ

 like we'll talk about replication next classÔºå but it's sort of like when do I send the outside world to a replica„ÄÇ

 heyÔºå here's here's all my updates for my transaction in this case also too if I update 10 billion Tupples„ÄÇ

 I had to have a log buffer that can handle 10 billion twos„ÄÇAnd I may run out of memory„ÄÇ

 So for this reasonÔºå most systems are going to do this approach„ÄÇ

 but there are some advantages to doing this„ÄÇAll right„ÄÇ

 these are the standard techniques we talked about before in the introduction class that would still apply for us in a Iory system„ÄÇ

 So we' do group commit„ÄÇ It just means that we had this log buffer that we can fill in with log record from different transactions and that we can then flush them out whenever that log buffer is full and then have a log buffer another log buffer start getting filled up by other transactions the idea here is we want to amortize the E calls across multiple transactions„ÄÇ

 So if you're the first guy getting added of this queueÔºå then yes„ÄÇ

 you wait in the longest because you's wait for the log buffer to get full„ÄÇ

 but if you're the last guy then you're basically running with a dedicated E call right„ÄÇ

So this is a really old techniqueÔºå as I saidÔºå it is used in pretty much every system today„ÄÇ

 It was originally developed for this thing called Fast path„ÄÇ

 which was a specialized in memorymory engine for IBM's IMS from the early 1980sÔºå but like I said„ÄÇ

 everyone pretty much does this today„ÄÇThe other thing we can do now„ÄÇ

 which is related to the speculative lock release or speculative res we saw under Heathton and NVCC is that„ÄÇ

When a transaction goes ahead and commitsÔºå we don't have to wait for the updates to become durable on disk before we release all our locks„ÄÇ

 We can let other transactions start modifying the tus that we've modified„ÄÇ

 assuming those log records will get will get written our loggrs will get written out of the disk„ÄÇ

 or they read things that we' modified before the log recordss get flushed to disk„ÄÇ

 And we have to sort to keep track of that we did these speckative reads„ÄÇ

 even though the transaction it's logically committed physically it hasn't got made the disk gap„ÄÇ

 So we know that if any transaction reads something that we wrote they have to get stall until until we actually flushed a disk„ÄÇ

 So that means that my read only transaction that it reads an update from your transaction but your transaction has been written in the disk app„ÄÇ

 I have to wait for you to get flushed before I can get committed„ÄÇAgain„ÄÇ

 this is our standard technique and our MVCCÔºå know we would know this information because we can maintain the state map that says this transactions law of records have not been flushed yet„ÄÇ

 so therefore you can't commit right away if you read something from them„ÄÇOkay„ÄÇ

So I sort of allludeed this a little bit now in the last couple of slides where„ÄÇ

If we have a multiversion systemÔºå then the deelta records we're going to generate for MVCC are more or less the same thing we're going to generate for in our log it's not exactly the same because depending how we're doing oldestus and newest or newest oldesttius we could be generating redo records or undo records for our tus are're modifying whereas database log file wants to have redo records at least for MM databasease so the idea is that what if we can combine all the metadata we're generating for NVCC with all the metadata we're generating with the log file and that way we're not duplicating this effort„ÄÇ

üò°ÔºåSo that was the paper I had you guys read for today„ÄÇ

 it came out just this year or last year in the to be 2019Ôºå so as I saidÔºå it's not inmemory system„ÄÇ

 but it's a multiversion systemÔºå and I think it's a good description of how you can piggyback off of the MVCC metadata you're generating to make logging work better„ÄÇ

So this protocol is going be a physical logging protocol that's going to rely on the database systems's time travel table„ÄÇ

 which they call the version storeÔºå the TTBÔºå so we're going to rely on that time travel table to act almost as a recovery log„ÄÇ

So what will happen is we'll start writing out the changes we make to this version store to a log file„ÄÇ

 but they're only going to contain essentially the redo records of the versions we've generated„ÄÇ

And then when we crash and come backÔºå all we need to do now is just suck in this version store into disk or sorry alpha disk into memory„ÄÇ

üò°ÔºåAnd we don't need to undo anything right away„ÄÇ We just bring those pages back in„ÄÇ

 Then the database is now at the state it was at the moment of the crash„ÄÇ

And then we just needed to do some background extra work to do this logical work to make sure that we don't read things„ÄÇ

 we don't persist versions from transactions that did not commit at the crash„ÄÇ

So the problem they were trying to solve that they mentioned in the paper is that„ÄÇ

They had certain customers running on Azure in the cloud or SQL server in the cloud where they would have these really long transactions that when there was a crash„ÄÇ

 now I need to undo all the changes„ÄÇ So this 10 billion table update we just mentioned like Vi crash before I finished updating all those tus When I come back now I got to spend a long time undoing everything„ÄÇ

So what they wanted to do is have what they call constant time recovery„ÄÇ

 which was saying that the time it's going to take for me to restore the database back to the correct state is only contingent on the size of the log file because I just need to be able to read that in the redo phase and I don't to worry about how much time I'm going to spend going reverse direction to undo things„ÄÇ

That makes sense„ÄÇSo let's go to many examples and then we'll see how it works„ÄÇ So again„ÄÇ

 this is with SQL serverÔºå Azure SQL is the name the cloud version of it„ÄÇ But as I said„ÄÇ

 for this featureÔºå they're putting out in the onprem versionÔºå which includes the Linux 1„ÄÇ

So they're doing time travel version storage„ÄÇ but they just call it a versionin store„ÄÇ

 And so the idea here is that you have the main table and this has the latest version and they're doing newsest to oldest„ÄÇ

 So this version has a pointer now to another tuple that's the next oldest tuple„ÄÇ

 and then there's a versionin chain allowed how to go back in time to find previous versions„ÄÇ

 So now if a transaction comes along and wants to update this tuple here„ÄÇ

 we first make a copy of the current version into our version store update the pointer to connect the chain„ÄÇ

 and then we can overwrite the old version with our new one hereÔºå right„ÄÇüòä„ÄÇ

So the idea here is that they want to leverage this thingÔºå which is basically redo information„ÄÇ

To then allow them to restore the state of the database just by loading this thing back in„ÄÇüò°„ÄÇ

So theres going be two variants of how they're going to do this persistent version store„ÄÇ

 So the first one is that they're going to do in row versioning where this is actually„ÄÇ

 you're not going to use the version store„ÄÇ This is going to look like that cicada inlining where„ÄÇ

Instead of making a whole copy of the tuple and putting into the Virgin store„ÄÇ

 they're going to pack it in in the special field inside the tuple of just the Delta record of what got what was modified„ÄÇ

 And then that gets written out to the right ahead logÔºå just as it would any other tuple update„ÄÇ

And that's enough information to how recreate the database state„ÄÇAndI'll show this in the next slide„ÄÇ

And then for the off versioningÔºå which is actually using the time travel table„ÄÇ

Instead of having a time travel table for every single logical table in your database„ÄÇ

 they're going to have one giant time travel table for every single table„ÄÇ

This is way different I've never seen this used before„ÄÇ And so the way it works is that in here„ÄÇ

 like instead of having discrete columns or attributesÔºåThat represent the columns in the table„ÄÇ

 they're just going to have one column and then just store a blob or by string of the serialized data for that tuple and that way you can store any tables tus inside there because there's no we're sort of we're not doing any processing on looking at the actual columns in this we're almost using it as a in- memorymory log and that then gets flushed„ÄÇ

üò°ÔºåSo they're also going to modify the data table data structure itself so that you can do fast andcurrent inserts so that means that they're going to partition the database in memory so that or sorry the table in memory so that different threads right to different regions of the table and then this allows you to you sort of write them out very quickly to the right ahead log and you never can allowed to go back and update any other tuple it's only append only and can now you do sequential rights out to the log which is efficient„ÄÇ

So this is the in row versioning„ÄÇ This is the same thing we talked about with Cicada„ÄÇ Basically„ÄÇ

 theyÔºå when I do it updated this tuÔºå instead of making this copy of this old version and putting into the time travel table„ÄÇ

 those embed Delta record inside of here„ÄÇAnd then that just gets written out to a redhead log just as you normally would„ÄÇ

So an important distinction here about why they can do this and why we can't do this very easily in an Emory database system is that in a disk orient database„ÄÇ

 the twoos can be very very length„ÄÇIn in a immeory database„ÄÇ

 we said we had the primary location of a tuL has to be fixed length„ÄÇ

 so we either have to allocate this delta space for every single tuple it can only be a certain size if for an inmeory„ÄÇ

 but in a disbased database I can leave this thing empty and then if I decide to use it when I insert the new Tple then I just grab a new slot in my page and can write everything out„ÄÇ

Right„ÄÇSo that's one important distinction between what they're doing and what how you do this in every database„ÄÇ

Okay„ÄÇSo let's see how they're going to do recovery so this almost looks like standard Aries„ÄÇ

 except that under AriesÔºå the database is not available until you complete the undo phase in their world„ÄÇ

 it's immediately available after the redo phase„ÄÇüò°„ÄÇ

Because all they're doing is they're loading back in that version store and then all the versions„ÄÇ

 the old versions that are around are now available„ÄÇ

 and then we just piggyback of all that same version detection or identification that we do under MVCC to identify which tuples are actually visible to our transaction„ÄÇ

üò°ÔºåSo analysis phaseÔºå againÔºå we just scan through the log up to the last checkpoint to figure out what transactions are running„ÄÇ

 and then in the redo phaseÔºå we're going to replay the right ahead log to put us back in the correct state for the most main table in the Virgin store but as I said„ÄÇ

 the Virgin store is it's going to contain versions from transactions that did not commit as well as in the main table but because we're NVCC we know what the timestamps were for those versions and then when we come back online start running regular transactions„ÄÇ

 we would know to skip over things that were uncommitted„ÄÇ

Because we're going to maintain this global state map that says„ÄÇ

 here's all the transactions that are around„ÄÇ and here's what their status is„ÄÇThen in the undo phase„ÄÇ

 we select transactions to start executeÔºå and when they find things that are aborted„ÄÇ

 they can go aheadÔºå either clean them upÔºå or there would be a separate background process to clean them up asynchronously„ÄÇ

And they refer to this thing as a logical revertÔºå for my purpose„ÄÇ

 it just sounds like garbage collectionÔºå although the fact that transactions can do slightly different when they find aboard of versions is not exactly garbage collection„ÄÇ

So this is already what it says saidÔºå so you have separate threads they're going to scan through the blocks„ÄÇ

 find all theC versionsÔºå and then if you recognize that the latest version of a Tupple is in the version store„ÄÇ

üò°ÔºåThen you just move it backÔºå knowÔºå into the main table„ÄÇ rightÔºå And again„ÄÇ

 that's the same thing we would see and regularal GC that we've already talked about„ÄÇ

The only one optimization they do that's slightly different is if a transaction recognizes thatÔºå oh„ÄÇ

 in the main tableÔºå I see the current versionÔºå the master version of the TupL is from an aborted transaction„ÄÇ

 then instead of copying that out and making new version in the time travel„ÄÇ

 that is completely overwrite it„ÄÇRight„ÄÇSo againÔºå like I said„ÄÇ

 I think this is interesting because this isÔºå although it's a disb system„ÄÇ

 it's showing how you can use the NPPCC metadata to do a ti or tablet logging scheme and potentially get better performance„ÄÇ

Any questions about this„ÄÇ There's this other bit about the cis log where they would store changes for nonversion data structures like page tables and people Street layouts that again„ÄÇ

 all we can ignore all of that in our image system because we're not going to maintain that information„ÄÇ

Yes„ÄÇCme away extra cost when they reading from„ÄÇYeahÔºå so he'sing it and he's correct„ÄÇ

 like does this mean at runtimeÔºå if I need to go read an old version„ÄÇ

 I had to go to the versionin storeÔºüAnd everything just serializes a blob„ÄÇ

 Do I pay a penalty for having to read thatÔºå YesÔºå but I think they measured itÔºå It's kind of small„ÄÇ

 same thing with the Delta record„ÄÇ I at the inro Delta record„ÄÇ

 I have to replay it and they pay computational costs for that„ÄÇ

If it is inro then you don't have much say that YeahÔºå but it's not as free as like going„ÄÇ

 reading and having everything thereÔºå I agree with you yes„ÄÇ

And that's why we use Delta store in our our own system„ÄÇMost transactions„ÄÇ

I I don't remember that if they say this in paperÔºå most transactions don't update all columns„ÄÇ

 It's right„ÄÇ The Delta store is the in row version is usually gonna to be enough„ÄÇ

So this idea of what they're doing is not newÔºå this is actually very similar to how PostG was originally designed back in like 1986„ÄÇ

üò°ÔºåAgainÔºå PostgresÔºå if there's a paper called the design of PostgresÔºå and Stmaker talks about howÔºå oh„ÄÇ

 well the log is not really a log fileÔºå it's a log table and everything it's getting appended to that and that gets flushed out so at a high level this is very similar and they claim the difference here is that this paper is about taking a system that was not designed to be multiversion in this way and adding after the fact whereas Postgres was designed from the very beginning to be like this„ÄÇ

AgainÔºå process is an NCC system„ÄÇ‰Ω†Áúã„ÄÇSo againÔºå this is a protocol that takes advantage of the fact that we're multiversioned„ÄÇ

So now let's look at another blogging protocol that is for MM datas„ÄÇ

 That's more about how you could architect the system„ÄÇ

To be aware of what the the CPUs actually work and and how they operate„ÄÇ

So silo is a memory OTP embedded database engine that was developed out of Harvard by Eddie Kohler„ÄÇ

 so it's not going to be multiversionÔºå it's going to be a single version system that uses OCC„ÄÇ

 and they're going to use an epoCbased garbage collection protocol to keep track of when you actually commit transactions and flushings out the disk„ÄÇ

So this is silo is the same people that developed the masteryÔºå they wrote Ma Street first„ÄÇ

 and they built silo around it„ÄÇSo this paper here proposes a physical logging technique called silo R„ÄÇ

Is going to try to paralyze as much as possible all the logging checkpoint and recovery processes so that we get the best best performance and the way they're going to do this is that they're going to disaggregate the log across multiple files„ÄÇ

üò°Ôºåwhich could be stored on multiple disk allow them to be read in parallel at the same time„ÄÇ

So the way this works is that for every single CPUU socket in my system„ÄÇ

 I'm going to have a dedicated logger threatÔºå and dedicated log file„ÄÇAnd the idea here is that„ÄÇ

We want to localize all the memory rights we have to do to our local socket„ÄÇ

 So we're never going over shared memory to to another another socket of interconnect„ÄÇ

 And then that wayÔºå that log thread can be just pumping data out as fast as possible to the log file„ÄÇ

So now as transactions runÔºå they're going to get a log buffer from their local logger thread and then write out just redo information because we're in memory„ÄÇ

 we don't have to do any undo„ÄÇ And in this case hereÔºå they are„ÄÇTo make it simple„ÄÇ

 assume that all the log records for a transaction will be written out„ÄÇI actually know in this case„ÄÇ

 you can do incrementalÔºå but in most cases it could be done all at once„ÄÇAll right so again„ÄÇ

 I've already saved some of this so every logger threads going to pull log buffers„ÄÇ

 we hand them out to our local worker threads that are on the same socket as us when my buffer gets full„ÄÇ

 I hand it back to my logger threadÔºå the logger thread can then start writing it out the disk which means you can do incremental flushes and then the worker thread tries to get a new log buffer„ÄÇ

 if there's no more log buffers availableÔºå then the worker thread has to stall until the logger thread comes back and says I flush out enough here's a new log thread„ÄÇ

A log buffer„ÄÇSo now what is going to happenÔºå though„ÄÇ

 is that because now our log file is broken up across multiple files on different disks„ÄÇ

 it may be the case our transaction might modify data thats managed by different CPU sockets„ÄÇ

 and therefore the log records for that transaction might be broken up across different disks„ÄÇüò°„ÄÇ

So I need a way to coordinate all of them so that I don't haveÔºå you knowÔºå if I touch two log files„ÄÇ

 my transaction commits the first log log gets flushedÔºå but the second one doesn't„ÄÇ

 and then I tell the outside world that I crashed„ÄÇüò°ÔºåAnd I come back„ÄÇ And now thatÔºå you know„ÄÇ

 half the updates from the other log file never made it to disk„ÄÇ

So you need a way to coordinate all of them„ÄÇThat so that you know that all the updates for a transaction have been safely flushed„ÄÇ

So this is what the epochC is going to do for usÔºå so what's going to happen is every so often all this epoch will get incremented and that forces every log thread to write out the current content of the log buffer„ÄÇ

And then we record what was the epoch everyone wrote at„ÄÇ

 and now we know that any transaction that has been committed and flushed disk„ÄÇ

Prior to that epoch is now durableÔºå and we can tell the outside world that we've committed„ÄÇ

They're going to use this epoch and other ways of the system to minimizeÔºå again„ÄÇ

 coordination across different CPU socketsÔºå but for our purposes here we can just focus on how we do it for logging„ÄÇ

So our log records is contain the idea of the transaction that modified a given tuple and silo is going to be doiz serializable isolation or serializable scheduling So that means that the transaction ID will be enough to guarantee the ordering the correct ordering of the updates to the database So if we just replay the log and the order of the transaction Is then thatll put us back into the correct state at lower isolation levels you can't do this because I may update something and you may read something and update something else unless we have a log sequence number to order those things„ÄÇ

 we may come back and replay things in a different order So by being serialerizable we can guarantee that this will put it back to the correct state„ÄÇ

üòäÔºå„Å†„Åã„Çâ„Åù„Çå„Åß„Åô„Å≠„ÄÇThe sign„ÄÇWhat I do at this start of hereÔºüSaÔºå isn't the snap isolation„ÄÇ

 if you guarantee that the„ÄÇIt's not necessarily„ÄÇYeahÔºå if your first writer wins„ÄÇ

 then then it also solves that too„ÄÇThe main takeaway is that the transactionity is enough to guarantee the ordering to put you back in the correct state and you don't need a separate log seamless number because with a log sequence number„ÄÇ

 if an I deco that across multiple socketsÔºå then that'sÔºå that's a bottleneck„ÄÇ

So the log file is going to be a tripletÔºå just the tableÔºå the key and the value that gets modified„ÄÇ

 and the value can just be the Dlta record of what the change actually was„ÄÇSo again„ÄÇ

 if I'm doing a simple query to update all the people that are lame with Matt and myself„ÄÇ

 then the log record would be we'll have a separate log record for every single tool that this thing modified„ÄÇ

 rightÔºüso here's the high level architecture of the system„ÄÇ And again„ÄÇ

 the idea here is that I'm going to expose to you or show you that„ÄÇYou know„ÄÇ

 this is just physical logging„ÄÇ So there's nothing really novel being done here„ÄÇ

 It's how they organize the system that I think is actually quite interesting„ÄÇ And isÔºå is„ÄÇ

 is I don't see that„ÄÇ I haven't seen in any other system„ÄÇSo again„ÄÇ

 the transactions are going to update their work ethics are going to execute transactions„ÄÇ

 So they're only going to execute these things as store procedures„ÄÇ

 They call them onetop transactions„ÄÇ But the basic idea is that we do request like an RC„ÄÇ

 say execute this transaction and all the program logic what that transaction is going to do is embed inside the system„ÄÇ

 So we never go back to the client„ÄÇ Everything is done in one invocation„ÄÇ

So when a transaction starts runningÔºå the worker Thr has to go to the logger and get a log buffer„ÄÇ

 and once it has thatÔºå it can start filling up the changes that it starts making to the database„ÄÇ

And at some pointÔºå this log buffer will get fullÔºå so we hand it back to the logger thread and sayy this thing's full„ÄÇ

 please flush it for us and we'll go ahead and get another log buffer„ÄÇSo then now at this point here„ÄÇ

 say that transaction is still running„ÄÇThis other epoch thread wakes up and saysÔºå all right„ÄÇ

 now the new epoch is 200„ÄÇSo that's going to force all the logar threads in the system to now flush whatever buffers that they have„ÄÇ

 including any ones that were handed off beforeÔºå so now the worker thread has to hand back the log buffer to the logging thread„ÄÇ

And then it could keep on running„ÄÇ It could say nowÔºå give me another log buffer start filling up„ÄÇ

 But in this case hereÔºå there aren't anymore more„ÄÇ So it's going to have to stall and wait„ÄÇ

So now the logger thread can start flushing things out the disk right and as it flushes them„ÄÇ

 it frees up log spaceÔºå and then we can hand back the log buffers and let the guy keep it running„ÄÇ

So in the most simplest world I assume every transaction will be finished within EpoC„ÄÇ

 if it spans multiple epochsÔºå then you basically have to keep track of like this transaction was around„ÄÇ

 it spans multiple epochsÔºå so you have to go back further in the log just trying to figure out what actually happened to it„ÄÇ

But the main idea here is that by having this backpack pressure mechanism where if we run out of log buffers„ÄÇ

 we don't allocate more memoryÔºå we make this guy stall„ÄÇ

 that prevents us from generating log records faster than we can actually write them out the disk„ÄÇ

Because otherwise the long buffers just grow infinitely and we'll run out of space„ÄÇ

So let's talk about what this persistent eboch thing actually does„ÄÇSo„ÄÇ

Everyer every logger threads can have the zone file where records all the Delta records that transactions are generating when they run„ÄÇ

 but then there's going to be a special log file where we keep track of the highs epoC that all logger threads have flushed out successfully to disk Now everyone is flushing at the same time when the e increments„ÄÇ

 you tell everyone to flush„ÄÇBut then they may not all happen exactly with the same speed„ÄÇ

 And only when everyone saysÔºå all rightÔºå I flushed it„ÄÇ

 then you go update and update the persistent epoch„ÄÇNowÔºå you don't need this for correctness„ÄÇ

 this is actually just an optimization so that when you crash and come back„ÄÇ

 you can just look at this one file and sayÔºå all rightÔºå what's the epoch you need to start withÔºüüò°„ÄÇ

OtherwiseÔºå you'd have to go look at everyÔºå every single file to figure out what's theÔºå you know„ÄÇ

 the intersection of the epoch across all of them„ÄÇ This is just an optimization„ÄÇ It's a nice to have„ÄÇ

 And the overhead of it is„ÄÇüòäÔºåIt's somewhat small other than an E sync„ÄÇOkay„ÄÇ

 so we know that if this thing gets written up with this„ÄÇ

 then we know that any transaction that executed in an epoch that's than less than or equal to our persistent epoch„ÄÇ

 we know is durable„ÄÇSo it looks like thatÔºå say we have now three logger threads and they each have their own log file running out the disk and they each have a bunch of worker threads and again these guys are just going and getting the log buffers from the logger threads„ÄÇ

And then you have this now the special precision epoC thread that's going to update the file and disk„ÄÇ

Every so often nowÔºå the epoch changes„ÄÇ So everyone has then flush out all the changes they have up to that epoch„ÄÇ

 Once they all then confirm with this epoch thread„ÄÇ

The persistent e thread that they've written out to disk„ÄÇ

 then were're allowed to go ahead and write out the persistent ebox file„ÄÇSo for this one„ÄÇ

 like I saidÔºå you don't need this for correctness„ÄÇ This is just optimization„ÄÇ

 So I don't think you actually need this to be„ÄÇAnother Sy„ÄÇ

 like these you want Sync They know you actually made the disk„ÄÇ I think for this one„ÄÇÂìé„ÄÇ

If you crash into SyncÔºå nowÔºå you have the S syncÔºå if you're going rely on this to figure out what the intersection is of the epoch across all of them„ÄÇ

 you have the Sync„ÄÇRight„ÄÇThat could be just another5 millisecondsÔºå yes„ÄÇAnd every like 20030„ÄÇ

 are we also writing the pages out„ÄÇWhat pagesÔºüThe in memory pagesÔºå rightÔºå weÔºå we are writing them„ÄÇ

 These are not checkpoints„ÄÇ They're not checkpoints„ÄÇ These are just the log records„ÄÇ

 And we have to start from like the starting thing three more slides„ÄÇ We'll get the checkpoints„ÄÇ Yes„ÄÇ

 but yes„ÄÇThis switch checkpoint solvesÔºå yes„ÄÇThis is just again„ÄÇ

 the architecture here is how to do disk aggregateated log files across multiple disks„ÄÇ

And you just have some sort of centralized location that you only update whenever this thing gets changed in the side of the paper„ÄÇ

 they do this every 40 milliseconds„ÄÇ So in a real system„ÄÇ

 this is problematic because if you need like submisecond latency of your transactions„ÄÇ

 you're not going be able to get that with thisÔºå you have to crank this thing down when I asked them why they pick 40 milliseconds that he said this seemed like a decent number right So you could ratchet it down so this thing gets updated every 10 milliseconds or 5 milliseconds„ÄÇ

But now you're flushing a lotÔºå and this thing is getting written out a lot„ÄÇ

 And you now potentially have transactions that spanm multiple epochs„ÄÇ

And you have to do more stuff in recovery to handle this„ÄÇAll right„ÄÇ

 so now next slide we get to your question„ÄÇSo„ÄÇWe'll talk about checkpoints in a few more slides in more detail„ÄÇ

As he saidÔºå if you don't have checkpointsÔºå then these log files grow forever„ÄÇ

 and when I crash and restartÔºå I got to go back and potentially look at the entire log file„ÄÇ

So every so often they're going to take a checkpoint and then when after a recovery„ÄÇ

 you load the last checkpoint in and that sort of balances how much log file you have to look at„ÄÇ

 and they're going to rebuild the indexes based on the checkpointÔºå as I already said„ÄÇ

 because we're not going to make any log records for the indexes„ÄÇNow„ÄÇ

 what is going to be different though than a disk space system„ÄÇ

 which is super interesting is that when they do recovery„ÄÇ

Instead of doing in the redo phase we saw with S server„ÄÇ

 where they start at some point in the past and they process log records going forward in time„ÄÇ

Sillos actually can start at the end of the log file and go in reverse order„ÄÇüò°„ÄÇ

And start playing ball records fromÔºå from newest to oldest„ÄÇAnd again„ÄÇ

 we can do this because we're in memory because we know that there's no dirty pages sitting around that got loaded in from the last checkpoint„ÄÇ

So as we replay the logÔºå we just need to know that what should be the final state of the database of a twooleÔºü

üò°ÔºåIn order for me to say the data has been restored„ÄÇ

 so if I have a tool that's updated been updated 20 timesÔºå if I'm going in reverse order„ÄÇ

 I don't need to replay that log record for all those 20 updatesÔºå I need to find the last update„ÄÇüò°„ÄÇ

And apply that„ÄÇRightÔºå that's totally different than than as you would do this in a discing system„ÄÇ

 becauseÔºå againÔºå dirty pages may have written a disk„ÄÇ So at some pointÔºå I need„ÄÇ

 I need I have to replay everything and then undo everything that shouldn't be around„ÄÇ right„ÄÇ

 So what they're gonna againÔºå they're gonna keep track of the„ÄÇ

Transaction IDs in every tuupple to keep track of what was the timestamp of when this tuup got updated So as I'm replaying the log in reverse order„ÄÇ

 if I find a log record that has a timestamp that's smaller than the current tuple's timestamp„ÄÇ

 meaning it was updated by a transaction in the future that I replayed earlier in the log„ÄÇ

 then I just ignore that log record and I don't have to apply it„ÄÇüò°ÔºåSo may be the case„ÄÇ

 if I'm only updating a small number of tus over and over againÔºå my log„ÄÇ

I may be able to realize thatÔºå you knowÔºå withinÔºå you know„ÄÇ

 maybe the the first megabyte of log record dataÔºå I can ignore everything else after that„ÄÇ

I still want to look at itÔºå but I stillÔºå I don't have to replay it„ÄÇ Yes„ÄÇ

 This question is this won't work if it's not single versioned„ÄÇIf you don't need the old versions„ÄÇüò°„ÄÇ

Then I think this is okay„ÄÇAnd so you could sayÔºå all rightÔºå wellÔºå if I don't need the old versions„ÄÇAm„ÄÇ

We kind of say„Åß„Åô„ÄÇWellÔºå one is no transaction„ÄÇ It's not like after our restart„ÄÇ

 any transaction that was running prior to the restart„ÄÇ

 it's not magically reappear when you come back„ÄÇ So there's no active time transactions of timestamps that could possibly read those things„ÄÇ

So I could just ignore them„ÄÇ NowÔºå if I'm trying to do audit logs or retain things and do time travel queries„ÄÇ

 then yesÔºå I got to replay everythingÔºå then yesÔºå this won't work„ÄÇ I still need to redo it„ÄÇ

But if I don't care about itÔºås car what is the latest versionÔºå then this works„ÄÇ

The reason why most systems don't do this is that when you do replication„ÄÇ

 which we'll talk about next class„ÄÇThe replicas are essentially like in recovery mode„ÄÇ

 and they're just replaying the log„ÄÇ So if you do it from oldest and newest„ÄÇ

 then the same mechanism you would do to do log replays is the same thing you can do for on the replicas„ÄÇ

If now I have a specialized recovery mode where I can go reverse orderÔºå but only on the single node„ÄÇ

 then I have to have basically reim this twice„ÄÇüò°ÔºåSo silo is the only system I know that does this„ÄÇ

 It's just's an interesting way to think about itÔºå thoughÔºå which I like„ÄÇOkay„ÄÇ

 this one I think we've already talked about right so we go look in the persistent epoC file„ÄÇ

 figure out the most recent persistent epoch that was flushed to disk„ÄÇ

 then as our logger threads start replaying the log„ÄÇ

 they just ignore anything that's greater than the persistent epoC„ÄÇAnd actually„ÄÇ

 I already said this all before that because we're going newest to Otis„ÄÇ

 if I recognize that the tuL has already to be modified by a transaction that came later in the log that I've already processed„ÄÇ

 then I don't need to replay that log record„ÄÇ‰Ω†Áúã„ÄÇSo now we also get to and check going into more detail„ÄÇ

 as we already saidÔºå the log file can grow forever„ÄÇ

 so that means that I potentially get to replay the entire log every single time at the restart„ÄÇüò°„ÄÇ

If I have a one year's worth of log without a checkpoint„ÄÇ

 then it potentially take me at one year to restore to the databaseÔºå which is nonsense„ÄÇ

 like nobody could do this„ÄÇSo for an MMory checkpoint„ÄÇ

 the different approaches we're going to choose are going to be tightly coupled with our concurrential scheme and in some ways„ÄÇ

 if we're focusing on amongst the emerge systemsÔºå NPCC„ÄÇ

 then checkpoints essentially can become easy depending on how what we want the consistency level to be in our checkpoint„ÄÇ

Because it could just be we just have start a transaction that takes a snapshot„ÄÇ

 scans through a tableÔºå and we just write out all the versions of the TL that were visible to our snapshot„ÄÇ

 and anything that's not visible to usÔºå meaning it came in the future„ÄÇ

 we'll have the law records and we just replay those„ÄÇRight„ÄÇSo there's a paper written by Dan Boddy„ÄÇ

 who did some early work on Com So a few years ago in SIGBOD„ÄÇ

 where he basically lays out what are the ideal properties you'd want for a checkpointing schema and memory database system„ÄÇ

 and these sort of seem obviousÔºå it's important to just keep these back of our mind„ÄÇ

So obviously we don't want to slow down the regular transaction processing because it's not good if we can run really fast and also we take a checkpoint and now the speed of our system is cut in by half„ÄÇ

So the conventional wisdom for checkpointing schemes is that about a 10% to 15% overhead is considered acceptable„ÄÇ

So every so oftenÔºå if I'm taking a checkpointÔºå if I get 10% slowerÔºå then people are okay with that„ÄÇ

LikewiseÔºå you don't want any sort of huge latency spikes meaning I don't want a blocking checkpoint scheme„ÄÇ

 I don't want to lock the system or lock a table while I take the checkpoint and have all these transactions queue behind this„ÄÇ

 and then finally I release the lock and then they're allowed to run because that's gonna to be a huge spike in our latency and people would pay attention to this„ÄÇ

And the last one also too is that we don't want to require any excess of memory overhead„ÄÇ

 meaning ideally we don't have to take a complete copy of the database in memory as we write down on a checkpoint„ÄÇ

 we want to be able to minimize that overhead because that puts pressure on our dims our memory bandwidth and our caches so we to reduce this as much as possible„ÄÇ

So let's talk about the different properties you can have for a checkpoint for an MMR database„ÄÇ

So looks this one here is this idea here is very similar to what we talked about for dis systems last semester„ÄÇ

üò°ÔºåSo you have this notion of fuzzy versus consistent checkpoints„ÄÇ

So a consistent checkpoint is when the snapshot of the database that's written at disk only contains updates from transactions that committed it„ÄÇ

AgainÔºå thing of just like under NVCC with Sap isolation„ÄÇ

 the file I write the disc only contains the updates from transactions that committed before the transaction started„ÄÇ

 So now when I crash and restartÔºå when I load the checkpoint in„ÄÇ

 I don't have to worry about does my checkpoint contain changes from uncommitted transactions„ÄÇ

 it only contains changes for committed transactions„ÄÇSo againÔºå this is easy to do with NVCC„ÄÇ

 I run my query that scans the entire table and write it out„ÄÇ

The other approach to do fuzzy checkpointsÔºå this is where the snapshot could contain updates from transactions that committed after my checkpoint started„ÄÇ

So my checkpoint starts running and I have a transaction that updates to two tuples and say I scan through half the table„ÄÇ

Then the transaction updates a tu I've already passed through„ÄÇ

 but a tool I haven't passed through yet„ÄÇ my checkpoint now get half the updates of that transaction„ÄÇ

 So now I have to do some extra stuff when I come back and recognize that oh„ÄÇ

 there's an update that I may have missed because this guy was running when I was running and I make sure I find the log records to reapply things correctly„ÄÇ

So this is the easiest to do with NVCCÔºå this one could potentially be faster and have less memory overhead because now I don't have to worry about maintaining old versions and pas into the garbage collector„ÄÇ

 most of them choose thisÔºå this one has advantages for storage overhead„ÄÇ

The next is how we're actually going to do the checkpointÔºå so as already said„ÄÇ

 do it yourself implementation of this would be just a sequential scan on the table and write out every single tu that I find that's visible to me„ÄÇ

 right„ÄÇAnother approach is to do an OS fork„ÄÇSo this idea is interesting because of an MME database„ÄÇ

 when we call fork in the operating systemÔºå what happensÔºüWe have a child process„ÄÇ

 what's in that child process memoryÔºüIt's exact same thingÔºå as the parent process„ÄÇRight„ÄÇ

 yes questions on on don' correct Yes so the way it works is does's copy on right„ÄÇ

 So I call fork the child process now has mapped in in its virtual memory table„ÄÇ

 all the same pages as the parent processÔºå but if the parent process updates any of those pages or if my child process updates into these pages„ÄÇ

 then the OS will make a copy of it and remap it for your process„ÄÇ

 So as the parent process starts modifying in memory that the database„ÄÇ

 the child process won't see this„ÄÇSo the only sort of wellknown database system that actually does this approach is Redis„ÄÇ

 so this is how Redis takes checkpoints and they can do this because they're single threaded engine„ÄÇ

 so it is Paul's transactions do the fork and then now the chart process can has a consistent snapshot that it can start writing out the disk right„ÄÇ

üò°ÔºåIf you don'tÔºå if you're allowedÔºå if you're not going to pause all transactions or updates while you do this„ÄÇ

 then in the trial processÔºå you not need to reconcile the database to remove any uncommitted changes from transactions that are running at the time you you forked„ÄÇ

üò°ÔºåSo hyper actually did this back in the day„ÄÇ So this is the paper from 2011„ÄÇ

 This is the first version of hyper„ÄÇ It was actually influenced by a system that I was working on or help build H store„ÄÇ

 which then became Voltb„ÄÇ So they basically sort of built their own version of Volb„ÄÇ

 but they also wanted to do analytical queries„ÄÇ So they would do OS fork„ÄÇ

 and then on the child processÔºå they could run analytical queries without slowing down the the parent process that was running transactions„ÄÇ

 But then they also could then take the child process and write out that checkpoint to disk without slowing down the„ÄÇ

Slowing down the parent process execution„ÄÇ But because now when they took the checkpoint or did the fork„ÄÇ

 there might have been some inf transactions that were running at the same time„ÄÇ

They then in the child processÔºå you need to look at the undue logs for those transactions„ÄÇ

 which are in memory and make sure you reverse the database„ÄÇ

 reverse those changes so that again you have a consistent snapshot„ÄÇ

And then after some period of timeÔºå either when the checkpoint was written in a disk or when you're finished processing you analytical queries„ÄÇ

 they would kill the child processÔºå all the memory gets cleaned up„ÄÇ

 and then the parent would fork it again„ÄÇSo againÔºå Redis is the only one that does this it's easy to do because they're single threaded„ÄÇ

 I don't know of any other system other than hyper that has attempted this„ÄÇ Well„ÄÇ

 we try to do this in each storeÔºå but we were based on the JVM„ÄÇ

 if you read the manual for the JVM that says don't fork it„ÄÇ

 We said screw that and we forked it anyway„ÄÇ but it has also of problems because you have much of zombie threats because the garbage collector doesnt doesn't start keep running again„ÄÇ

 other background things don't run„ÄÇ So it would workÔºå it was a bad idea„ÄÇOkay„ÄÇ

The next issue is that what are we actually going to store in our checkpoint„ÄÇ

 the two approaches are do complete checkpointsÔºå the Delta checkpoints„ÄÇ

 complete checkpoints is taking whatever to my snapshot of my table or my tables in my database and just write that them out entirely out the disk„ÄÇ

The Delta checkpoint is where you try to recognizeÔºå well„ÄÇ

 what has changed since the last time I took a checkpoint and only write out those updates„ÄÇ

So most systems do this„ÄÇThe only system that I know that does Delta checkpoints is heka Tom„ÄÇ

Because the issue is that with a complete checkpoint from a sort of sort of administrative management standpoint„ÄÇ

 I have this file now on disk„ÄÇ I can sayÔºå ohÔºå this is my checkpoint„ÄÇ

 This is the exact snapshot of the database at this given time„ÄÇ With the Delta checkpoint„ÄÇ

 I need to retain a bunch of Dltas„ÄÇBecause there may be some updates that were in this snapshot„ÄÇ

 but not the next snapshot„ÄÇ And in order for me to make sure I put the database back into the correct state„ÄÇ

 I need to have all of them„ÄÇSo when hackathton does this„ÄÇ

 they have a background thread that will start coalescing combining these vector checkpoints to make it basically one giant complete checkpoint„ÄÇ

 but you need to be mindful of this with the file is that with the file contains that you're looking at So this is easier to implement a waste more space but from engineering and management point„ÄÇ

 this ones better one way to also to make this not have a huge storage overhead like if my database is one terabyte and then in the since the last checkpoint only update one megabyte„ÄÇ

 this thing stores one megabyte this stores one terabyte over and over again if I store the data uncompressed on a file system that supports deduplication„ÄÇ

Then the the pages of memory that I write out are going to be duplicated over and over again„ÄÇ

 and the file system could cut could compress them down for me„ÄÇ

So you can rely on things outside the database to make this thing actually tenable„ÄÇAll right„ÄÇ

 the last one is gonna to be the frequency of how often we're going to take a checkpoint„ÄÇ Again„ÄÇ

 we couldn't take a checkpoint all the timeÔºå but that could slow down the regular transaction workload„ÄÇ

 And so typically what you do is either say I'm going to fire off the checkpoint every at a fixed interval like every five minutes or I fire off a checkpoint after I've written a certain amount of data to my log file„ÄÇ

So in this case hereÔºå like this oneÔºå you can bound how much time it's going take for you to recover„ÄÇ

 like say if I crashÔºå I want my database to come back within five minutes so I can take a checkpoint every every four minutes„ÄÇ

 So I know that when I crashÔºå I only have at most four minutes of log I need to replay to put me back in the correct state„ÄÇ

This oneÔºå you can sort of do the math and figure outÔºå ohÔºå if I canÔºå you know„ÄÇ

 if I can replay the log at1 megaby per secondÔºå then if I set it so that you know„ÄÇ

 I take a checkpoint after 100 megabytesÔºå then I know I can recover in 100 seconds„ÄÇ

 So they're essentially the same thing is just sort of different way to think about the problem„ÄÇ

And againÔºå some applications were you know„ÄÇSome applications where you maybe don't care about things being super highly available within a single node„ÄÇ

 They can use replicas to hle this„ÄÇ So maybe you take a checkpoint at you know„ÄÇ

 at longer intervals or longer fall buffer sizes„ÄÇAnd so that wayÔºå if you have a replica„ÄÇ

 if the master crashesÔºå the replica can come up without having to recover the log„ÄÇ

We'll cover that on Wednesday„ÄÇThe the other thingÔºå thoughÔºå we need to do„ÄÇ

 which every system has to do is that if the data system is toldÔºå heyÔºå we're going to shut down„ÄÇ

Then we want to take a checkpoint at that moment of time„ÄÇ

 we acquiesce or stop all the worker threadsÔºå let them finish whatever transactions they're running and then take a„ÄÇ

 complete snapshotÔºå a complete checkpoint in the database„ÄÇThis is why you want to tell the database„ÄÇ

 heyÔºå I want to shut downÔºå just don't pass it kill D9 into do a hard sick term„ÄÇ

 You want the database to be able to write things out gracefully„ÄÇBecause otherwiseÔºå if you do this„ÄÇ

 then you don't have to replay the law because you know the database is in the correct state„ÄÇ

So this is just a quick summary of what other what some in memory data systems actually do„ÄÇ

 And as I saidÔºå most systemsÔºå in terms of what they're actually going to store„ÄÇ

 are going to do complete complete checkpoints„ÄÇ only Heathton is doing the is doing the Delta 1„ÄÇ

And then what's sort of interesting too is like some of the MVCC systems like Heathton and MemsqL are doing consistent checkpoints game„ÄÇ

 which is just like snap isolation doing scamwriting thing out„ÄÇ both Db is not an MVCC system„ÄÇ

 but they still new consistent checkpoints because what happens is when you say I want to take a checkpoint„ÄÇ

 they switch to this sort of specialized two version or multiversion system where you just have the version of a twobo that existed at the checkpoint„ÄÇ

 And then you just have another version that's always the latest version„ÄÇ

 you can' you don't really have version changes other than you only have two versions„ÄÇ

 So that's how they able do consistent checkpoints„ÄÇ

 Albase can actually do fuzzy and consistent checkpoints„ÄÇ

 I think they do the same thing under times 10 where like if I'm shutting down„ÄÇ

 I'll do a consistent blocking checkpointÔºå but otherwise I normally take a fuzzy checkpoint And the hana does fuzzy with timebased„ÄÇ

üòäÔºåAgainÔºå different database systems do different things„ÄÇ If you're doing NBCCÔºå then my opinion„ÄÇ

 doing the consistent checkpoint„ÄÇAnd taking a complete snapshot is the way to go because there's less engineering overhead of figuring out what they actually write out„ÄÇ

 You just scan through and write everything„ÄÇSo any questions about checkpointsÔºüAgainÔºå after restart„ÄÇ

 I load the checkpoint in„ÄÇAnd as I scan the checkpoint„ÄÇ

 I'm copying data into my tables and as I do thatÔºå it's essentially like an insert„ÄÇ

 I update any indexes that are on that table so that they they get populated correctly„ÄÇüò°„ÄÇ

And then once that all that's doneÔºå now that Davis is back online„ÄÇ Yes„ÄÇ

 the question is with the fourth thing to the index already to get copied depends on implementation„ÄÇ

 You don't have to„ÄÇ so forkÔºå yesÔºå all the indexes get copied„ÄÇ butÔºå you don't have to write it„ÄÇ„ÅØ„ÅÑ„ÄÇ

It's piÔºå againÔºå it's a waste of space„ÄÇ My indexs are huge„ÄÇ

 It's not worth the dis guyo to write that out„ÄÇ If I'm gonna to rep populate it anyway on recovery„ÄÇ

To trade up computation and storage„ÄÇAll rightÔºå so the last thing I want to talk about very quickly is„ÄÇ

How to do fast restartsÔºüSo„ÄÇEverything I've talked about so far„ÄÇ

 the crash recovery for silo and SQL Ser and all the checkpoint stuff„ÄÇThis is assumed thatÔºå ohÔºå well„ÄÇ

 our system was operatingÔºå or our system was runningÔºå something happens„ÄÇ

 when he took over the power cordÔºå lightstruct the data centerÔºå and we did a hard crash„ÄÇ

 It was unexpected„ÄÇBut there's other times we actually may need to restart the database system what it's not going to be from a crash„ÄÇ

Very commonlyÔºå maybe we have to update our OS libraries„ÄÇ

 there's a technique in Linux called CaseP that allows you to update kernels without having to restart the system„ÄÇ

 but let's say you can't always do thatÔºå sometimes you have to restart the whole OS„ÄÇCertainly„ÄÇ

 if you're going to upgrade the hardwareÔºå some disks are swappableÔºå but dims„ÄÇ

 I don't think are like you have to turn the system„ÄÇ

 I'll put the new dims in or certainly if you're moving to another AWS instance„ÄÇ

 that's a whole other piece of hardware„ÄÇAnd then sometimes you just want to update the data system software„ÄÇ

 againÔºå Oracle has techniques that allow you to do patching without taking everything offline„ÄÇ

 but most systems don't have that„ÄÇSo„ÄÇLet's say we want to do this one here all assume these ones here„ÄÇ

 we have to restartÔºå there's no way to get around this„ÄÇRe the entire box„ÄÇFor this one hereÔºå though„ÄÇ

 we don't have to restart the US„ÄÇ We don't have to restart the hardware„ÄÇ

 So be interesting to see way if we can restart the system for an imageory database and not have to flush a checkpoint at the disk and then load it all back in over and over again„ÄÇ

So this is what Facebook can do in their scuba system„ÄÇ

 so I'll briefly talk about what scuba is in a second„ÄÇ

 but like if you remember from the introduction class„ÄÇ

 we spent you know this is what the students voted for the most interesting system that they wanted to talk about so„ÄÇ

Scuuba is aÔºå its a distributed in memoryory of that system developed at Facebook to do event log processing„ÄÇ

 So whenever you load a page in FacebookÔºå they're going keep track of what every single stage„ÄÇ

 every single service it touches for that request„ÄÇ record all that information and then dump it up to scuba„ÄÇ

 so you can do analytics and sayÔºå find me theÔºå you know„ÄÇ

Explain to me why my page request went 20% slower than than yesterday„ÄÇ

And because they're pushing out updates all the time for their web apps„ÄÇ

 they want to know whether things are regressing„ÄÇSo what they're going to do though„ÄÇ

 is that when they want to update the database system software„ÄÇ

That instead of shutting it down and taking checkpoint and loading all back in„ÄÇ

 they're going to write out the contents of the database„ÄÇ

 essentially the checkpoint to shared memory„ÄÇRestart the processÔºå come back up„ÄÇ

 see that my database state is now in shared memory and suck it all back in„ÄÇ

 They're essentially using shared memory as like a Ram disk„ÄÇWhich is kind of interesting„ÄÇSo again„ÄÇ

 I've already said thisÔºå it's a distributed MRI system„ÄÇ

 it has a heterogeneous architecture with leaf nodes and aggregator nodes„ÄÇ

 this is not too interesting for usÔºå but just the thing to be mindful is the state of the database is only at these leaf nodes here„ÄÇ

RightÔºå the aggregator nodes and the root node up above„ÄÇ These are stateless„ÄÇ

 And there's combining results of queries that these guys are they generate„ÄÇ

 So all the updates of inserting new data goes to these leaf nodes„ÄÇ

 So this is the primary source location„ÄÇ So if we want to restart these guysÔºå we need a way to„ÄÇSo„ÄÇ

 try this out the shared memory so we don't have to load the checkpoint from disk„ÄÇAs a high level„ÄÇ

 the way it works is that if I have a query like this„ÄÇ

 they're going pick up to plan fragments and say this guy goes down„ÄÇ

 these guys are all going to send their updates up„ÄÇ

 and then we just combine it together to produce the final answer„ÄÇ

This is actually basically how Memsql works up wellÔºå because the story goes„ÄÇ

 The guy that founded MmsqLÔºå he was at Microsoft saw the Hexton project„ÄÇ

 barred some of their hours inspired by their ideasÔºå went to FacebookÔºå I don't think he saw scuba„ÄÇ

 but he saw patterns used in other systems at Facebook„ÄÇ

 saw this idea and then combined together to makemsql„ÄÇüòäÔºåAll right„ÄÇ

 so there's two approaches to do thisÔºå so we've already said this„ÄÇ

We could just do the shared memory heaps so that we could just„ÄÇIn our system„ÄÇ

 we modify the memory allocator so that whenever we call malloc for the data we're storing in the table„ÄÇ

 instead of being local to my process now it's sitting in shared memory and as far as I know„ÄÇ

 there's no overhead in the OS of saying something is in shared memory because it's just getting back a memory address in the OS knows that that memory should should last beyond the process lifetime„ÄÇ

So to do this thoughÔºå againÔºå you have to modify J Mal or TC Mal or whatever mal implementation you're using to be able to write things out to shared memory and be able to divide things up efficiently so that multiple threads can be writing in the same location„ÄÇ

So in the paperÔºå they talk about how they can't do lazy allocation of backing pages and shared memory„ÄÇ

 meaning if I call malic and something in shared memory„ÄÇ

The OS is actually going need to have that back by physical memory„ÄÇSo they claim this in the paper„ÄÇ

 Facebook bought the guy or they hired the guy that created JE Maloc„ÄÇ

 so in the paper you about how they talk to the JE Malick guy he says you can't do this„ÄÇ

 I posted this on Twitter or at least the slides or this on Twitter and then some dude reached out to me and says you actually could do this like he actually tried it and at least in the newer version of Linux that you could you could allocate memory shared memory and not have it backed by physical memory right away so this part is actually not true anymore so you actually could still do it this way so you could have your memory allocateocator allocate pages on the heap and shared memory and not worry about any thread safety issues or backing it right away„ÄÇ

But insteadÔºå what they're going to do in this version of scuba is that when I'm told my process is going to shut down„ÄÇ

 I stop all updates from any transactionsÔºå any queriesÔºå write everything out to share memory„ÄÇ

 and then I can go ahead and restart„ÄÇAnd so they do some extra stuff where they keep track of what's the layout of that memory„ÄÇ

That they're writing out the shared memory„ÄÇ LikeÔºå what's the version of the database that wrote it out so that if you restart and come back and you recognize„ÄÇ

 ohÔºå I have some shared memory contents of the of the database from what it was before I've restarted„ÄÇ

 then you make sure that the layout is still correct„ÄÇ

 So they basically maintain some extra metadata when they write out the shared memory to sayÔºå oh„ÄÇ

 by the wayÔºå my layout looks like this because I' on this version„ÄÇ

 So if I come back and it's incompatibleÔºå then I just load it back from disk„ÄÇ

So scuba is an interesting system because in their worldÔºå thisÔºå this is not high value dataÔºå right„ÄÇ

 It's not like your timeline or all your whatever your friend messaging crap„ÄÇ

 like they it's data that they could potentially lose„ÄÇ They don't want to„ÄÇ

But it's not like they lose money if this goes away„ÄÇIdeally„ÄÇ

 if they say a node comes back and there's nothing on diskÔºå but the shared memory„ÄÇ

 sorry the database system comes backÔºå there's nothing on diskÔºå everything's in shared memory„ÄÇ

 but the shared memory data is not compatible with our new version of our software„ÄÇ

 then they'll just backfill it from another disk from some results in their worldÔºå that's okay„ÄÇ

So I like this idea„ÄÇ I don't know if anybody else does this with shared memory„ÄÇ

 The most famous shared memory system is PostgresÔºå but they're obviously not in memory„ÄÇ

 And they're doing this to coordinate across different processes„ÄÇ This is like this is now„ÄÇüòä„ÄÇ

This is an interesting idea to think aboutÔºå because it'sÔºå it's„ÄÇ

Passing data from one instance of the process to the next„ÄÇ

 even though they don't actually overlap in time„ÄÇThey're allowing the memory of the database„ÄÇ

Go beyond the lifetime of the data decision process itselfÔºå which I find super fascinating„ÄÇ

Any questions about thisÔºüAgainÔºå if my database is one terabyte„ÄÇ

 if I don't have this technique and I restart the database„ÄÇ

 then I got to suck in one terabyte off a diskÔºå which could be slow„ÄÇüò°ÔºåBut in this case here„ÄÇ

 I could come back and instantaneously have everything that I that I need„ÄÇ‰Ω†Áúã„ÄÇAll right„ÄÇ

 so just to finish up the„ÄÇThe main takeaways from this is that physical logging is probably the best approach you would want to use for an MMmeory database and it's going support all possible contractual schemes„ÄÇ

 there are some advantages we take things we can take advantage of if we're using NPPCC like doing the copy on updates to get consistent checkpoints by just relying on the idea of Snashot isolation to only see changes from transactions that have already committed and as I'll talk about at the end of the semester„ÄÇ

 nonvatal memory is here„ÄÇIt is going to change how we'd want to do some of these logging checkpoint protocols„ÄÇ

 but the high level idea will still roughly be the same that we don't need to maybe restore or don't need to log any undo information if we're careful about where we store our data„ÄÇ

 that we only need to keep tracker redo informationÔºå and that makes the log replay much faster„ÄÇ‰Ω†Áúã„ÄÇüòä„ÄÇ

Al rightÔºå so next classÔºå we'll talk about networking protocols„ÄÇ And I I don't have to listed here„ÄÇ

 but we'll also introduce Project 2„ÄÇ I'll post this on Piaz„ÄÇ

 you guys should start thinking about how to form groups of three because that's project Project  two will be a group project„ÄÇ

 OkayÔºå if you can't find a a group to be inÔºå send me an email and we'll figure out something for you„ÄÇ

 okay„ÄÇüòäÔºåI forget how many students are in the classÔºå I don't think many of here are dropped„ÄÇ

 so we should have enough to do exactly threeÔºå I think 13 groups or three or something like that„ÄÇ

 OkayÔºå YesÔºå so right it is like this there are  six groupsÔºå all of them are two people SB3„ÄÇO„ÄÇ

So make friends„ÄÇAnd againÔºå and whoever is in your group for Project3'm sorry„ÄÇ

 Project two will also be in the same group for Project 3„ÄÇ

 so someone is like an has hygiene problems„ÄÇYou knowÔºå if you can't stand when in project 2„ÄÇ

 then you have to deal with the Project 3Ôºå and that's not going to be goodÔºå okay„ÄÇ

And if I had to break up fightsÔºå I done a before and I can do it again„ÄÇ

 I ideally don't want to do thisÔºå okay„ÄÇ

![](img/5bfd10e92f11ecac17826e17d9bc8016_3.png)

Any questionsÔºüBnk it in the side parkÔºå what is thisÔºü



![](img/5bfd10e92f11ecac17826e17d9bc8016_5.png)

I took a sip and had to sp his ain„ÄÇHere called the OECustom O G IQ down with the STI„ÄÇ

 you look and it was going„ÄÇG me a just to get my paws on because I needed just a little more cake„ÄÇ

W a fish I the just one sipÔºå put it to my lips and„ÄÇjust the as„ÄÇAnd my hood boom feet is saying„ÄÇ

 after ice cubeÔºå take a say a to the brain„ÄÇ