- en: 哈佛CS50-AI ｜ Python人工智能入门(2020·完整版) - P15：L4- 模型学习 2 (回归，损失函数，过拟合，正则化，强化学习，sklearn)
    - ShowMeAI - BV1AQ4y1y7wy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 哈佛CS50-AI ｜ Python人工智能入门(2020·完整版) - P15：L4- 模型学习 2 (回归，损失函数，过拟合，正则化，强化学习，sklearn)
    - ShowMeAI - BV1AQ4y1y7wy
- en: not rain authentic or counterfeit but，sometimes what we want to predict is a。real
    numbered value and for that we have，a related problem not classification but。instead
    known as regression and，regression is the supervised learning。problem where we
    try and learn a，function mapping inputs to outputs same。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 并不是降雨的真实与否，而有时我们要预测的是一个实数值，对此我们有一个相关问题，不是分类，而是称为回归，回归是监督学习问题，我们尝试学习一个将输入映射到输出的函数。
- en: as before but instead of the outputs，being discrete categories things like。rain
    or not rain in a regression problem，the output values are generally。continuous
    values some real number that，we would like to predict happens all the。time as
    well you might imagine that a，company might take this approach if it's。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 但不同的是，输出不再是离散类别，如降雨或不降雨，而在回归问题中，输出值通常是连续值，一些我们希望预测的实数，这种情况时常发生，你可能想象公司会采取这种方法。
- en: trying to figure out for instance what，the effective it's advertising is like。how
    do advertising dollars spent，translate into sales for the company's。product for
    example and so they might，like to try to predict some function。that takes as input
    the amount of money，spent on advertising and here we're just。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，试图弄清楚广告的有效性。广告支出如何转化为公司产品的销售。例如，他们可能想尝试预测一个函数，该函数将广告支出的金额作为输入。
- en: going to use one input but again you，could scale this up，many more inputs as
    well if you have a。lot of different kinds of data you have，access to and the goal
    is to learn a。function that given this amount of，spending on advertising we're
    gonna get。this amount in sales and you might judge，based on having access to a
    whole bunch。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们只使用一个输入，但你可以扩大到多个输入，如果你拥有多种不同类型的数据，目标是学习一个函数，给定这笔广告支出，我们将获得这个销售额。
- en: of data like for every past month here's，how much we spent on advertising and。here
    is what sales were and we would，like to predict some sort of hypothesis。function
    that again given the amount，spent on advertising can predict in this。case some
    real number some number，estimate of how much sales we expect。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会根据过去每个月的数据来判断，这里是我们花在广告上的金额，这里是销售额，我们想预测某种假设函数，再次给定广告支出的金额，可以预测在这种情况下某个实数，预计的销售额。
- en: that company to do in this month or in，this quarter or whatever unit of time。we're
    choosing to measure things in and，so again the approach to solving this。type of
    problem we could try using a，linear regression type approach where we。take this
    data and we just plot it on，the x-axis we have advertising dollars。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公司本月或这个季度或其他时间单位要做的事情。我们选择的衡量方式，因此再次解决这个类型问题的方法。我们可以尝试使用线性回归的方法，在x轴上绘制这些数据，广告支出如前所述。
- en: spent on the y-axis we have sales and we，might just want to try and draw a line。that
    does a pretty good job of trying to，estimate this relationship between。advertising
    and sales and in this case，unlike before we're not trying to。separate the data
    points into discrete，categories but instead in this case。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在y轴上是销售额，我们可能只是想画一条线，尽可能准确地估计广告与销售之间的关系，在这种情况下，与之前不同的是，我们不是尝试将数据点分为离散类别。
- en: we're just trying to find a line that，approximates this relationship between。advertising
    and sales so that if we want，to figure out what the estimated sales。are for a
    particular advertising budget，you just look it up in this line figure。out for
    this amount of advertising we，would have this amount of sales and just。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是试图找出一条线，近似广告与销售之间的关系，这样如果我们想要弄清楚特定广告预算的预计销售额，你只需在这条线中查找，确定对于这个广告支出，我们会有这个销售额。
- en: try and make the estimate that way if，you can try and come up with a line。again
    figuring out how to modify the，weights using various different。techniques to try
    and make it so this，line fits as well as possible so with。all of these approaches
    then to trying，to solve machine learning style problems。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 而是试图以这种方式进行估计，如果你可以试图画出一条线，再次弄清楚如何使用各种技术修改权重，以使这条线尽可能贴合。因此，所有这些方法都是为了尝试解决机器学习风格的问题。
- en: the question becomes how do we evaluate，these approaches how do we evaluate
    the。various different hypotheses that we，could come up with because each of these。algorithms
    will give us some sort of，hypothesis some function that map's。inputs to outputs
    and we want to know，how well does that function work and you。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 问题变成了我们如何评估这些方法，如何评估我们可能提出的各种不同假设，因为这些算法会给我们某种假设，一个将输入映射到输出的函数，我们想知道这个函数的效果如何。
- en: can think of evaluating these hypotheses，and trying to get a better hypothesis
    as。kind of like an optimization problem in，an optimization problems you'll recall。from
    before we were either trying to，maximize some objective function by。trying to
    find a global maximum or we，were trying to minimize some cost。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将评估这些假设并尝试得到更好假设的过程看作是一个优化问题，在优化问题中，我们以前提到过，我们要么试图通过寻找全局最大值来最大化某个目标函数，要么试图最小化某些成本。
- en: by trying to find some global minimum，and in the case of evaluating these。hypotheses
    ISM one thing we might say is，that this cost function the thing we're。trying to
    minimize we might be trying to，minimize what we would call a loss。function and
    what a loss function is if，it is a function that is going to。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通过尝试找到某个全局最小值，在评估这些假设的情况下，我们可以说这个成本函数是我们试图最小化的东西，我们可能试图最小化我们所称之为损失函数的东西，而损失函数是一个。
- en: estimate for us how poorly our function，performs more formally it's like a loss。of
    utility by whenever we predict，something that is wrong that is a loss。of utility
    that's going to add to the，output of our loss function and you。could come up with
    any loss function，that you want just some mathematical way。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为我们估计我们的函数表现不佳的程度，更正式地说，这就像是效用损失，每当我们预测错误时，都会增加我们的损失函数的输出，而你可以提出任何你想要的损失函数，只要是某种数学方式。
- en: of estimating you know given each of，these data points given what the actual。output
    is and given what our projected，output is our estimate you could。calculate some
    sort of numerical loss，for it but there are a couple of popular。loss functions
    that are worth discussing，just so that you've seen them before。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一个数据点，考虑到实际输出以及我们预测的输出，我们可以计算某种数值损失，但有几个流行的损失函数值得讨论，以便你之前见过它们。
- en: when it comes to discrete categories，things like rain or not rain counterfeit。or
    not counterfeit one approaches the 0，1 loss function and the way that works。is
    for each of the data points our loss，function takes as input what the actual。output
    is like whether it was actually，raining or not raining and takes our。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到离散类别时，比如下雨或不下雨，假币或真币，我们接近于0，1损失函数，其工作方式是对每一个数据点，我们的损失函数以实际输出为输入，比如它是否真的在下雨。
- en: prediction into account did we predict，given this data point that it was。raining
    or not raining and if the actual，value equals the prediction well then。the 0 1
    loss function will just say the，loss is 0 there was no loss of utility。because
    we were able to predict，correctly and otherwise if the actual。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到预测，我们是否预测了在这个数据点下是下雨还是不下雨，如果实际值等于预测值，那么0，1损失函数就会说损失为0，因为我们能够正确预测，没有效用损失。
- en: value was not the same thing as what we，predicted well then in that case our。loss
    is 1 we lost something lost some，utility because what we predicted was。the output
    of the function was not what，it actually was and the goal then in a。situation
    like this would be to come up，with some hypothesis that minimizes that。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果实际值与我们预测的值不相同，那么在这种情况下我们的损失为1，我们失去了一些效用，因为我们预测的函数输出并不是实际的值，那么在这种情况下的目标就是提出一个最小化损失的假设。
- en: the total empirical loss the total，amount that we've lost if you add up for。all
    these data points what the actual，output is and and what your hypothesis。would
    have predicted so in this case for，example if we go back to classifying。days as
    raining or not raining and we，came up with this decision boundary how。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 总的经验损失是我们损失的总量，如果将所有这些数据点的实际输出和我们的假设预测相加，就可以得到。在这种情况下，例如，如果我们回到将天分类为下雨或不下雨，我们得出了这个决策边界。
- en: would we evaluate this decision boundary，how much better is it then drawing
    the。line here or drawing the line there well，we could take each of the input data。points
    and each input data point has a，label whether it was raining or whether。it was
    not raining and we could compare，it to the prediction where。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何评估这个决策边界，它比在这里画线或在那里画线好多少呢？我们可以获取每个输入数据点，每个输入数据点都有一个标签，表明是否下雨，我们可以将其与预测进行比较。
- en: we predicted it would be raining or not，raining and assign it a numerical value。as
    a result so for example these points，over here they were all rainy days and。we
    predicted they would be rainy because，they fall on the bottom side of the line。so
    they had a loss of zero nothing lost，from those situations and likewise Amos。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预测它会下雨或不下雨，并为其分配一个数值。例如，这些点都是雨天，我们预测它们会下雨，因为它们位于线的下方，因此它们的损失为零，没有损失。
- en: true for some of these points over here，where it was not raining and we。predicted
    it would not be raining either，where we do have loss our points like。this point
    here and that point there，where we predicted that it would not be。raining but
    in actuality it's a blue，point it was raining or likewise here we。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些点来说，确实如此，例如在这里没有下雨，而我们预测也不会下雨，然而我们确实有损失，例如这里和那里这两个点，我们预测不会下雨，但实际上是一个蓝点，它在下雨，或者在这里。
- en: predicted that it would be raining but，in actuality it's a red point it was
    not。written and so as a result we，miscategorized these data points that we。were
    trying to train on and as a result，there's some loss here one loss here。there
    here and there for a total loss of，four for example in this case and that。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 预测它会下雨，但实际上是一个红点，它没有下雨，因此我们错误分类了这些我们试图训练的数据点，因此这里有一些损失，例如这里一个，那里一个，总损失为四，例如在这种情况下。
- en: might be how we would estimate or how we，would say that this line is better
    than。a line that goes somewhere else or，alignments further down because this。line
    might minimize the loss so there's，no way to do better than just these four。points
    of loss if you're just drawing a，straight line through our space so the 0。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何估计或说这条线比另一条线更好，或者在更远处的对齐，因为这条线可能最小化损失，所以如果你只是通过我们的空间画一条直线，无法比这四个损失点做得更好。
- en: 1 loss function checks did we get it，right did we get it wrong if we got it。right
    the loss is zero nothing lost if，we got it wrong then our loss function。for that
    data point says one and we add，up all of those losses across all of our。data points
    to get some sort of，empirical loss how much we have lost。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 1 损失函数检查我们是否做对了，如果做对了，损失为零，没有损失；如果做错了，那么我们的损失函数对该数据点的值为1，我们将所有数据点的损失相加，得到某种经验损失，表示我们损失了多少。
- en: across all of these original data points，that our algorithm had access to there。are
    other forms of loss as well that，work especially well when we deal with。more real
    valued cases cases like the，mapping between advertising budget and。amount that
    we do in sales for example，because in that case you care not just。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些原始数据点中，我们的算法可以访问到其他形式的损失，当我们处理更多实值案例时，尤其有效，例如广告预算与销售额之间的映射，因为在这种情况下，你不仅关心。
- en: that you get the number exactly right，but you care how close you were to the。actual
    value if the actual value is you，did like $2，800 in sales and you。predicted that
    you would do $2，900 in，sales you know maybe that's pretty good。that's much better
    than if you had，predicted you do $1，000 in sales for。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能并不在乎数字完全准确，而是关心你离实际值有多近。如果实际销售额是$2,800，而你预测的是$2,900，或许这还不错；这比你预测$1,000的销售额要好得多。
- en: example and so we would like our loss，function to be able to take that into。account
    as well take apart take into，account not just whether the actual。value in the
    expected value are，exactly the same but also take into。account how far apart they
    were and so，for that one approaches what we call l1。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们希望我们的损失函数能够考虑到这一点，不仅考虑实际值与预期值是否完全相同，还要考虑它们之间的差距，因此对于这个，我们采用我们所称的L1。
- en: loss l1 loss doesn't just look at，whether actual and predicted are equal。to
    each other but we take the absolute，value of the actual value minus the。predicted
    value in other words we just，ask how far apart were the actual and。predicted values
    and we sum that up，across all of the data points to be able。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: l1损失不仅仅看实际值和预测值是否相等，而是取实际值减去预测值的绝对值，换句话说，我们只是在问实际值和预测值之间的距离有多远，并将所有数据点的损失加起来。
- en: to get what our answer ultimately is so，what might this actually look like for。our
    data set well if we go back to this，representation where we had advertising。along
    the x axis sales along the y axis，our line was our prediction our estimate。for
    any given amount of advertising what，we predicted sales was going to be and。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这样我们就能得出最终的答案。那么，对于我们的数据集，这可能是什么样子呢？如果我们回到这个表示，其中x轴是广告支出，y轴是销售额，我们的线是我们对于给定广告支出量的预测。
- en: our l1 loss is just how far apart，vertically along the sales axis our。prediction
    was from each of the data，points so we could figure out exactly。how far apart
    our prediction was from，each of the data points and figure out。as a result of
    that what our loss is，overall for this particular hypothesis。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的l1损失就是在销售轴上，预测值与每个数据点之间的垂直距离。这样我们就可以准确地计算出预测值与每个数据点之间的距离，并因此得出这个特定假设的总体损失。
- en: just by adding up all of these various，different individual losses for each
    of。these data points and our goal then is，to try and minimize that loss to try
    and。come up with some line that minimizes，what the utility losses by judging how。far
    away our estimate amount of sales is，a from the actual amount of sales and。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将所有这些不同的数据点的个别损失加起来，我们的目标是尽量最小化这些损失，试图找到一条线，使得通过判断我们预测的销售额与实际销售额的差距来最小化效用损失。
- en: turns out there are other loss functions，as well one that's quite popular is
    the。l2 loss the l2 loss instead of just，using the absolute value like how far。away
    the actual value is from the，predicted value it uses the square of。actual minus
    predicted so how far apart，are the actual and predicted value and。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明还有其他损失函数，其中一种非常流行的是l2损失。l2损失不仅仅使用绝对值来衡量实际值与预测值之间的距离，而是使用实际值减去预测值的平方，即实际值和预测值之间的差距。
- en: it squares that value effectively，penalizing much more harshly anything。that
    is a worse prediction so you，imagine if you have two data points that。you predict
    as being you know one value，away from their actual value as opposed。to one data
    point that you predict as，being too away from its actual value the。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 它有效地平方了该值，严厉惩罚任何更差的预测。想象一下，如果你有两个数据点，你预测的值与它们的实际值相差很远，而不是一个数据点与其实际值相差不远。
- en: l2 loss function will more harshly，penalize that one that is two away。because
    it's going to square however，much the differences between the actual。value and
    the predicted value and，depending on the situation you might。want to choose the
    loss function depend，what you care about minimizing if you。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: l2损失函数会更严厉地惩罚与实际值相差较远的预测，因为它会平方实际值和预测值之间的差异，根据具体情况，你可能想选择某个损失函数，这取决于你最关心的最小化目标。
- en: really care about minimizing the error，on more outlier cases then you might。want
    to consider something like this but，if you've got a lot of outliers and you。don't
    necessarily care about modeling，them then maybe in l1 loss function is。preferable
    but there are trade-offs here，that you need to decide based on a。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你真的想在更多异常值的情况下最小化误差，你可能想考虑类似的情况，但如果你有很多异常值，并且并不一定关心对它们建模，那么l1损失函数可能更可取，但在这里有一些权衡，你需要根据具体情况做出决定。
- en: particular set of data but what you do，run the risk of with any of these loss。functions
    with anything that we're，trying to do is a problem known as。overfitting and overfitting
    is a big，problem you can encounter in machine。learning which happens anytime a
    model。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 针对特定数据集，你确实面临的风险是任何这些损失函数都可能导致一个叫做过拟合的问题，过拟合是机器学习中的一个重大问题，每当模型过于复杂以至于无法泛化到新数据时就会发生。
- en: '![](img/789c854420af1d2fd8944135cd97bb86_1.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/789c854420af1d2fd8944135cd97bb86_1.png)'
- en: fits too closely with a data set and as，a result fails to generalize we would。like
    our model to be able to accurately，predict data and inputs and output pairs。for
    the data that we have access to but，the reason we wanted to do so is because。we
    want our model to generalize well to，data that we haven't seen before。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 该边界与数据集的拟合不太紧密，因此可能无法很好地进行推广。我们希望我们的模型能够准确预测数据及输入输出对。我们想这么做的原因是希望我们的模型能够很好地推广到我们未见过的数据。
- en: I would like to take data from the past，year of whether it was raining and not。raining
    and use that data to generalize，it towards the future to say in the。future is
    it going to be raining or not，raining or if I have a whole bunch of。data on what
    counterfeit and not，counterfeit US dollar bills look like in，them。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我想利用过去一年是否下雨的数据来进行训练，并利用这些数据进行未来的推断，以判断将来是否会下雨。或者如果我有大量关于假币和真币的样本数据的话。
- en: I'd like to train a computer to be able，to in the future generalize to other。dollar
    bills that I might see as well，and the problem with overfitting is that。if you
    try and tie yourself too closely，to the data set that you're training。your model
    on you can end up not，generalizing very well so what does this。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望训练一台计算机，使其在未来能够推广到我可能看到的其他钞票上。而过拟合的问题在于，如果你过于紧密地绑定于训练数据集，你最终可能无法很好地进行推广。那么这又是什么意思呢？
- en: look like well we might imagine the，rainy day and not rainy day example。again
    from here where the blue points，indicate rainy days and the red points。indicate
    not rainy day isn't and we，decided that we felt pretty comfortable。with drawing
    a line like this as the，decision boundary between rainy days and。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们可以想象一个下雨天和非下雨天的例子。从这里，蓝点表示下雨天，而红点表示非下雨天。我们决定感到相当舒适，可以画一条这样的线作为下雨天和非下雨天之间的决策边界。
- en: not rainy days so we can pretty，comfortably say that points on this side。more
    likely to be rainy days at points，on that side more likely to be not rainy。days
    but the loss the empirical loss，isn't zero in this particular case。because we
    didn't categorize everything，perfectly，there was this one outlier this one day。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非下雨天，我们可以相当自信地说，位于这一侧的点更可能是下雨天，而位于另一侧的点则更可能是非下雨天。但在这种情况下，经验损失并不是零，因为我们没有完美地对所有情况进行分类，这其中有一个异常值。
- en: that it wasn't raining but yet our model，sealed still predicts that it is raining。but
    that doesn't necessarily mean our，model is bad it just means the model。isn't 100%
    accurate if you really wanted，to try and find a hypothesis that，resulted in minim。the
    loss you could come up with a，different decision boundary it wouldn't。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管那天并没有下雨，但我们的模型仍然预测会下雨。但这并不一定意味着我们的模型很差，只是意味着模型并不是100%准确。如果你真的想找到一个能最小化损失的假设，你可以提出一个不同的决策边界。
- en: be a line but it would look something，like this like this decision boundary。does
    separate all of the red points from，all of the blue points because the red。points
    fall on this side of this，decision boundary the blue points fall。on the other
    side of the decision，boundary but this we would probably。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是一条线，但它看起来会像这样。这条决策边界确实将所有红点与所有蓝点分开，因为红点位于这条决策边界的一侧，而蓝点则位于决策边界的另一侧，但我们可能会这样认为。
- en: argue is not as good of a prediction，even though it seems to be more accurate。based
    on all of the available training，data that we have for training this。machine learning
    model we might say that，it's probably not going to generalize。well but if there
    were other data points，like here and there we might still want。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于我们训练这个机器学习模型所拥有的所有可用训练数据，它的预测似乎更准确，但它的预测效果却不如人意。我们可能会说，它可能无法很好地推广。如果还有其他数据点，比如在这里或那里，我们可能仍然想考虑这些点为下雨天，因为我们认为这可能只是一个异常值。
- en: to consider those to be rainy days，because we think this was probably just。an
    outlier so if the only thing you care，about is minimizing the loss on the data。you
    have available to you you run the，risk of overfitting and this can happen。in the
    classification case it can also，happen in the regression case that here。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你关心的唯一事情是最小化你拥有的数据上的损失，你就会面临过拟合的风险。这种情况可能发生在分类问题中，也可能发生在回归问题中。
- en: we predicted what we thought was a，pretty good line relating advertising to。sales
    trying to predict what sales were，going to be for a given amount of。advertising
    but I could come up with a，line that does a better job of。predicting the training
    data and it，would be something that looks like this。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预测了我们认为广告与销售之间有一个相当不错的关系，试图预测给定广告量的销售情况，但我可以得出一条更好地预测训练数据的线，它可能看起来像这样。
- en: just you know connecting all of the，various different data points and now，there's
    no loss at all。now I've perfectly predicted given any，advertising what sales are
    and for all。the data available to me it's going to，be accurate but it's probably
    not going。to generalize very well I have overfit，my model on the training data
    that is。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 只是，你知道，连接所有不同的数据点，现在根本没有损失。现在我已完美预测了在任何广告下的销售情况，并且对于我所有可用的数据，它会是准确的，但它可能不太会很好泛化。我已经对训练数据过拟合了我的模型。
- en: available to me and so in general we，want to avoid overfitting we'd like。strategies
    to make sure that we have an，overfit our model to a particular data。set and there
    are a number of ways that，you could try to do this one way is by。examining what
    it is that we're，optimizing for in an optimization。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我可用的数据，因此一般来说，我们希望避免过拟合，我们希望有策略确保我们没有将模型过拟合到特定数据集。有几种方法可以尝试做到这一点，其中一种方法是检查我们在优化中所优化的内容。
- en: problem all we do is we say you know，there's some cost and I want to minimize。that
    cost and so far we've defined that，cost function the cost of a hypothesis。just
    as being equal to the empirical，loss of that hypothesis like how far。away are
    the actual data points the，outputs away from what I predicted them。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是我们只是说，存在某种成本，我想要最小化该成本，到目前为止我们已定义了该成本函数，即假设的成本等于该假设的经验损失，即实际数据点的输出与我预测的输出之间的距离。
- en: to be based on that particular，hypothesis and if all you're trying to。do is
    minimize cost meaning minimizing，the loss in this case then the result is。going
    to be that you might over fit，that to minimize cost you're going to。try and find
    a way to perfectly match，all the input data and that might happen。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要基于该特定假设，如果你所做的只是最小化成本，也就是说在这种情况下最小化损失，那么结果可能是你可能会过拟合，为了最小化成本，你会尝试找到一种方法来完美匹配所有输入数据，这可能会发生。
- en: as a result of overfitting on that，particular input data so in order to。address
    this you could add something to，the cost function what counts as cost。will not
    just loss but also some measure，of the complexity of the hypothesis。there were
    the complexity of the，hypothesis is something that you would。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在特定输入数据上过拟合，因此为了解决这个问题，你可以在成本函数中添加一些内容，什么算作成本，不仅仅是损失，还有假设复杂性的某种衡量标准。假设的复杂性是我们在考虑整体成本时需要考虑的内容。
- en: need to define for you know how，complicated does our line look this is。sort
    of an Occam's razor style approach，where we want to give preference to a。simpler
    decision boundary like a，straight line for example some simpler。curve as opposed
    to something far more，complex that might represent the。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 需要定义你知道我们的线看起来多复杂，这是一种奥卡姆剃刀风格的方法，我们希望优先考虑更简单的决策边界，比如一条直线，举个例子，某些更简单的曲线，而不是某些更复杂的东西。
- en: training data better but might not，generalize as well we'll generally say。that
    a simpler solution is probably the，better solution and probably the one。that is
    more likely to generalize well，to other inputs so we measure what the。loss is
    but we also measure the，complexity and now that all gets taken。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据可能更好，但我们通常会说它可能不够泛化。一个更简单的解决方案可能是更好的解决方案，更有可能在其他输入上良好泛化，所以我们衡量损失是什么，但我们也衡量复杂性，现在这一切都被考虑进来了。
- en: into account when we consider the，overall cost that yes something might。have
    less loss if it better predicts the，training data but if it's much more。complex
    it still might not be the best，option that we have and we need to come。up with
    some balance between loss and，complexity and for that reason you'll。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，某些东西可能在更好地预测训练数据时具有更少的损失，但如果它复杂得多，它仍然可能不是我们拥有的最佳选项，我们需要在损失和复杂性之间找到一些平衡，因此你会发现。
- en: often see this represented as，multiplying the complexity by some，parameter that
    we have to choose。parameter lambda in this case where，we're saying you know if
    lambda is a。greater value then we really want to，penalize more complex hypotheses。whereas
    if lambda is smaller we're gonna，penalize more complex hypotheses a。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通常将其表示为，将复杂性乘以某个我们必须选择的参数。在这种情况下的参数 lambda，我们说如果 lambda 的值更大，我们确实想要对更复杂的假设施加更多惩罚。相反，如果
    lambda 较小，我们将对更复杂的假设施加较少惩罚。
- en: little bit and it's up to the machine，learning programmer to decide where they。want
    to set that value of lambda for how，much do I want to penalize a more。complex
    hypothesis that might fit the，data a little better and again there's。no one right
    answer to a lot of these，things the depending on the data set。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点取决于机器学习程序员决定他们希望在哪里设置 lambda 的值，以便决定我想对一个可能更好地拟合数据的更复杂假设施加多少惩罚。再一次，这些问题没有一个正确的答案，这取决于数据集。
- en: depending on the data you have available，to you and the problem you're trying
    to。solve your choice of these parameters，may vary and you may need to experiment。a
    little bit to figure out what the，right choice of that is ultimately going。to
    be this process then of considering，not only loss but also some measure of。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你可用的数据和你要解决的问题，这些参数的选择可能会有所不同，你可能需要稍微实验一下，以找出最终合适的选择。这一过程不仅考虑损失，还考虑某种复杂性的衡量。
- en: the complexity is known as，regularization regularization is the。process of penalizing
    hypothesis that is，more complex，in order to favor a simpler hypothesis。than is
    more likely to generalize well，more likely to be able to apply to other。situations
    that are dealing with other，input points unlike the ones that we've。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂性被称为正则化，正则化是对更复杂的假设施加惩罚的过程，以偏向于更简单的假设，后者更可能很好地进行泛化，更有可能应用于处理其他输入点的其他情况，而不是我们已知的。
- en: necessarily seen before so oftentimes，you'll see us add some regularizing term。to
    what we're trying to minimize it in，order to avoid this problem of。overfitting
    now another way of making，sure we don't over fit is to run some。experiments and
    to see whether or not we，are able to generalize our model that。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 通常没有见过，所以我们常常会在我们试图最小化的内容中添加一些正则化项，以避免过拟合的问题。确保不发生过拟合的另一种方法是进行一些实验，以查看我们是否能够对我们的模型进行泛化。
- en: we've created to other data sets as well，and it's for that reason that oftentimes。when
    you're doing a machine learning，experiment when you've got some data and。you want
    to try and come up with some，function that predicts given some input。what the
    output is going to be you don't，necessarily want to do your training on。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的模型在其他数据集上的表现，因此，当你进行机器学习实验时，通常你有一些数据，并且你想尝试创建一个函数，根据某些输入预测输出时，你不一定希望在。
- en: all of the data you have available to，you that you could employ a method known。as
    holdout cross-validation，we're in holdout cross-validation we。split up our data
    we split up our data，into a training set and a testing set。the training set is
    the set of data that，we're going to use to train our machine。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你可用的所有数据可以使用一种称为留出交叉验证的方法。在留出交叉验证中，我们将数据分为训练集和测试集。训练集是我们将用于训练机器的数据集。
- en: learning model and the testing set is，the set of data that we're going to use。in
    order to test to see how well our，machine learning model actually。performed so
    the learning happens on the，training set we figure out what the。parameters should
    be we figure out what，the right model is and then we see all。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 学习模型，而测试集是我们将用于测试我们机器学习模型实际表现的数据集。因此，学习发生在训练集上，我们确定参数应该是什么，找出正确的模型，然后我们看看。
- en: right now that we've trained the model，see how well it does at predicting。things
    and inside of the testing set，some set of data that we haven't seen。before and
    the hope then is that we're，going to be able to predict the testing。set pretty
    well if we're able to，generalize based on the training data。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了模型，看看它在预测事物方面的表现如何，并且在测试集中，某些数据集是我们之前没有见过的。希望是，我们能够很好地预测测试集，如果我们能够基于训练数据进行泛化。
- en: that's available to us if we've overfit，the training data though and we're not。able
    to generalize well then when we，look at the testing set it's likely。going to be
    the case that we're not，going to predict things in the testing。set nearly as effectively
    so this is one，method of cross-validation validating to。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对训练数据过拟合而无法很好地泛化，当我们查看测试集时，通常情况是我们在测试集上无法有效预测，因此这是验证模型泛化能力的一种交叉验证方法。
- en: make sure that the work we have done is，actually going to generalize to other。data
    sets as well and there are other，statistical techniques we can use as。well one
    of the downsides of this just，hold out cross-validation is if you say。I just like
    split it 50/50 I train using，50% of the data and test using the other。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 确保我们所做的工作能够推广到其他数据集上，还有其他统计技术可以使用。其中一个缺点是，如果你仅仅将数据以50/50分割，使用50%的数据进行训练，而使用另一半进行测试。
- en: 50% or you could choose other，percentages as well is，that there's a fair amount
    of data that。I am now not using to train that I might，be able to get a better
    model as a。result for example so one approach is，known as k-fold cross-validation
    in。k-fold cross-validation rather than just，divide things into two sets and run
    one。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 50% 或者你也可以选择其他百分比，因为有相当多的数据我现在没有用来训练，可能因此能够得到一个更好的模型。例如，一种方法被称为k折交叉验证。在k折交叉验证中，与其将数据仅分为两组并运行其中一个...
- en: experiment we divide things into K，different sets so maybe I divide things。up
    into 10 different sets and then run，ten different experiments so if I split。up
    my data into ten different sets of，data then what I'll do is each time for。each
    of my 10 experiments I will hold，out one of those sets of data where I'll。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 实验中我们将数据分为K个不同的集合，可能我将数据分为10个不同的集合，然后运行十个不同的实验。如果我将数据分成十个不同的数据集合，那么我每次在十个实验中将保留其中一个数据集合。
- en: say let me train my model and these 9，sets and then test to see how well it。predicts
    onset number 10 and then pick，another set of 9 sets to train on and。then test
    it on the other one that I，held out where each time I train the。model on everything
    - the one set that，I'm holding out and then test to see how。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 说让我训练我的模型和这9个集合，然后测试它对第10个的预测效果如何，再选择另一组9个集合进行训练，然后在我保留的那个集合上测试，每次我都在所有数据上训练模型——我保留的那一组，然后测试效果如何。
- en: well our model performs on the test that，I did hold out and what you end up。getting
    is like ten different results，ten different answers for how accurately。our model
    worked and oftentimes you，could just like take the average of。those 10 to get
    an approximation for how，well we think our model performs overall。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型在我保留的测试集上的表现如何，最终得到的是十个不同的结果，十个不同的答案，显示我们的模型的准确性，通常你可以取这十个结果的平均值来估算我们认为模型整体表现如何。
- en: but the key idea is separating the，training data from the testing data。because
    you want to test your model on，data that is different from what you。trained the
    model on because the，training you want to avoid overfitting。you want to be able
    to generalize and，the way you test whether you're able to。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 关键思想是将训练数据与测试数据分开。因为你希望在与训练模型的数据不同的数据上测试你的模型，以避免过拟合。你希望能够泛化，而测试你是否能够做到这一点的方法是...
- en: generalize them is by looking at some，data that you haven't seen before and。seeing
    how well we're actually able to，perform and so if we want to actually。implement
    any of these techniques inside，of a programming language like Python。number of
    ways we could do that we could，write this from scratch on our own but。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化的一个方法是查看一些你之前未见过的数据，看看我们实际能够达到的效果。如果我们想在像Python这样的编程语言中实现任何这些技术，有很多方法可以做到。我们可以自己从头开始编写，但...
- en: there are libraries out there that allow，us to take advantage of existing。implementations
    of these algorithms that，we can use the same types of algorithms。in a lot of different
    situations and so，there's a library very popular one known。as scikit-learn which
    allows us in，python to be able to very quickly get。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些库允许我们利用现有算法的实现，以便在很多不同的情况下使用相同类型的算法。因此，有一个非常流行的库叫做scikit-learn，它让我们在Python中能够非常快速地获得。
- en: set up with a lot of these different，machine learning models this library has。already
    written an algorithm for nearest，neighbor classification for doing。perceptron
    learning for doing a bunch of，other types of inference and supervised。learning
    that we haven't yet talked，about but using it we can begin to try，actually。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 设置许多不同的机器学习模型，这个库已经为**最近邻分类**、**感知机学习**以及许多其他我们尚未讨论的推理和监督学习类型编写了算法，但使用它我们可以开始尝试。
- en: how these methods work and how，accurately they perform so let's go。ahead and
    take a look at one approach to，try to solve this type of problem。alright so I'm
    first going to pull up，bank notes dot CSV which is a whole。bunch of data provided
    by UC Irvine，which is information about various。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这些方法是如何工作的，以及它们的准确性表现如何，所以我们来看一种尝试解决这类问题的方法。好的，我首先会打开**bank_notes.csv**，这是由**加州大学尔湾分校**提供的一整套数据，关于各种银行票据的信息。
- en: different bank notes so people took，pictures of various different bank notes。and
    measured various different，properties of those Bank notes and in。particular some
    human categorized each，of those Bank notes as either a。counterfeit banknote or
    as not，counterfeit and so what you're looking。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的银行票据，因此人们拍摄了各种不同的银行票据，并测量了这些银行票据的不同属性，特别是有一些人将每一张银行票据分类为伪钞或非伪钞。所以你正在查看的就是这些。
- en: at here is each row represents one bank，note this is formatted as a CSV。spreadsheet
    we're just comma separated，values separating each of these various。different fields
    we have four different，input values for each of these data。points just information
    some measurement，that was made on the bank note and what。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里每一行代表一张**银行票据**，这个数据被格式化为**CSV**电子表格，我们只是用逗号分隔这些不同的字段。我们为每一个数据点有四个不同的输入值，都是一些对银行票据所做测量的信息。
- en: those measurements exactly are aren't as，important is the fact that we do have。access
    to this data but more importantly，we have access for each of these data。points
    to a label where 0 indicates，something like this was not a，counterfeit bill meaning
    it was an。authentic bill and a data point labeled，1 means that it is a counterfeit
    bill at。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这些测量值并不是那么重要，重要的是我们确实能够访问这些数据，更重要的是，我们可以为每个数据点访问一个标签，其中0表示这不是一张伪钞，意味着它是一张**真实钞票**，而标记为1的数据点则表示它是一张伪钞。
- en: least according to the human researcher，who labeled this particular data so
    we。have a whole bunch of data representing，a whole bunch of different data points。each
    of which has these various，different measurements that were made on。that particular
    bill and each of which，has an output value 0 or 1 0 meaning it。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 至少根据给这组特定数据贴标签的人类研究者的说法，我们有一整套数据，代表着大量不同的数据点，每个数据点都有这些不同的测量，且每个数据点都有一个输出值0或1，0表示它。
- en: was a genuine bill 1 meaning it was a，counterfeit bill and what we would like。to
    do is use supervised learning to，begin to predict or model some sort of。function
    that can take these four values，as input and predict what the output。would be
    we want our learning algorithm，to find some sort of pattern that is。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个真实的钞票1，意味着它是一张**伪钞**，我们希望做的是使用**监督学习**，开始预测或建模某种函数，该函数可以将这四个值作为输入并预测输出是什么。我们希望我们的学习算法能够找到某种模式。
- en: able to predict based on these，measurements something you could measure。just
    by taking a photo of a bill predict，whether that bill is authentic or。whether
    that bill is counterfeit and so，how can we do that，well I'm first going to open
    up bank。notes 0 PI and see how it is that we do，this I'm first importing a lot
    of things。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些测量能够预测的东西，比如说你只需拍一张钞票的照片，就可以预测那张钞票是否真实，或者是否为伪钞。那么我们该如何做到这一点呢？首先我要打开**bank_notes_0.py**，看看我们该如何实现这一点。我首先导入许多东西。
- en: from scikit-learn but importantly I'm，going to set my model equal to the。perceptron
    model which is one of those，models that we talked about before we're。just going
    to try and figure out some，setting of weights that is able to。divide our data
    into two different，groups，then I'm going to go ahead and read data。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn，但重要的是我将把我的模型设为**感知机模型**，这是我们之前讨论过的模型之一。我们将尝试找出一些权重设置，以便将数据划分为两个不同的组，然后我会继续读取数据。
- en: in for my file from bank notes CSV and，basically for every row I'm going to。separate
    that row into the first four，values of that row which is the evidence。for that
    row and then the label where if，the if the final column in that row is a。zero
    the label is authentic and，otherwise it's going to be counterfeit。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我的银行票据CSV文件，我基本上会将每一行分开为前四个值，这一行的证据，然后是标签。如果这一行的最后一列是零，标签为真品，否则为假冒品。
- en: so I'm effectively reading data in from，the CSV file dividing into a whole bunch。of
    rows where each row has some evidence，those four input values that are going。to
    be inputs to my hypothesis function，and then the label the output whether it。is
    authentic or counterfeit that is the，thing that I am then trying to predict。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我实际上是从CSV文件中读取数据，将其划分为许多行，每一行都有一些证据，这四个输入值将作为我的假设函数的输入，然后是标签，输出是否是真品或假冒品，这是我试图预测的内容。
- en: so the next step is that I would like to，split up my dataset into a training
    set。and a testing set some set of data that，I would like to train my machine。learning
    model on and some set of data，that I would like to use to test that。model see
    how well it performed so what，I'll do is I'll go ahead and figure out。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所以下一步是我想将我的数据集划分为训练集和测试集，一些数据用于训练我的机器学习模型，一些数据用于测试该模型，看看它的表现如何。所以我会先确定。
- en: length of the data how many data points，do I have I'll go ahead and take half
    of。them save that number is a number called，holdout that is how many items I'm
    going。to hold out for my dataset to save for，the testing phase I'll randomly shuffle。the
    data so it's in some random order，and then I'll say my testing set will be。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的长度有多少数据点，我将取一半。这个数字叫做**保留集**，这是我将为测试阶段保留的数据项数量。我会随机打乱数据，使其顺序随机，然后我会说我的测试集将是。
- en: all of the data up to the holdout so，I'll take hold up many data items and。that
    will be my testing set my training，data will be everything else the。information
    that I'm going to train my，model on and then I'll say I need to。divide my training
    data into two，different sets I need to divide it into。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据都到保留集为止，我将取出许多数据项，这将是我的测试集，而我的训练数据将是其他所有信息，我将用来训练我的模型，然后我会说我需要将我的训练数据划分为两个不同的集合。
- en: my X values where X here represents the，inputs so the X values the X values
    that。I'm going to train on are basically for，every row in my training set I'm
    gonna。get the evidence for that row those four，values where it's basically a vector
    of。four numbers where that is going to be，all of the input and then I need the
    Y。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我的X值，X在这里代表输入，因此我要训练的X值基本上是每一行的证据，这四个值构成一个四维向量，这是所有输入，然后我需要Y。
- en: values what are the outputs that I want，to learn from the labels that belong
    to。each of these various different input，points well that's going to be the same。thing
    for each row in the training data，but this time I take that row and get。what its
    label is whether it is，authentic or counterfeit so I end up。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我想从每个输入点的标签中学习的输出值是什么？对于训练数据中的每一行也是一样，但这次我会取这一行并获取它的标签，看它是**真品**还是**假冒品**，所以我最终得到了。
- en: with one list of all of these vectors of，my input data and one list which follows。the
    same order but as all of the labels，that correspond with each of those。vectors
    and then to train my model which，in this case is just this。Sep Tron model I just
    called model dot，fit pass in the training data and what。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一组所有输入数据的向量列表和一组按相同顺序排列的标签列表，然后训练我的模型，这里使用的是**Sep Tron**模型，我只是调用`model.fit`，传入训练数据。
- en: the labels for those training data are，and scikit-learn will take care of。fitting
    the model will do the entire，algorithm for me and then when it's done。I can then
    test to see how well that，model performed so I can say let me get。all of these
    input vectors for what I，want to test on，so for each row in my testing data set。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这些训练数据的标签是什么，`scikit-learn`将处理模型拟合，整个算法将为我完成，然后当它完成后，我可以测试模型的表现，所以我可以说让我获取所有输入向量，用于我想要测试的内容，每一行在我的测试数据集中。
- en: go ahead and get the evidence and the，y-values those are what the actual。values
    were for each of the rows and the，testing data set what the actual label。is but
    then I'm going to generate some，predictions I'm gonna use this model and。try and
    predict based on the testing，output is，and my goal then is to now compare why。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 继续获取证据和y值，那些是每一行的实际值，以及测试数据集中，实际标签是什么，但接下来我要生成一些预测，我将使用这个模型，尝试预测基于测试，输出是什么，我的目标是现在比较原因。
- en: testing with predictions I want to see，how well my predictions based on the。model
    actually reflect what the Y values，were what the output is that we're。actually
    labeled because I now have this，label data I can assess how well the。algorithm
    worked and so now I can just，compute how well we did I'm going to。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 测试与预测，我想看看我的预测基于模型，实际上与Y值的反映如何，输出是什么，这些实际上是标记的，因为我现在有这个标签数据，我可以评估算法的效果，所以现在我可以计算我们做得多好，我要。
- en: this zip function basically just lets me，look through two different lists one
    at。one by one at the same time，so for each actual value and for each。predicted
    value if the actual is the，same thing as what I predicted I'll go。ahead and increment
    the counter by one，otherwise I'll increment my incorrect。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个zip函数基本上让我同时查看两个不同的列表，因此对于每个实际值和每个预测值，如果实际值与我预测的相同，我将递增计数器，否则我将递增我的错误计数。
- en: counter by one and so at the end that，can print out here are the results。here's
    how many I got right here's how。![](img/789c854420af1d2fd8944135cd97bb86_3.png)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 计数器加一，所以到最后可以打印出结果。这是我正确预测的数量。![](img/789c854420af1d2fd8944135cd97bb86_3.png)
- en: many I got wrong and here was my overall，accuracy for example so I can go ahead。and
    run this I can run Python banknote 0，dot pi and it's going to Train on half。the
    data set and then test on half the，data set and here are the results for my。perceptron
    model in this case it，correctly was able to classify 679 bills。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我犯了多少错误，这里是我的总体准确率，例如，所以我可以继续进行。我可以运行Python banknote 0，dot pi，它将对一半的数据集进行训练，然后对另一半的数据集进行测试，这里是我在这个案例中，感知器模型的结果，它能正确分类679张票据。
- en: as correctly eyes are authentic or，counterfeit and incorrectly classified。seven
    of them for an overall accuracy of，close to 99% accurate so on this。particular
    dataset using this perceptron，model we were able to predict very well。![](img/789c854420af1d2fd8944135cd97bb86_5.png)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 作为正确的，眼睛是正品或伪造，并错误分类七张，整体准确率接近99%。因此，在这个特定数据集上，使用这个感知器模型，我们能够很好地预测。![](img/789c854420af1d2fd8944135cd97bb86_5.png)
- en: what the output was going to be and we，can try different models to the。scikit-learn
    makes it very easy just to。![](img/789c854420af1d2fd8944135cd97bb86_7.png)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 预期输出是什么，我们可以尝试不同的模型，scikit-learn使得这非常简单。![](img/789c854420af1d2fd8944135cd97bb86_7.png)
- en: swap out one model for another model so，instead of the perceptron model I can。use
    the support vector machine using the，SVC otherwise known as a support vector，class。fire
    using a support vector machine to，classify things into two different。groups and
    now see alright how well does，this perform and alright this time we。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 替换一个模型为另一个模型，因此，不是感知器模型，我可以使用支持向量机，使用SVC，也就是支持向量类。利用支持向量机将事物分类为两个不同的组，现在看看，表现如何，这次我们。
- en: were able to correctly predict 682 and，incorrectly predicted for for accuracy，of
    99。4% and we could even try the K，neighbors classifier as the model。instead and
    this takes a parameter and，neighbors for how many neighbors you。want to look at
    let's just look at one，neighbor the one nearest neighbor and。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 能够正确预测682个，并且，错误预测准确率为99.4%。我们甚至可以尝试K邻居分类器作为模型。这需要一个参数，邻居的数量，你想查看多少邻居。我们就看一个，最近邻居。
- en: use that to predict go ahead and run，this as well and it looks like based on。the
    K neighbors pasta fire looking at，just one neighbor we were able to。correctly
    classify 685 data points，incorrectly classified one maybe let's。try three neighbors
    instead instead of，just using one neighbor do more of a K。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个来进行预测，继续运行这个，看来基于K邻居的设置，仅查看一个邻居，我们能够正确分类685个数据点，错误分类一个，也许我们试试三个邻居，而不是仅使用一个邻居做更多的K。
- en: nearest neighbors approach where I look，at the three nearest neighbors and see。![](img/789c854420af1d2fd8944135cd97bb86_9.png)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最近邻居方法，我查看三个最近邻居，看看。![](img/789c854420af1d2fd8944135cd97bb86_9.png)
- en: how that performs and that one in this，case seems to have gotten a hundred。percent
    of all of the predictions，correctly described as either authentic。banknotes or
    as counterfeit banknotes，and you know we could run these。experiments multiple
    times because I'm，randomly reorganizing the data every。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例中，它似乎准确地将所有预测的百分之百描述为**真实的**纸币或**伪造的**纸币，我们可以多次进行这些实验，因为我每次都在随机重组测试组中的一半数据。
- en: time we're technically training these on，slightly different data sets and so
    you。might want to run multiple experiments，to really see how well they're actually。going
    to perform but in short they all，perform very well and while some of them。perform
    slightly better than others here，that might not always be the case for。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们实际上是在稍微不同的数据集上进行训练，因此你可能想运行多个实验，看看它们的表现如何，但简而言之，它们的表现都很好，尽管其中一些的表现略好于其他，但这并不总是适用。
- en: every data set but you can begin to test，now by very quickly putting together。these
    machine learning models using，scikit-learn to be able to train on some。training
    set and then test on some，testing set as well and this splitting。up into training
    groups and testing，groups and testing happens so often that。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 但是你现在可以开始测试，通过快速组装这些机器学习模型，使用scikit-learn在一些训练集上进行训练，然后在一些测试集上进行测试，分成训练组和测试组，测试如此频繁。
- en: scikit-learn has functions built-in for，trying to do it I did it all by hand。just
    now but if we take a look at bank，notes one we take advantage of some。![](img/789c854420af1d2fd8944135cd97bb86_11.png)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn内置了尝试实现的功能，我刚才都是手动完成的，但如果我们看一下纸币案例，就可以利用一些。
- en: other features that exist in，scikit-learn where we can really，simplify a lot
    of our logic that there。is a function built-in to scikit-learn，called train test
    split which will。automatically split data into a training，group and a testing
    group I just have to。say what proportion should be in the，testing group something
    like point five。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn中还有其他功能，可以简化我们的逻辑，内置了一个函数，称为train test split，它会自动将数据分成训练组和测试组，我只需指定测试组的比例，比如0.5。
- en: half the data inside the testing group，then I can fit the model on the training。data
    make the predictions on the testing，data and then just count up and。scikit-learn
    have some nice methods for，just counting up how many times are。data match the
    predictions how many，times our testing data didn't match the。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我可以在训练数据上拟合模型，对测试数据进行预测，然后进行统计，scikit-learn有一些很好的方法来统计我们的数据与预测的匹配次数，以及测试数据未匹配的次数。
- en: predictions so very quickly you can，write programs with not all that many。lines
    of code it's maybe like 40 lines，of code to get through all these。predictions
    and then as a result see how，well we're able to do so these types of。libraries
    can allow us without really，knowing the implementation details of。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可以非常快速地编写程序，只需不多的代码行，可能只需40行代码就能完成所有预测，然后根据结果查看我们的表现，这些类型的库使我们能够在不真正了解这些算法的实现细节的情况下，以非常实用的方式使用这些算法来解决这些问题。
- en: these algorithms to be able to use the，algorithms in a very practical way to
    be。able to solve these types of problems so，that then was supervised learning
    this。task of given a whole set of data some，input-output pairs we would like to。learn
    some function that map's those，inputs to those outputs but turns out。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这就是监督学习的任务，给定一整套数据一些输入-输出对，我们希望学习一个函数，将这些输入映射到这些输出，但结果是。
- en: there are other forms of learning as，well and another popular type of machine。learning
    especially nowadays is known as，reinforcement learning and the idea of。reinforcement
    learning is rather than，just being given a whole data set at the。beginning of
    input-output paradism，reinforcement learning is all about，learning from experience
    in。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他形式的学习，另一个流行的机器学习类型，尤其是现在被称为**强化学习**，强化学习的思想不是一开始就给定一整套输入-输出对，强化学习全在于从经验中学习。
- en: reinforcement learning our agent whether，it's like a physical robot that's trying。to
    make actions in the world or just，some virtual agent that is a program。running
    somewhere our agent is going to，be given a set of rewards or punishments。in the
    form of like numerical values but，you can think of them as reward or。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习我们的代理无论是，像一个试图在世界中进行动作的物理机器人，还是某个在某处运行的虚拟程序代理。我们的代理将会获得一系列奖励或惩罚，形式是数值，但你可以把它们视为奖励或。
- en: punishment and based on that it learns，what actions to take in the future that。our
    agent our AI will be put in some，sort of environment it will make some。actions
    and based on the actions that it，makes it learns something it either gets。a reward
    when it does something well it，gets a punishment when it does something。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 惩罚，基于此它学习未来该采取什么动作。我们的代理，我们的AI将被放置在某种环境中，它将进行一些动作，基于它所做的动作，它学习一些东西。当它做得好时，它会获得奖励，而当它做得不好的时候，它会受到惩罚。
- en: poorly and it learns what to do or what，not to do in the future。based on those
    individual experiences，and so what this will often look like is。it will often
    start with some agent some，AI which might again be a physical robot。if you're
    imagining a physical robot，moving around but it can also just be a。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 学习如何做或者在未来不该做什么，基于这些个体经验。这通常的样子是，它往往会从某个代理开始，某个AI，可能再次是一个物理机器人。如果你想象的是一个四处移动的物理机器人，但它也可以仅仅是一个。
- en: program and our agent is situated in，their environment where the environment。is
    where they're going to make their，actions and it's what's going to give。them rewards
    or punishments for various，actions that they're in so for example。the environment
    is going to start off by，putting our agent inside of a state our。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代理位于他们的环境中，环境就是他们将进行动作的地方，也是给予他们各种动作奖励或惩罚的地方。例如，环境将开始将我们的代理放置在一个状态中。
- en: agent has some state that in a game，might be the state of the game that the。agent
    is playing in a world that the，agent is exploring might be some。position inside
    of the grid representing，age，in some sort of state and in that state。the agent
    needs to choose to take in，action the agent likely has multiple。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 代理在游戏中的某个状态，可能是代理正在玩的游戏状态，代理正在探索的世界可能是某个在网格中表示的，状态中的位置，并且在该状态中，代理需要选择采取一个动作。代理可能有多个。
- en: actions that can choose from but they，pick an action so they may they take an。action
    in a particular state and as a，result of that the agent will generally。get two
    things in response we model them，the agent gets a new state that they。find themselves
    in after being in this，state taking one action they end up in。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 可以选择的动作，但它们，选择一个动作，所以它们可能在某个特定状态下采取一个动作，作为结果，代理通常会得到两个响应，我们将其建模为，代理在采取一个动作后会找到一个新的状态。
- en: some other state and they're also given，some sort of numerical reward positive，thing。negative
    generally meaning they did，something bad they received some sort of。punishment
    and that is all the，information the agent has it's told what。state it's in it
    makes some sort of，action and based on that it ends up in。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 其他状态，它们也会获得某种数值奖励，正面，负面通常意味着它们做了某些不好的事情，收到了某种惩罚。这就是代理所拥有的所有信息，它被告知自己处于哪个状态，它做出某种行动，基于此它最终会处于。
- en: another state and it ends up getting，some particular reward and it needs to。learn
    based on that information what，actions to begin to take in the future。and so you
    could imagine generalizing，this to a lot of different situations。this is oftentimes
    how you train if，you've ever seen those robots that are。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个状态，然后它最终获得某种特定的奖励，基于这些信息，它需要学习未来该采取什么动作。因此，你可以想象将其推广到很多不同的情况。这通常就是你训练的方式，如果你见过那些机器人，它们是。
- en: now able to like walk around sort of the，way humans do it would be quite。difficult
    to program the robot in，exactly the right way to get it to walk。the way humans
    do you could instead，train it through reinforcement learning。give it some sort
    of numerical reward，every time it does something good like。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在能够像人类一样走动，这将非常困难，要准确编程机器人让它像人类那样走动。你可以通过强化学习来训练它，每次它做得好时给予它某种数值奖励。
- en: take steps forward and punish it every，time it does something bad like fall。over
    and then let the AI just learn，based on that sequence of rewards based。on trying
    to take various different，actions you can begin to have the agent。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 采取积极的措施，每次它做错事，比如摔倒时就惩罚它。然后让AI根据这个奖励的序列进行学习，基于尝试采取各种不同的行动，你就可以开始让代理进行学习。
