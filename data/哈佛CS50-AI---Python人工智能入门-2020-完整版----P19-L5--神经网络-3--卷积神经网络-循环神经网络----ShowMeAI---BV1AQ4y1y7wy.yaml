- en: 哈佛CS50-AI ｜ Python人工智能入门(2020·完整版) - P19：L5- 神经网络 3 (卷积神经网络，循环神经网络) - ShowMeAI
    - BV1AQ4y1y7wy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 哈佛CS50-AI ｜ Python人工智能入门(2020·完整版) - P19：L5- 神经网络 3 (卷积神经网络，循环神经网络) - ShowMeAI
    - BV1AQ4y1y7wy
- en: regions inside of the image and now we，can put all of these ideas together。pooling
    image convolution and neural，networks all together into another type。of neural
    network called a convolutional，neural network or a CNN which is a。neural network
    that uses this，convolution step usually in the context。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 图像内部的区域，现在我们可以将所有这些概念结合在一起。将图像池化、卷积和神经网络结合成另一种神经网络，称为卷积神经网络或CNN，这是一个使用卷积步骤的神经网络，通常是在上下文中。
- en: of analyzing an image for example and so，the way that a convolutional neural。network
    works is that we start with some，sort of input image some grid of pixels。![](img/ea1acf5e96ec75cca40fe685daa4d48e_1.png)
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，分析一张图像的方式是，卷积神经网络的工作原理是从某种输入图像开始，某个像素网格。![](img/ea1acf5e96ec75cca40fe685daa4d48e_1.png)
- en: but rather than immediately put that，we've seen before，we'll start by applying
    a convolution。step where the convolution step involves。![](img/ea1acf5e96ec75cca40fe685daa4d48e_3.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 但不是立即放置我们之前看到的，我们将首先应用一个卷积步骤，其中卷积步骤涉及。![](img/ea1acf5e96ec75cca40fe685daa4d48e_3.png)
- en: applying some number of different image，filters to our original image in order。to
    get what we call a feature map the，result of applying some filter to an。image
    and we could do this once but in，general we'll do this multiple times。getting
    a whole bunch of different，feature Maps each of which might extract。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 将一些不同的图像滤波器应用于我们的原始图像，以获取我们称之为特征图的结果，即将某个滤波器应用于图像的结果，我们可以做一次，但通常会做多次，得到一堆不同的特征图，每个特征图可能提取。
- en: some different relevant feature out of，the image some different important。characteristic
    of the image that we，might care about using in order to。calculate what the result
    should be and，in the same way that when we to when we。train neural networks we
    can train，neural networks，learn the weights between particular。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 从图像中提取一些不同的相关特征，图像的一些不同重要特征，我们可能会关心这些特征，以计算结果应该是什么，和我们训练神经网络时一样，我们可以训练神经网络，学习特定之间的权重。
- en: units inside of the neural networks we，can also train neural networks to learn。what
    those filters should be what the，values of the filters should be in order。to get
    the most useful most relevant，information out of the original image。just by figuring
    out what setting of，those filter values the values inside of。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络内部，我们还可以训练神经网络以学习这些滤波器应该是什么，滤波器的值应该是什么，以便从原始图像中提取出最有用、最相关的信息，仅通过找出这些滤波器值的设置。
- en: that kernel results in minimizing the，loss function minimizing how poorly our。hypothesis
    actually performs in figuring，out the classification of a particular。image for
    example so we first apply this，convolution step get a whole bunch of。these various
    different feature Maps but，these feature maps are quite large right。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个核的结果是最小化损失函数，最小化我们的假设在识别特定图像分类时表现的多差。例如，我们首先应用这个卷积步骤，得到一堆不同的特征图，但这些特征图相当大。
- en: there's a lot of pixel values that，happen to be here and so a logical next。step
    to take is a pooling step where，we've reduced the size of these images。by using
    max pooling for example，extracting the maximum value from any。particular region
    there are other，pooling methods that exist as well。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多像素值，因此接下来的逻辑步骤是池化步骤，我们通过使用最大池化等方法减少这些图像的大小，从特定区域提取最大值，还有其他存在的池化方法。
- en: depending on the situation you could use，something like average pooling we're。instead
    of taking the maximum value from，a region you take the average value from。a region
    which has its uses as well，but in fact what pooling will do is。it'll take these
    feature maps and reduce，their dimensions so that we end up with。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 根据情况，你可以使用像平均池化这样的东西，取一个区域的平均值，而不是取最大值，这也有其用处，但实际上，池化会将这些特征图的维度降低，最终我们得到了。
- en: smaller grids with fewer pixels and this，then is going to be easier for us to。deal
    with it's gonna mean fewer inputs，that we have to worry about and it's。also going
    to mean we're more resilient，more robust against potential movements。of particular
    values just by one pixel，when ultimately we don't really don't。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 更小的网格，像素更少，这样我们处理起来就更容易了，意味着我们要担心的输入会更少，也意味着我们对特定值潜在的移动会更有韧性、更强健，即使只是一个像素的变化，而最终我们并不真的。
- en: care about those one pixel differences，that might arise in the original image。and
    now after we've done this pooling，step now we have a whole bunch of values。that
    we can then flatten out and just，put it into a more traditional neural。network
    so we go ahead and flatten it，and then we end up with a traditional。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 关注原始图像中可能出现的一像素差异。在我们完成池化步骤后，我们现在拥有一堆值。然后我们可以将这些值展平，并放入一个更传统的神经网络中，因此我们继续展平它，然后得到一个传统的。
- en: neural network that has one input for，each of these values in each of these。resulting
    feature Maps after we do the，convolution and after we do the pooling。step and
    so this then is the general，structure of a convolutional Network we。begin with
    the image apply convolution，apply pooling flatten the results and。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络对每一个这些值有一个输入，这些值来自每个结果特征图，经过卷积和池化步骤后，这就是卷积网络的一般结构。我们从图像开始，应用卷积，应用池化，展平结果。
- en: then put that into a more traditional，neural network that might itself have。hidden
    layers you can have deep，convolutional networks that have hidden。layers in between
    this flattened layer，and the eventual output to be able to。calculate various different
    features of，those values but this then can help us，to be able to use。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将其放入一个更传统的神经网络，该网络本身可能有隐藏层。你可以拥有深度卷积网络，在这个展平层和最终输出之间有隐藏层，以便能够计算这些值的各种不同特征，但这可以帮助我们能够使用。
- en: convolution and pooling to use our，knowledge about the structure of an。image
    to be able to get better results，to be able to train our networks faster。in order
    to better capture particular，parts of the image and there's no reason。necessarily
    why you can only use these，steps once in fact in practice you'll。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用卷积和池化利用我们对图像结构的知识，以便获得更好的结果，从而能更快地训练我们的网络，以更好地捕捉图像的特定部分。并且没有理由一定只能使用这些步骤一次，实际上在实践中你会。
- en: often use convolution and pooling，multiple times in multiple different。steps
    see what you might imagine doing。![](img/ea1acf5e96ec75cca40fe685daa4d48e_5.png)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 多次在多个不同的步骤中使用卷积和池化，看看你可能想做什么。![](img/ea1acf5e96ec75cca40fe685daa4d48e_5.png)
- en: is starting with an image first applying，convolution to get a whole bunch of
    maps。then applying pooling then applying，convolution again because these maps
    are。still pretty big you can apply，convolution to try and extract relevant。features
    out of this result then take，those results apply pooling in order to。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先从图像开始，应用卷积以获得一堆映射。然后应用池化，再次应用卷积，因为这些映射仍然相当大。你可以应用卷积来尝试提取相关特征，然后将这些结果应用池化。
- en: reduce their dimensions and then take，that and feed it into a neural network。that
    maybe has fewer inputs so here I，have two different convolution and。pooling steps
    I do convolution and，pooling once and then I do convolution。and pooling a second
    time each time，extracting useful features from the。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 减少它们的维度，然后将其输入到一个神经网络中。这个网络可能有更少的输入，因此我这里有两个不同的卷积和池化步骤。我先进行一次卷积和池化，然后再进行第二次卷积和池化，每次都从中提取有用的特征。
- en: layer before it each time using pooling，to reduce the dimensions of what you're。ultimately
    looking at and the goal now，of this sort of model is that in each of。these steps
    you can begin to learn，different types of features of the。original image that
    may be in the first，step you learn very low-level features。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 每次在前一层使用池化来减少你最终查看的内容的维度，现在这种模型的目标是，在每个步骤中，你可以开始学习原始图像的不同类型特征，在第一步中你学习非常低级的特征。
- en: just learn and look for features like，edges and curves and shapes because。based
    on pixels and their neighboring，values you can figure out alright what。are the
    edges one of the curves what are，the various different shapes that might。be present
    there but then once you have，a mapping that just represents where the。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 只学习并寻找特征，比如边缘、曲线和形状，因为基于像素及其邻近值，你可以弄清楚哪些是边缘，哪些是曲线，哪些是可能存在的各种不同形状。但是一旦你有一个映射，表示在哪里。
- en: edges and curves and shapes happen to be，you can imagine applying the same sort。of
    process again to begin to look for，higher-level features look for objects。maybe
    look for people's eyes and facial，recognition for example maybe look her。more
    complex shapes like the curves on a，particular number if you're trying to。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘、曲线和形状可以想象为，你可以想象再次应用相同的过程来开始寻找更高层次的特征，寻找物体，可能寻找人的眼睛和面部识别，例如，也许寻找更复杂的形状，比如特定数字上的曲线。
- en: recognize a digit in a handwriting，recognition sort of scenario and then。after
    all of that now that you have，these results that represent these。higher-level
    features you can pass them，into a neural network which is really。just a deep neural
    network that looks，like this where you might imagine making。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在手写识别的场景中识别一个数字，然后在拥有这些表示更高层次特征的结果后，你可以将它们传入一个神经网络，这实际上是一个深度神经网络。
- en: a binary classification or classifying，into multiple categories or performing。various
    different tasks on this sort of，model so convolutional neural networks。can be
    quite powerful and quite popular，when it comes towards trying to analyze，could。just
    use the vanilla neural network that，just operates with layer after layer as。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类或多类别分类，进行各种不同的任务，卷积神经网络在这类模型中可以非常强大且受欢迎，尤其在分析时。
- en: we've seen before but these，convolutional neural networks can be。quite helpful
    in particular because of，the way they model the way a human might。look at an image
    that instead of a human，looking at every single pixel。simultaneously and trying
    to convulse，applying them together you might imagine。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前见过，但这些卷积神经网络在特别情况下非常有用，因为它们模拟了人类看图像的方式，而不是人类同时查看每一个像素并试图将其结合起来。
- en: that what convolution is really doing is，looking at various different regions
    of。the image and extracting relevant，information and features out of those。parts
    of the image the same way that a，human might have visual receptors that。are looking
    at particular parts of what，they see and using those combining them。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积真正做的就是查看图像的不同区域，并提取相关信息和特征，就像人类的视觉感受器一样，查看他们所见的特定部分并将其组合起来。
- en: to figure out what meaning they can draw，from all of those various different。inputs
    and so you might imagine applying，this to a situation like handwriting。recognition
    so we'll go ahead and see an，example of that now where I'll go ahead。and open
    up handwriting dot pi again，what we do here is we first import。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找出从这些不同输入中可以提取的意义，你可以想象将其应用于手写识别的情境，现在我们就来看看一个例子，我会再次打开handwriting.py，我们在这里首先导入。
- en: tensorflow and then tensorflow it turns，out has a few data sets that are built。in
    built into the library that you can，just immediately access and one of the。most
    famous data sets in machine，learning is the EM NIST data set which。is just a data
    set of a whole bunch of，samples of people's handwritten digits I。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow中内置了一些数据集，你可以立即访问，其中一个最著名的机器学习数据集是EM NIST数据集，这是一个包含大量人们手写数字样本的数据集。
- en: showed you a slide of that a little，while ago and what we can do is just。immediately
    access that data set which，is built into the library so that if I。want to do something
    like train on a，whole bunch of handwritten digits I can。just use the data set
    that it's provided，to me of course if I had my own data set。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前给你展示了这个数据集的幻灯片，我们可以立即访问这个内置数据集，这样如果我想在大量手写数字上进行训练，我可以使用提供给我的数据集，当然，如果我有自己的数据集。
- en: set of handwritten images I can apply，the same idea I'd first just need to。take
    those images and turn them into，like an array of pixels because that's。the way
    that these are going to be，formatted they're gonna be formatted as。effectively
    an array of individual，pixels now there's a bit of reshaping I。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一组手写图像，我可以应用同样的思路，首先需要将这些图像转换为像素数组，因为这就是它们的格式，它们将有效地格式化为单个像素的数组，现在需要进行一些重塑。
- en: need to do just turning the data into a，format that I can put into my。convolutional
    neural network so this is，doing things like taking all the values。and dividing
    them by 255 if you remember，these color values tend to range from 0。to 255 so
    I can divide them by 255 and，just to put them into 0 to 1 range which。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 需要将数据转换为可以输入到卷积神经网络中的格式，这包括将所有值除以255，如果你记得，这些颜色值通常在0到255之间，所以我可以将它们除以255，把它们转换为0到1的范围。
- en: might be a little bit easier to train on，and then doing various other。modifications
    to the data just to get it，into a nice usable format but here's the。interesting
    and important part here is，where I create the convolutional neural。network the
    CNN where here I'm saying go，ahead and use a sequential model。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会更容易训练，然后对数据进行各种其他修改，以使其变得更加可用，但这里有一个有趣且重要的部分，就是我创建卷积神经网络（CNN），在这里我说继续使用一个顺序模型。
- en: and before I could use model add to say，add a layer add a layer add a layer。another
    way I could define it is just by，passing as input to this sequential。neural network
    a list of all of the，layers that I want and so here the very。first layer in my
    model is a convolution，layer where I'm first going to apply。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在我能够使用模型添加之前，我需要说，添加一层再添加一层。另一种定义它的方法是，通过将我想要的所有层作为输入传递给这个顺序的神经网络。在我的模型中，第一层是一个卷积层，我将首先应用。
- en: convolution to my image I'm going to use，13 different filters so my model is。going
    to learn there 32 rather 32，different filters that I would like to。learn on the
    input image where each，filter is going to be a 3x3 kernel so we。saw those 3x3
    kernels before where we，could multiply each value in a 3x3 grid。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对我的图像进行卷积，我将使用13个不同的滤波器，因此我的模型将学习32种不同的滤波器，我希望在输入图像上学习，每个滤波器将是一个3x3的核，我们之前看到过这些3x3的核，我们可以对3x3网格中的每个值进行乘法运算。
- en: by a value multiplied and add all the，results together so here I'm gonna learn。32
    different of these 3x3 filters I can，again specify my activation function and。I
    specify what my input shape is my，input shape in the banknotes case was。just 4
    I had 4 inputs my input shape，here is going to be 28 comma 28 comma 1。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个值相乘并将所有结果加在一起，所以这里我将学习32种不同的3x3滤波器，我可以再次指定我的激活函数，并指定我的输入形状，我的输入形状在钞票的情况下是。只有4，我有4个输入，我的输入形状将在这里是28，28，1。
- en: because for each of these handwritten，digits it turns out that the NS dataset。organizes
    their data each image is a 28，by 28 pixel grade so we're gonna have a。28 by 28
    pixel grid and each one of，those images only has one channel value。these handwritten
    digits are just black，and white so it's just a single color。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因为对于这些手写数字，NS数据集组织了它们的数据，每个图像是28x28像素的网格，所以我们将有一个28x28的像素网格，并且每个图像只有一个通道值。这些手写数字只有黑色和白色，所以它只是一个单一的颜色。
- en: value representing how much black or how，much white you might imagine that in
    a。color image if you were doing this sort，of thing you might have three different。channels
    a red a green in a blue channel，for example but in the case of just。handwriting
    recognition recognizing a，digit we're just gonna use a single。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 表示有多少黑色或多少白色，你可以想象在彩色图像中，如果你进行这种操作，你可能会有三个不同的通道，红色、绿色和蓝色通道，例如，但在仅进行手写识别，识别数字的情况下，我们只会使用一个。
- en: value for like shaded in or not shade it，in and it might range but it's just
    a。single color value and that then is the，very first layer of our neural network
    a。convolutional layer that will take the，input and learn a whole bunch of。different
    filters that we can apply to，the input to extract meaningful features。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 像是有阴影还是没有阴影的值，它可能会有变化，但它只是一个单一的颜色值，这就是我们神经网络的第一层，一个卷积层，它将接受输入并学习许多不同的滤波器，我们可以应用这些滤波器来提取有意义的特征。
- en: next step is going to be a max pooling，layer also built right in the tensor。flow
    where this is going to be a layer，that is going to use a pool size of 2 by。2 meaning
    we're gonna look at two by two，regions inside of the image and just。extract the
    maximum value again we've，seen why this can be helpful it'll help。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步将是一个最大池化层，这也直接构建在TensorFlow中，这将是一个层，使用2x2的池化大小，意味着我们将查看图像内部的2x2区域，并提取最大值，我们已经看到这可以如何有帮助。
- en: to reduce the size of our input once，we've done that we'll go ahead and。flatten
    all of the units just into a，single layer that we can then pass into。the rest
    of the neural network，and now here's the rest of the neural。network here I'm saying
    let's add a，hidden layer to my neural network with a。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成了这一步，就会减少输入的大小，我们将把所有的单元压平，变成一个单层，然后我们可以将其传递给其余的神经网络，现在这是其余的神经网络，在这里我说让我们给我的神经网络添加一个隐藏层。
- en: hundred and twenty eight unit so a whole，bunch of hidden units inside of the。hidden
    layer and just to prevent，overfitting I can add a dropout to that。say you know
    what when you're training，randomly dropout half of the nodes from。this hidden
    layer just to make sure we，don't become over Alliant on any。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一百二十八个单元，所以有一整堆隐藏单元在隐藏层里面。为了防止过拟合，我可以添加一个 dropout。比如说，你知道，在训练时随机丢弃这隐藏层中的一半节点，以确保我们不对任何特定节点过于依赖。
- en: particular node we begin to really，generalize and stop ourselves from，overfitting。so
    tensorflow allows us just by adding a，single line to add dropout into our。model
    as well such that when it's，training it will perform this dropout。step in order
    to help make sure that we，don't over fit on this particular data。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始真正地泛化，并阻止自己过拟合。所以，TensorFlow 允许我们通过添加一行代码将 dropout 加入我们的模型，这样在训练时，它会执行这个
    dropout 步骤，以帮助确保我们不会对这特定数据过拟合。
- en: and then finally I add an output layer，the output layer is going to have ten。units
    one for each category that I would，like to classify digits into so zero。through
    nine ten different categories，and the activation function I'm going to。use here
    is called the softmax，activation function and in short with。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最后我添加一个输出层，输出层将有十个单元，分别对应我想将数字分类到的每个类别，从零到九，共十个不同类别，激活函数将使用 softmax 激活函数。
- en: the softmax activation function is going，to do is it's going to take the output。and
    turn it into a probability，distribution so ultimately it's going to。tell me like
    what did we estimate the，probability is that this is a two versus。a three versus
    a four and so it will，turn it into that probability。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: softmax 激活函数的作用是将输出转换为概率分布，所以最终它会告诉我，比如说我们估计这个是二与三或四的概率是多少，因此它将其转化为那个概率。
- en: distribution for me next up I'll go，ahead and compile my model and fit it on。all
    of my training data and then I can，evaluate how well the neural network。performs
    and then I've added to my，Python program if I've provided a。command-line argument
    like the name of a。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我将编译我的模型，并在所有训练数据上进行拟合，然后我可以评估神经网络的表现。如果我在 Python 程序中提供了命令行参数，比如某个文件名。
- en: '![](img/ea1acf5e96ec75cca40fe685daa4d48e_7.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea1acf5e96ec75cca40fe685daa4d48e_7.png)'
- en: file I'm gonna go ahead and save the，model to a file and so this can be quite。useful
    too once you've done the training，step which could take some time in terms。of
    taking all the time going through the，data running backpropagation with。gradient
    descent to be able to say，alright how should we adjust the weight。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我将模型保存到文件中，这在训练步骤完成后非常有用，这可能需要一些时间，涉及到通过数据进行反向传播和梯度下降，来调整权重分布。
- en: to this particular model you end up，calculating values for these weights。calculating
    values for these filters，you'd like to remember that information。so you can use
    it later and so，tensorflow allows us to just save a。model to a file such that
    later if we，want to use the model we've learned use。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个特定模型，你最终会计算这些权重的值，计算这些过滤器的值，你希望记住这些信息，以便稍后使用。因此，TensorFlow 允许我们将模型保存到文件中，这样如果我们想使用已经学习的模型。
- en: the weights that we've learned to make，some sort of new prediction we can just。use
    the model that already exists so，what we're doing here is after we've。done all
    the calculation we go ahead and，save the model to a file such that we。can use
    it a little bit later so for，example if I，going to go into digits。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用已经学到的权重进行一些新的预测，因此这里的做法是，在完成所有计算后，将模型保存到文件中，以便稍后使用。例如，如果我进入数字识别。
- en: I'm gonna run handwriting that piyah I，won't save it this time we'll just run。it
    and go ahead and see what happens，what will happen is we need to go。through the
    model in order to train on，all of these samples of handwritten。digits that the
    MS dataset gives us，thousands and thousands of sample。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我将运行手写数字识别，这次不保存，只是运行并看看会发生什么。我们需要遍历模型，以训练 MS 数据集提供的所有手写数字样本，成千上万的样本。
- en: handwritten digits in the same format，that we can use in order to train and
    so。now what you're seeing is this training，process and unlike the bank notes case。where
    there was much much fewer data，points the data was very very simple。here this
    data is more complex and this，training process takes time and so this。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 手写数字以相同的格式，我们可以用来进行训练。因此，现在你看到的是这个训练过程，与银行票据的案例不同，数据点要少得多，数据非常简单。这里的数据更复杂，因此这个训练过程需要时间。
- en: is another one in those cases where when，training neural networks this is why。computational
    power is so important that，you often times you see people wanting。to you is a
    sophisticated GPUs in order，to more efficiently be able to do this。sort of neural
    network training it also，speaks to the reason why more data can。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一个案例，当训练神经网络时，这就是为什么计算能力如此重要，你经常看到人们想要使用**复杂的GPU**，以便更有效地进行这种神经网络训练。这也说明了为什么更多的数据可以。
- en: be helpful the more sample data points，you have the better you can begin to
    do。this training so here we're going，through 60，000 different samples of。handwritten
    digits and I said gonna，we're gonna go through them ten times so。we're gonna go
    through the data set ten，times training each time hopefully。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有更多的样本数据点会更加有帮助，你的训练会做得更好。所以这里我们正在处理60,000个不同的手写数字样本，我说我们要遍历十次，所以我们将训练这个数据集十次。
- en: improving upon our weights with every，time we run through this data set and
    we。can see over here on the right what the，accuracy is each time we go ahead and。run
    this model that the first time it，looks like we got an accuracy of about。92% of
    the digits correct based on this，training set we increased that to 96 or。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们每次运行这个数据集时，都在改进我们的权重。我们可以看到右侧显示了每次运行模型时的准确率，第一次看起来我们在这个训练集上约有92%的数字正确率，我们将其提升到96%或97%。
- en: 97 percent and every time we run this，we're going to see hopefully the。accuracy
    improve as we continue to try，and use that gradient descent that。process of trying
    to run the algorithm，to minimize the loss that we get in。order to more accurately
    predict what，the output should be and what this。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 每次运行这个模型时，我们希望准确率能够提高，随着我们继续使用梯度下降的方法，尽量运行算法以最小化损失，从而更准确地预测输出应该是什么。
- en: process is doing is it's learning not，only the weight but it's learning the。feature
    is to use the kernel matrix to，use when performing that convolution。step because
    this is a convolutional，neural network where I'm first。performing those convolutions
    and then，doing the more traditional neural。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程不仅在学习权重，还在学习特征，使用**核矩阵**在执行卷积步骤时，因为这是一个卷积神经网络，我首先执行卷积，然后进行更传统的神经网络操作。
- en: network structure this is going to learn，all of those individual steps as well。and
    so here we see the tensorflow，provides me with some very nice output。telling me
    about like how many seconds，are left with each of these training。runs that allows
    me to see just how well，we're doing so we'll go ahead and see。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络结构将学习所有这些个体步骤，因此这里我们看到**TensorFlow**为我提供了一些非常好的输出，告诉我每次训练运行剩余的秒数，让我看到我们的表现如何，所以我们继续观察。
- en: how this Network performs it looks like，we've gone through the data set seven。times
    we're going through it in eight，now and at this point the accuracy is。pretty hot
    we saw we went from 92，percent up to 97 percent now it looks。like 98 percent and
    at this point it，seems like things are starting to level。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络的表现看起来，我们已经遍历了数据集七次，现在正在进行第八次，此时准确率相当高，我们看到准确率从92%提升到97%，现在看起来是98%，此时似乎事情开始趋于平稳。
- en: out there's probably a limit to how，accurate we can ultimately be without。running
    the risk of overfitting of，course what's enough nodes you could。just memorize
    the input and overfit upon，them but we'd like to avoid doing that。and drop out
    will help us with this but，now we see we're almost done finishing。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 可能存在一个限制，关于我们能最终达到的准确率，而不冒过拟合的风险，当然，足够的节点可能会使你仅仅记住输入并导致过拟合，但我们希望避免这样，**dropout**将帮助我们，但现在我们看到快要完成了。
- en: our training step we're at fifty five，thousand all right we finished training。and
    now it's going to go ahead and test，for us on ten thousand sample ISM and it。looks
    like on the testing set we were，ninety eight point eight percent。accurate so we
    ended up doing pretty，well it seems on this test on this。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的训练步骤中，我们达到了五万五千，好的，我们完成了训练。现在它将继续为我们在一万个样本上进行测试，似乎在测试集中我们的准确率是九十八点八百分之，所以在这个测试上，我们表现得相当不错。
- en: testing set to see you know how，accurately can we predict these。handwritten
    digits and so what we could，do then is actually test it up I've。written a program
    called recognition dot，pi using PI game if you pass it a model。that's been trained
    and I pre trained in，an example model using this input data。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集以查看我们能够多准确地预测这些手写数字。因此，我们可以测试一下，我写了一个程序叫做 recognition.dot pi，使用 PI game，如果你传入一个经过训练的模型，而我使用这些输入数据预训练了一个示例模型。
- en: what we can do is see whether or not，we've been able to train this。convolutional
    neural network to be able，to predict handwriting for example so I。can try just
    like drawing a handwritten，digit I'll go ahead and draw like the。number two for
    example so there's my，number two again this is messy if you。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看看是否能够训练这个卷积神经网络，以预测手写。例如，我可以尝试像画一个手写数字一样，我将绘制数字二。比如说，这是我的数字二，再次看起来有点凌乱。
- en: tried to imagine how would you write a，program with just like ifs and then x'。to
    be able to do this sort of，calculation it would be tricky to do so。but here I'll
    plus classify and all，right it seems it was able to correctly。classify that what
    I drew was the number，two I'll go ahead and reset it try it。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试想象如何用仅仅是 if 语句和 x' 来编写程序，以进行这样的计算，这会很棘手。但是在这里我会进行分类，似乎它能够正确地分类我画的数字是二，我将重置它并再试一次。
- en: again will draw like an eight for。![](img/ea1acf5e96ec75cca40fe685daa4d48e_9.png)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 再次画一个八的样子。![](img/ea1acf5e96ec75cca40fe685daa4d48e_9.png)
- en: example so here is an eight press，classifying and all right it predicts at。the
    digit that I drew was an eight and，the key here is this really begins to。show
    the power of what the neural，network is doing somehow looking at。various different
    features of these，different pixels figuring out what the。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这里是一个八的分类，它预测我画的数字是八，关键在于这开始展示了神经网络的强大，它以某种方式观察这些不同像素的各种特征，找出这些特征。
- en: relevant features are and figuring out，how to combine them to get a。classification
    and this would be a，difficult task to provide explicit。instructions to the computer
    on how to，do like two views a whole bunch of if。bends to process all these pixel
    values，to figure out what the handwritten digit。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 相关特征并找出如何组合它们以获得分类，这将是一个困难的任务，因为需要向计算机提供明确的指令，如何处理大量的 if 语句以处理所有这些像素值，找出手写数字。
- en: is like everyone's gonna draw their，eights a little bit differently if I。drew
    the eight again it would look a，little bit different and yet ideally we。want to
    train a network to be robust，enough，so that it begins to learn these。patterns
    on its own all I said was here，is the structure of the network and here。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 每个人画的八都会有所不同，如果我再次画八，它看起来会有所不同，而理想情况下，我们希望训练网络足够稳健，以便它开始自行学习这些模式。我所说的就是，这里是网络的结构。
- en: is the data on which to train the，network and the network learning。algorithm
    just tries to figure out what，is the optimal set of weights what is。the optimal
    set of filters to use them，in order to be able to accurately。classify a digit
    into one category or，another just going to show the power of。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于训练网络的数据，而网络学习算法只是试图找出什么是最优的权重集合，什么是最优的过滤器，以便能够准确地将数字分类到一个类别或另一个类别，这将展示出强大的能力。
- en: these sorts of convolutional neural，networks and so that then was a look and。how
    we can use a convolutional neural，networks to begin to solve problems with。relate
    with regards to computer vision，the ability to like take an image and。begin to
    analyze it so this is the type，of analysis you might imagine that's。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些卷积神经网络的种类以及我们如何利用卷积神经网络开始解决与计算机视觉相关的问题，例如能够分析图像，这就是你可以想象的分析类型。
- en: happening in self-driving cars that are，able to figure out what filters to apply。to
    an image to understand what it is，that the computer is looking at or the。same
    type of idea that might be applied，to facial recognition in social media to。be
    able to determine how to recognize，faces in an image as well you can。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况在自动驾驶汽车中发生，它们能够判断应用什么过滤器来理解计算机正在查看的图像，或者相同的想法也可以应用于社交媒体中的面部识别，以确定如何在图像中识别面孔。
- en: imagine a neural network that instead of，classifying into one of ten different。ten
    different digits could instead，classify like is this person a or is。this person
    B trying to tell those，people apart just based on convolution。and so now what
    we'll take a look at is，yet another type of neural network that。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个神经网络，它不是将输入分类为十个不同的数字，而是能够判断这个人是A还是B，试图仅仅基于卷积将这些人区分开。因此，现在我们将看的是另一种类型的神经网络。
- en: can be quite popular for certain types，of tasks but to do so will try to。generalize
    and think about our neural，network a little bit more abstract Li。that here we
    have a sample deep neural，network where we have this input layer a。whole bunch
    of different hidden layers，that are performing certain types of。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些类型的任务，可能相当受欢迎，但这样做会尝试进行泛化，并更抽象地思考我们的神经网络。这里我们有一个样本深度神经网络，输入层有一整堆不同的隐藏层，执行某些类型的操作。
- en: calculations and then an output layer，here that just generates some sort of。output
    that we care about calculating，but we could imagine representing this a。little
    more simply like this here is，good just a more abstract representation。of our
    neural network we have some input，that might be like a vector of a whole。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 进行计算，然后是输出层，生成我们关心的某种输出，但我们可以想象将其表示得更简单一点，这里是我们神经网络的更抽象表示，输入可能像是一个向量。
- en: bunch of different values as their input，that gets passed into a network to。perform
    some sort of calculation or，computation and that network produces。some sort of
    output that output might be，a single value it might be a whole bunch。of different
    values but this is the，general structure of the neural networks。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一堆不同的值作为输入传递到网络中，以执行某种计算或运算，该网络产生某种输出。这个输出可能是一个单一值，也可能是一整堆不同的值，但这就是神经网络的一般结构。
- en: that we've seen there is some sort of，input that gets set into the network and。using
    that input than it were calculates，what the output should be and this sort。of
    model for an oral network is what we，might call a feed-forward neural network。feed-forward
    neural networks have，connection，only in one direction they move from one。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到有某种输入被输入到网络中，并且使用该输入计算输出应该是什么，这种口头网络模型我们可能称之为前馈神经网络。前馈神经网络的连接仅在一个方向上，它们从一个层移动到下一个层。
- en: layer to the next layer to the layer，after that such that the inputs pass。through
    various different hidden layers，and then ultimately produce some sort of。output
    so feed-forward neural networks，were very helpful for solving these。types of classification
    problems that we，saw before we have a whole bunch of。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 从一层传递到下一层，再到之后的层，使得输入通过不同的隐藏层，最终产生某种输出。因此，前馈神经网络在解决这些我们之前看到的分类问题时非常有帮助，我们有一整堆的案例。
- en: input we want to learn what setting of，weights will allow us to calculate the。output
    effectively but there are some，limitations on feed-forward neural。networks that
    we'll see in a moment in，particular the input needs to be like of。a fixed shape
    like a fixed number of，neurons or in the input layer and。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想学习什么权重设置将有效地计算输出，但前馈神经网络有一些限制，我们稍后会看到，特别是输入需要像固定形状一样，比如输入层中的神经元数量是固定的。
- en: there's a fixed shape for the output，like a fixed number of neurons in the。output
    layer and that has some，limitations of its own and a possible。solution to this
    and we'll see examples，of the types of problems we can solve。with this in just
    a second is instead of，just a feed-forward neural network where。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 输出有固定形状，比如输出层中的固定神经元数量，这也有其自身的限制，而一个可能的解决方案，我们稍后会看到可以解决的这类问题的示例，而不是仅仅是一个前馈神经网络。
- en: there are only connections in one，direction from left to right effectively。across
    the network we could also imagine，a recurrent neural network where a。recurrent
    neural network generates，output that gets fed back into itself as。input for future
    runs of that network so，whereas in a traditional neural network。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络中，连接仅在一个方向上，从左到右有效。我们还可以想象一个递归神经网络，其中递归神经网络生成的输出会反馈到自身，作为未来运行该网络的输入，而在传统神经网络中则不是这样。
- en: we have inputs that get fed into the，network that gets fed into the output。and
    the only thing that determines the，output is based on the original input。and based
    on the calculation we do，inside of the network itself this goes。in contrast with
    a recurrent neural，network where in a recurrent neural。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有输入传入网络，最终得出输出。决定输出的唯一因素是基于原始输入和我们在网络内部进行的计算。这与递归神经网络形成对比，在递归神经网络中。
- en: network you can imagine output from the，network feeding back to itself into
    the。network again as input for the next time，you do the calculations and inside
    of。the network what this allows is it，allows the network to maintain some sort。of
    State to store some sort of，information that can be used on future。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以想象网络的输出会反馈到自身，再作为输入供下次计算使用。在网络内部，这使得网络能够维持某种状态，存储一些可以在未来使用的信息。
- en: runs of the network the previously the，network just to find some weights and
    we。passed inputs through the network and it，generated outputs but the network
    wasn't。saving any information based on those，inputs to be able to remember for
    future。iterations or for future runs what a，recurrent neural network will let
    us do。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的运行以往只是找到一些权重，我们将输入传递给网络，生成输出，但网络并没有根据这些输入保存任何信息，以便在未来的迭代或运行中记住。递归神经网络让我们能够做到这一点。
- en: is let the network store information，that gets passed back in as input to the。network
    again the next time we try and，perform some sort of action and this is。particularly
    helpful when dealing with，sequences of data so we'll see a，actually。Microsoft
    has developed an AI，known as the caption bot and what the。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 它让网络存储信息，这些信息将在下次我们尝试执行某种操作时作为输入再次传入网络。这在处理数据序列时特别有用，所以我们实际上会看到，微软开发了一种被称为caption
    bot的AI。
- en: caption butt does is it says I can，understand the content of any photograph。and
    I'll try to describe it as well as，any human I'll analyze your photo but I。won't
    store it or share it and so what，Microsoft's caption bot seems to be。claiming
    to do is it can take an image，and figure out what's in the image and。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: caption bot的功能是我可以理解任何照片的内容。我会尽量描述得和任何人一样好，我会分析你的照片，但我不会存储或分享它。因此，微软的caption
    bot似乎声称它可以处理一张图片，并弄清楚图片中的内容。
- en: '![](img/ea1acf5e96ec75cca40fe685daa4d48e_11.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea1acf5e96ec75cca40fe685daa4d48e_11.png)'
- en: just give us a caption to describe it，so let's try it out here for example is。an
    image of Harvard Square and some，people walking in front of one of the。buildings
    at Harvard Square I'll go，ahead and take the URL for that image。and I'll paste
    it into caption bot and，just press GO so caption bot is。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 只给我们一个描述的标题，所以我们来试试，比如这是哈佛广场的一张图片，前面有一些人在建筑前走。我会拿这个图像的URL，然后把它粘贴到caption bot中，按下GO，caption
    bot就是这样。
- en: analyzing the image and then it says I，think it's a group of people walking
    in。front of the building which seems，amazing the AI is able to look at this。image
    and figure out what's in the image，and the important thing to recognize。here is
    that this is no longer just a，classification task we saw being able to。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 分析图像后，它表示我认为这是在建筑前走动的一群人，这似乎令人惊叹，AI能够查看这个图像并弄清楚图像中的内容，重要的是要认识到这不再只是一个分类任务。
- en: classify images with a convolutional，neural network where the job was you。know
    take the image and then figure out，is it a 0 or a 1 or a 2 or if this。person's
    face or that person's face what，seems to be happening here is the input。is an
    image and we know how to get，networks to take input of images but the。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用卷积神经网络对图像进行分类，其工作是接收图像，然后判断它是0、1还是2，或者是这个人的脸还是那个人的脸。这里似乎发生的事情是，输入是一个图像，我们知道如何让网络接收图像输入，但。
- en: output is text like it's a sentence of，the phrase like a group of people。walking
    in front of the building and，this would seem to pose a challenge for。our more
    traditional feed-forward neural，networks for the reason being that in。traditional
    neural networks we just have，a fixed size input and a fixed size。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是文本，就像是“在建筑物前走动的人群”的句子，这似乎对我们更传统的前馈神经网络构成挑战，因为在传统神经网络中，我们只有固定大小的输入和固定大小的。
- en: output there are a certain number of，neurons in the input to our neural。network
    and a certain number of outputs，for our neural network and then some。calculation
    that goes on in between but，the size of the inputs and the number of。values in
    the input and the number of，values in the output those are always。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 输出中输入到神经网络的神经元数量是一定的，而输出的数量也是固定的，接下来在两者之间进行一些计算，但输入的大小和输入值的数量，以及输出值的数量始终是。
- en: going to be fixed based on the structure，of the neural network and that makes
    it。difficult to imagine how a neural，network could take an image like this。and
    say you know it's a group of people，walking in front of the building because。the
    output is text like it's a sequence，of words now it might be possible for a。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 将会固定在神经网络的结构基础上，这让我们很难想象神经网络如何处理这样的图像，并说这是一个在建筑物前走动的人群，因为输出是文本，像是一系列单词，现在这可能对我们的。
- en: neural network to output like one word，one word you could represent as like
    a。vector of values and you can imagine，ways of doing that next time we'll talk。a
    little bit more about AI as it relates，to language and language processing but。a
    sequence of words is much more，challenging because depending on the。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的输出是一个单词，一个单词可以表示为一组值的向量，我们可以想象，下一次我们将更多地探讨与语言及语言处理相关的人工智能，但一系列单词要挑战得多，因为这取决于。
- en: image you might imagine the output，different number of words we could have。sequences
    of different lengths and，somehow we still want to be able to。generate the appropriate
    output and so，the strategy here is to use a recurrent。neural network a neural
    network that can，feed its own output back into itself as。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图像，你可以想象输出可能有不同数量的单词，我们可以有不同长度的序列，并且我们仍然希望能够生成适当的输出，因此这里的策略是使用递归神经网络，一种能够将自己的输出反馈回自己的神经网络。
- en: input for the next time and this allows，us to do what we call a one-to-many。relationship
    for inputs to outputs that，in vanilla more traditional neural。networks these are
    what we might，consider to be one-to-one neural。networks you pass in one set of
    values，as input you get one vector of values as。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 下一次的输入，这使我们能够实现输入到输出的所谓一对多关系，而在传统的简单神经网络中，我们可能会认为这些是“一对一”的神经网络，你输入一组值，就得到一组值作为。
- en: the output but in this case we want to，pass in one value as input the image
    and。we want to get a sequence many values as，output where each value is like one
    of。these words that gets produced by this，particular algorithm and so the way
    we。might do this is we might imagine。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，但在这种情况下，我们希望将一个值作为输入（图像），并希望获得多个值的序列作为输出，每个值像是这个特定算法生成的一个单词，因此我们可能会想象。
- en: '![](img/ea1acf5e96ec75cca40fe685daa4d48e_13.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea1acf5e96ec75cca40fe685daa4d48e_13.png)'
- en: starting by providing input the image，into our neural network and the neural。network
    is going to generate output but，the output is not going to be the whole。sequence
    of words because we can't，represent the whole sequence of words。and using just
    a fixed set of neurons，instead the output is just going to be。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从提供图像作为输入开始，神经网络将生成输出，但输出不会是整个单词序列，因为我们不能仅用固定数量的神经元来表示整个单词序列，而输出只会是。
- en: the first word we're gonna train the，network to output what the first word of。the
    caption should be and you can，imagine that Microsoft has trained to。this by running
    a whole bunch of，training samples through the AI giving。it a whole bunch of pictures
    and what，the appropriate caption was and having。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练网络输出标题的第一个单词，你可以想象，微软通过运行大量训练样本来训练这一点，将大量图片与相应的标题进行匹配。
- en: the AI begin to learn from that but now，because the network generates output。that
    can be fed back into itself you，could imagine the output of the network。being
    fed back into the same network，this here looks like a separate network。but it's
    really the same network that's，just getting different input that this。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: AI开始从中学习，但现在，由于网络生成的输出可以反馈回自己，你可以想象网络的输出被反馈回同一个网络，这里看起来像是一个独立的网络，但实际上是同一个网络，只是获取了不同的输入。
- en: network's output gets fed back into，itself but it's going to generate。another
    output and that other output is，going to be like the second word in the。caption
    and this recurrent neural，network then is this network is going to。generate other
    output that can be fed，back into itself to generate yet another。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的输出会反馈回自己，但它会生成另一个输出，而那个输出就像是标题中的第二个单词，这个递归神经网络将会生成可以反馈回自己的其他输出，从而生成另一个输出。
- en: word set back into itself to generate，another word and so recurrent neural。networks
    allow us to represent this sort，of one-to-many structure you provide one。image
    as input and the neural network，can pass data into the next run of the。network
    and then again and again such，that you could run the network multiple。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 单词被反馈回自己以生成另一个单词，因此递归神经网络允许我们表示这种一对多的结构，你提供一个图像作为输入，神经网络可以将数据传递到网络的下一次运行中，然后一次又一次地，这样你可以多次运行网络。
- en: times each time generating a different，output still based on that original。input
    and this is where recurrent neural，networks become particularly useful when。dealing
    with sequences of inputs or，outputs and my output is a sequence of。words and since
    I can't very easily，represent outputting an entire sequence。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 每次生成不同的输出，仍然基于原始输入，而这正是递归神经网络在处理输入或输出序列时特别有用的地方，我的输出是一个单词序列，由于我无法很容易地，表示整个序列的输出。
- en: of words and I'll instead output that，sequence one word at a time by allowing。my
    network to pass information about，what still needs to be spit said about。the photo
    into the next stage of running，the network so you could run the network。multiple
    times the same network with the，same weights just getting different。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 单词的输出将是逐个输出，而不是一次性输出，通过允许我的网络传递关于照片的，信息到下一阶段的运行，从而可以多次运行相同的网络。
- en: input each time first getting input from，the image and then getting input from。the
    network itself as additional，information about what additionally。needs to be given
    in a particular，caption for example so this then is a。one-to-many relationship
    inside of a，recurrent neural network but it turns。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输入每次首先从图像获取，然后作为关于特定标题需要额外提供的信息从网络本身获取，所以这是递归神经网络内部的一种一对多关系，但它转变。
- en: out there are other models that we can，use other ways we can try and use。recurrent
    neural networks to be able to，represent data that might be stored in。other forms
    as well we saw how we could，use neural networks in order to analyze。images in
    the context of convolutional，neural networks that take an image。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他模型我们可以使用，其他方式可以尝试使用递归神经网络来表示可能以其他形式存储的数据，我们看到我们如何使用神经网络分析图像，尤其是在卷积神经网络的背景下处理图像。
- en: figure out various different properties，of the image and were able to draw some。sort
    of conclusion based on that but you，might imagine that something like。YouTube
    they need to be able to do a lot，of learning based on video they need to。look
    through videos to detect if they're，like copyright violations or they need。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 确定图像的各种不同属性，并能够基于此得出某种结论，但你可能想象像YouTube这样的东西，他们需要能够基于视频进行大量学习，他们需要查看视频，以检测是否存在版权侵犯或他们需要。
- en: to be able to look through videos to，maybe identify what particular items are。inside
    of the video for example and，video you might imagine is much more。difficult to
    put in as input to a neural，network because whereas an image you can。just treat
    each pixel as a different，value videos or sequences there's。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看视频来识别视频中的特定项目，例如，视频你可能想象将其作为输入放入神经网络要困难得多，因为与图像相比，你可以将每个像素视为不同的值，而视频或序列则。
- en: sequences of images and each sequence，might be of different lengths and so it。might
    be challenging to represent that，entire video as a single vector of。values that
    you could pass in to a，neural network and so here two recurrent。neural networks
    can be a valuable，solution for trying to solve this type。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图像序列，每个序列可能长度不同，因此将整个视频表示为一个单一的值向量可能会很具挑战性，你可以传入神经网络，这里两个递归神经网络可能是解决这类问题的宝贵方案。
- en: of problem then instead of just passing，in a single input into our neural。network
    we could pass in the input like，one frame at a time you might imagine。first taking
    the first frame of the，video passing it into the network and。then maybe not having
    the network output，anything at all yet let it take in。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个问题中，我们不是仅仅将单一输入传入神经网络，而是可以一次传入一个帧的输入，想象一下，首先获取视频的第一帧，传入网络，然后可能不让网络输出任何结果，而是让它接收。
- en: another input and this time pass it into，the network but the network gets。information
    from the last time we，provided an input，the network then we pass in a third。input
    and then a fourth input where each，time what the network gets is it gets。the most
    recent input like each frame of，the video but it also gets information。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个输入，这次将其传入网络，但网络会获取我们上次提供输入的信息，然后我们传入第三个输入，再传入第四个输入，每次网络获得的是最近的输入，比如视频的每一帧，但它也获得信息。
- en: the network processed from all of the，previous iterations so on frame number。four
    you end up getting the input for，frame number four plus information the。network
    has calculated from the first，three frames and using all of that data。combined
    this recurrent neural network，can begin to learn how to extract。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 网络处理来自所有先前迭代的信息，因此在第四帧时，你会获得第四帧的输入，加上网络从前三帧计算得到的信息，利用所有这些数据，结合递归神经网络可以开始学习如何提取。
- en: patterns from a sequence of data as well，and so you might imagine if you want
    to。classify a video into a number of，different genres like an educational。video
    or a music video or different，types of videos that's a classification。task where
    you want to take as input，each of the frames of the video and you。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据序列中提取模式，你可能会想象如果你想把视频分类为多个不同的类型，比如教育视频或音乐视频，或者其他类型的视频，这是一项分类任务，你希望将每一帧视频作为输入。
- en: want to output something like what it is，what category that it happens to belong。to
    and you can imagine doing this sort，of thing sort of many to one learning。anytime
    your input is a sequence and，some input is a sequence in the context。of video
    it could be in the context of，like if someone has typed a message and。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 想要输出一些像它是什么，或者它属于哪个类别的东西，你可以想象这种多对一学习的事情，任何时候你的输入都是一个序列，在视频的上下文中也可能是这样的，如果有人输入了一条消息。
- en: you want to be able to categorize that，message like if you're trying to take
    a。movie review and trying to classify it，as is it a positive review or a negative。review
    that input is a sequence of words，and the output is a classification。positive
    or negative there to a，recurrent neural network might be。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望能够对该消息进行分类，比如如果你想对电影评论进行分类，是正面评论还是负面评论，输入是一系列单词，输出是正面或负面的分类，对于递归神经网络可能是。
- en: helpful for analyzing sequences of words，and they're quite popular when it comes。to
    dealing with language could even be，used for spoken language as well that。spoken
    language is you know an audio，waveform that can be segmented into。distinct chunks
    and each of those could，be passed in as an input into a。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分析单词序列非常有帮助，而且在处理语言时它们相当受欢迎，甚至可以用于口语语言，口语语言是你知道的音频波形，可以分段成不同的块，而每一个块都可以作为输入传入。
- en: recurrent neural network to be able to，classify someone's voice for instance
    if。you want to do voice recognition to say，is this one person or is this another。here
    are also cases where you might want，this many-to-one architecture for a。recurrent
    neural network and then as one，final problem just to take a look at in。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络能够对某个人的声音进行分类，例如，如果你想进行声音识别，判断这是一个人还是另一个人，这里还有许多情况下，你可能想要这种多对一的结构来进行递归神经网络的运用。
- en: terms of what we can do with these sorts，of networks imagine what like Google。![](img/ea1acf5e96ec75cca40fe685daa4d48e_15.png)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下我们可以用这些类型的网络做些什么，类似于谷歌的那种！[](img/ea1acf5e96ec75cca40fe685daa4d48e_15.png)
- en: Translate is doing so what Google，Translate is doing is it's taking some。text
    written in one language and，converting it into text written in some。other language
    for example where now，this input is a sequence of data it's a。sequence of words
    and the output is a，sequence of words as well it's also a。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Google Translate正在做的就是将某种语言写的文本转换成另一种语言的文本。例如，现在这个输入是一个数据序列，是一个单词序列，输出也是一个单词序列。
- en: sequence so here we want effectively，like a many-to-many，relationship our input
    is a sequence and。our output is a sequence as well and，it's not quite going to
    work to just say。like take each word in the input and，translate it into a word
    in the output。because ultimately different languages，put their words in different
    orders and。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在序列中，这里我们实际上想要实现一种多对多的关系，我们的输入是一个序列，输出也是一个序列。仅仅将输入中的每个词翻译为输出中的一个词是不够的，因为不同的语言排列词语的顺序是不同的。
- en: maybe one language uses two words for，something whereas another language only。uses
    one so we really want some way to，take this information this input encode。it somehow
    and use that encoding to，generate what the output ultimately。should be and this
    has been one of the，big advancements in automated。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 可能一种语言用两个词表示某个意思，而另一种语言只用一个词，因此我们确实需要某种方式来编码这个信息输入，并使用该编码生成最终的输出。这是自动化领域的一个重大进展。
- en: translation technology is the ability to，use the neural networks to do this。instead
    of older more traditional，methods and this has improved accuracy。dramatically
    and the way you might，imagine doing this is again using a。recurrent neural network
    with multiple，inputs and multiple outputs we start by。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译技术的关键在于使用神经网络，而不是旧的传统方法，这大大提高了准确性。你可以想象，使用一个具有多个输入和多个输出的循环神经网络来进行此操作，我们开始时。
- en: passing in all the input input goes into，the network another input like another。word
    goes into the network and we do，this multiple times like once for each。word in
    the input that I'm trying to，translate and only after all of that is。done does
    the network now is start to，generate output like the first word of。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 所有输入都被传入网络，另一个输入（如另一个词）也进入网络，我们重复这个过程，比如对输入中的每个词进行一次翻译，只有在所有这些完成后，网络才开始生成输出，比如第一个词。
- en: the translated sentence and the next，word of the translated sentence someone。and
    so forth where each time the network，passes information to itself by allowing。for
    this model of giving some sort of，state from one run in the network to the。next
    run assembling information about，all the inputs and then passing an。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译后的句子和翻译句子的下一个词之间存在联系，每次网络通过允许某种状态在网络的不同运行之间传递信息，从而将信息汇总关于所有输入，然后传递一个。
- en: information about which part of the，output in order to generate next and。there
    are a number of different types of，these sorts of recurrent neural networks。one
    of the most popular is known as the，long short term memory neural network。otherwise
    known as LST M but in general，these types of networks can be very very。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对输出的不同部分的信息进行处理，以生成下一个输出。这类循环神经网络有多种不同类型，其中最受欢迎的一种被称为长短期记忆神经网络（LSTM），但一般来说，这些类型的网络可以非常有效。
- en: powerful whenever we're dealing with，sequences whether those are sequences of。images
    or especially sequences of words，when it comes towards dealing with。![](img/ea1acf5e96ec75cca40fe685daa4d48e_17.png)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理序列时，无论是图像序列还是特别是单词序列，这种能力都是强大的。
- en: natural language so that then we're just，some of the different types of neural。networks
    that can be used to do all，sorts of different computations and。these are incredibly
    versatile tools，that can be applied to a number of。different domains we only looked
    at a，couple of there are the most popular。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言是我们可以用来进行各种不同计算的神经网络类型之一，这些都是极其多功能的工具，可以应用于多个不同领域，我们只看了一些最流行的。
- en: types of neural networks for more，traditional feed-forward neural networks。convolutional
    neural networks and，recurrent neural networks but there are。other types as well
    they're adversarial，networks where networks compete with。each other to try and
    be able to，generate new types of data as well as，other networks。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的类型包括传统的前馈神经网络、卷积神经网络和循环神经网络，但还有其他类型，比如对抗网络，其中网络之间相互竞争，尝试生成新的数据类型以及其他网络。
- en: can solve other tasks based on what they，happen to be structured and adapted
    for。and these are very powerful tools in，machine learning for being able to very。easily
    learn based on some set of input，data and to be able to therefore figure。out how
    to calculate some function from，inputs to outputs whether its input to。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 可以根据它们的结构和适应性解决其他任务。这些是机器学习中非常强大的工具，能够基于一组输入数据非常轻松地学习，因此能够计算输入到输出的某些函数，无论是输入到。
- en: some sort of classification like，analyzing an image and getting a digit。or machine
    translation where the input，is in one language and the output is in。another these
    tools have a lot of，applications for machine learning more。generally next time
    we'll look at，machine learning and AI in particular in。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一种分类方法，比如分析图像并得到一个数字，或者机器翻译，其中输入是某种语言，输出是另一种语言。这些工具在机器学习中有很多应用。下次我们将特别探讨机器学习和人工智能。
- en: the context of natural language we，talked a little bit about this today but。looking
    at how it is that our AI can，begin to understand natural language and。can begin
    to be able to analyze and do，useful tasks with regards to human。language which
    turns out to be a，challenging and interesting task so。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言的背景下，我们今天对此谈了一些，但看我们的人工智能如何开始理解自然语言，并能够分析和执行与人类语言相关的有用任务，这被证明是一个具有挑战性且有趣的任务。
- en: '![](img/ea1acf5e96ec75cca40fe685daa4d48e_19.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea1acf5e96ec75cca40fe685daa4d48e_19.png)'
