- en: Lecture 6
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6讲
- en: 原文：[https://cs50.harvard.edu/ai/notes/6/](https://cs50.harvard.edu/ai/notes/6/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://cs50.harvard.edu/ai/notes/6/](https://cs50.harvard.edu/ai/notes/6/)
- en: These notes reflect the new version of Lecture 6, released on 14 August 2023\.
    If you watched the prior version of the lecture and wish to see its notes, [click
    here](old/).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这些笔记反映了2023年8月14日发布的第6讲的新版本。如果您观看了之前的版本，并希望查看其笔记，[请点击此处](old/)。
- en: Language
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言
- en: So far in the course, we needed to shape tasks and data such that an AI will
    be able to process them. Today, we will look at how an AI can be constructed to
    process human language.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在课程中，我们需要塑造任务和数据，以便AI能够处理它们。今天，我们将探讨如何构建AI以处理人类语言。
- en: '**Natural Language Processing** spans all tasks where the AI gets human language
    as input. The following are a few examples of such tasks:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理**涵盖了所有AI获取人类语言作为输入的任务。以下是一些此类任务的例子：'
- en: automatic summarization, where the AI is given text as input and it produces
    a summary of the text as output.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动摘要，其中AI被给出文本作为输入，并产生文本的摘要作为输出。
- en: information extraction, where the AI is given a corpus of text and the AI extracts
    data as output.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息提取，其中AI被给出文本语料库，并从中提取数据作为输出。
- en: language identification, where the AI is given text and returns the language
    of the text as output.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言识别，其中AI被给出文本并返回文本的语言作为输出。
- en: machine translation, where the AI is given a text in the origin language and
    it outputs the translation in the target language.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译，其中AI被给出原始语言的文本，并输出目标语言的翻译。
- en: named entity recognition, where the AI is given text and it extracts the names
    of the entities in the text (for example, names of companies).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名实体识别，其中AI被给出文本，并从中提取文本中的实体名称（例如，公司名称）。
- en: speech recognition, where the AI is given speech and it produces the same words
    in text.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音识别，其中AI被给出语音，并产生相同的文本。
- en: text classification, where the AI is given text and it needs to classify it
    as some type of text.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类，其中AI被给出文本，并需要将其分类为某种类型的文本。
- en: word sense disambiguation, where the AI needs to choose the right meaning of
    a word that has multiple meanings (e.g. bank means both a financial institution
    and the ground on the sides of a river).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词义消歧，其中AI需要选择具有多个意义的单词的正确含义（例如，银行既指金融机构也指河流的河岸）。
- en: Syntax and Semantics
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语法和语义
- en: '**Syntax** is sentence structure. As native speakers of some human language,
    we don’t struggle with producing grammatical sentences and flagging non-grammatical
    sentences as wrong. For example, the sentence “Just before nine o’clock Sherlock
    Holmes stepped briskly into the room” is grammatical, whereas the sentence “Just
    before Sherlock Holmes nine o’clock stepped briskly the room” is non-grammatical.
    Syntax can be grammatical and ambiguous at the same time, as in “I saw the man
    with the telescope.” Did I see (the man with the telescope) or did I see (the
    man), doing so by looking through the telescope? To be able to parse human speech
    and produce it, the AI needs to command syntax.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**语法**是句子结构。作为某些人类语言的母语者，我们不会在产生语法正确的句子和标记非语法正确的句子为错误时感到困难。例如，句子“在九点之前，福尔摩斯敏捷地走进了房间”是语法正确的，而句子“在福尔摩斯九点之前敏捷地走进了房间”则是非语法正确的。语法可以同时是语法正确的和模糊的，例如，“我看到了拿着望远镜的男人。”我是看到了（拿着望远镜的男人）还是我看到了（男人），通过望远镜看到了？为了能够解析人类语言并产生它，AI需要掌握语法。'
- en: '**Semantics** is the meaning of words or sentences. While the sentence “Just
    before nine o’clock Sherlock Holmes stepped briskly into the room” is syntactically
    different from “Sherlock Holmes stepped briskly into the room just before nine
    o’clock,” their content is effectively identical. Similarly, although the sentence
    “A few minutes before nine, Sherlock Holmes walked quickly into the room” uses
    different words from the previous sentences, it still carries a very similar meaning.
    Moreover, a sentence can be perfectly grammatical while being completely nonsensical,
    as in Chomsky’s example, “Colorless green ideas sleep furiously.” To be able to
    parse human speech and produce it, the AI needs to command semantics.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**语义**是单词或句子的意义。虽然句子“在九点之前，福尔摩斯敏捷地走进了房间”在语法上与“福尔摩斯敏捷地走进了房间，就在九点之前”不同，但它们的内容实际上是相同的。同样，尽管句子“A
    few minutes before nine, Sherlock Holmes walked quickly into the room”使用了与前句不同的单词，但它仍然传达了非常相似的意义。此外，一个句子可以完全语法正确，但完全无意义，如乔姆斯基的例子，“无色的绿色想法疯狂地睡觉。”为了能够解析人类语言并产生它，AI需要掌握语义。'
- en: Context-Free Grammar
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文无关语法
- en: '**Formal Grammar** is a system of rules for generating sentences in a language.
    In **Context-Free Grammar**, the text is abstracted from its meaning to represent
    the structure of the sentence using formal grammar. Let’s consider the following
    example sentence:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**形式语法**是一种用于生成语言中句子的规则系统。在**上下文无关语法**中，文本从其意义中抽象出来，使用形式语法来表示句子的结构。让我们考虑以下示例句子：'
- en: She saw the city.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 她看到了这个城市。
- en: This is a simple grammatical sentence, and we would like to generate a syntax
    tree representing its structure.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的语法句子，我们希望生成一个表示其结构的语法树。
- en: We start by assigning each word its part of speech. *She* and *city* are nouns,
    which we will mark as N. *Saw* is a verb, which we will mark as V. *The* is a
    determiner, marking the following noun as definite or indefinite, and we will
    mark it as D. Now, the above sentence can be rewritten as
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先为每个单词分配其词性。*她*和*城市*是名词，我们将它们标记为N。*看到*是动词，我们将它标记为V。*这个*是限定词，标记后面的名词是确定的还是不确定的，我们将它标记为D。现在，上述句子可以重写为
- en: N V D N
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N V D N
- en: 'So far, we have abstracted each word from its semantic meaning to its part
    of speech. However, words in a sentence are connected to each other, and to understand
    the sentence we must understand how they connect. A noun phrase (NP) is a group
    of words that connect to a noun. For example, the word *she* is a noun phrase
    in this sentence. In addition, the words *the city* also form a noun phrase, consisting
    of a determiner and a noun. A verb phrase (VP) is a group of words that connect
    to a verb. The word *saw* is a verb phrase in itself. However, the words *saw
    the city* also make a verb phrase. In this case, it is a verb phrase consisting
    of a verb and a noun phrase, which in turn consists of a determiner and a noun.
    Finally, the whole sentence (S) can be represented as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经将每个单词从其语义意义抽象到其词性。然而，句子中的单词相互连接，要理解句子，我们必须了解它们是如何连接的。名词短语（NP）是一组与名词连接的单词。例如，单词*她*是这个句子中的名词短语。此外，单词*这个城市*也形成一个名词短语，由一个限定词和一个名词组成。动词短语（VP）是一组与动词连接的单词。单词*看到*本身就是一个动词短语。然而，单词*看到这个城市*也构成一个动词短语。在这种情况下，它是一个由动词和名词短语组成的动词短语，而名词短语又由一个限定词和一个名词组成。最后，整个句子（S）可以表示如下：
- en: '![Syntactic Tree](../Images/629f37a1e86bd1ebaf62986f1149af1c.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![句法树](../Images/629f37a1e86bd1ebaf62986f1149af1c.png)'
- en: Using formal grammar, the AI is able to represent the structure of sentences.
    In the grammar we have described, there are enough rules to represent the simple
    sentence above. To represent more complex sentences, we will have to add more
    rules to our formal grammar.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用形式语法，人工智能能够表示句子的结构。在我们描述的语法中，有足够的规则来表示上述简单句子。要表示更复杂的句子，我们不得不向我们的形式语法中添加更多规则。
- en: nltk
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: nltk
- en: 'As is often the case in Python, multiple libraries have been written to implement
    the idea above. nltk (Natural Language Toolkit) is one such library. To analyze
    the sentence from above, we will provide the algorithm with rules for the grammar:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，通常会有多个库被编写来实现上述想法。nltk（自然语言工具包）就是这样一个库。为了分析上述句子，我们将为语法提供算法规则：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Similar to what we did above, we define what possible components could be included
    in others. A sentence can include a noun phrase and a verb phrase, while the phrases
    themselves can consist of other phrases, nouns, verbs, etc., and, finally, each
    part of speech spans some words in the language.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们上面所做的一样，我们定义了可能包含在其他中的可能组件。一个句子可以包含一个名词短语和一个动词短语，而短语本身可以由其他短语、名词、动词等组成，最终，每个词性在语言中跨越一些单词。
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: After giving the algorithm an input sentence split into a list of words, the
    function prints the resulting syntactic tree (pretty_print) and also generates
    a graphic representation (draw).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在向算法提供一个输入句子并将其拆分为单词列表后，函数将打印出结果语法树（pretty_print）并生成图形表示（draw）。
- en: '![Syntactic Trees](../Images/a8907ee345b205385524d7ccbd5a38cd.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![句法树](../Images/a8907ee345b205385524d7ccbd5a38cd.png)'
- en: '*n*-grams'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*n*-grams'
- en: An *n*-gram is a sequence of *n* items from a sample of text. In a **character
    *n*-gram**, the items are characters, and in a **word *n*-gram** the items are
    words. A *unigram*, *bigram*, and *trigram* are sequences of one, two, and three
    items. In the following sentence, the first three *n*-grams are “how often have,”
    “often have I,” and “have I said.”
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*-元组是从文本样本中提取的*n*个项目的序列。在**字符*n*-元组**中，项目是字符，而在**单词*n*-元组**中，项目是单词。*单元组*、*二元组*和*三元组*分别是一、两个和三个项目的序列。在以下句子中，前三个*n*-元组是“how
    often have”、“often have I”和“have I said”。'
- en: “How often have I said to you that when you have eliminated the impossible whatever
    remains, however improbable, must be the truth?”
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: “我曾经说过多少次，当你排除了不可能的，无论多么不可能，剩下的就一定是真相？”
- en: '*n*-grams are useful for text processing. While the AI hasn’t necessarily seen
    the whole sentence before, it sure has seen parts of it, like “have I said.” Since
    some words occur together more often than others, it is possible to also predict
    the next word with some probability. For example, your smartphone suggests words
    to you based on a probability distribution derived from the last few words you
    typed. Thus, a helpful step in natural language processing is breaking the sentence
    into n-grams.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*-元组在文本处理中很有用。尽管AI之前不一定看到过整个句子，但它肯定看到过句子的一部分，比如“我曾经说过。”由于一些词比其他词更经常一起出现，因此也有可能用一定的概率预测下一个词。例如，你的智能手机根据你输入的最后几个词的概率分布来为你建议单词。因此，自然语言处理中的一个有用步骤是将句子分解成n元组。'
- en: Tokenization
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词
- en: Tokenization is the task of splitting a sequence of characters into pieces (tokens).
    Tokens can be words as well as sentences, in which case the task is called **word
    tokenization** or **sentence tokenization**. We need tokenization to be able to
    look at *n*-grams, since those rely on sequences of tokens. We start by splitting
    the text into words based on the space character. While this is a good start,
    this method is imperfect because we end up with words with punctuation, such as
    “remains,”. So, for example, we can remove punctuation. However, then we face
    additional challenges, such as words with apostrophes (e.g. “o’clock”) and hyphens
    (e.g. “pearl-grey). Additionally, some punctuation is important for sentence structure,
    like periods. However, we need to be able to tell apart between a period at the
    end of the word “Mr.” and a period in the end of the sentence. Dealing with these
    questions is the process of tokenization. In the end, once we have our tokens,
    we can start looking at *n*-grams.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是将字符序列分割成片段（标记）的任务。标记可以是单词，也可以是句子，在这种情况下，该任务被称为**单词分词**或**句子分词**。我们需要分词来查看*n*-元组，因为它们依赖于标记的序列。我们首先根据空格字符将文本分割成单词。虽然这是一个好的开始，但这种方法并不完美，因为我们最终会得到带有标点的单词，例如“remains”。因此，例如，我们可以移除标点。然而，然后我们会面临额外的挑战，例如带有撇号的单词（例如“o'clock”）和带有连字符的单词（例如“pearl-grey”）。此外，一些标点对于句子结构很重要，比如句号。然而，我们需要能够区分单词“Mr.”结尾的句号和句子结尾的句号。处理这些问题是分词的过程。最后，一旦我们有了标记，我们就可以开始查看*n*-元组。
- en: Markov Models
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔可夫模型
- en: As discussed in previous lectures, Markov models consist of nodes, the value
    of each of which has a probability distribution based on a finite number of previous
    nodes. Markov models can be used to generate text. To do so, we train the model
    on a text, and then establish probabilities for every *n*-th token in an *n*-gram
    based on the *n* words preceding it. For example, using trigrams, after the Markov
    model has two words, it can choose a third one from a probability distribution
    based on the first two. Then, it can choose a fourth word from a probability distribution
    based on the second and third words. To see an implementation of such a model
    using nltk, refer to generator.py in the source code, where our model learns to
    generate Shakespeare-sounding sentences. Eventually, using Markov models, we are
    able to generate text that is often grammatical and sounding superficially similar
    to human language output. However, these sentences lack actual meaning and purpose.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几节课所讨论的，马尔可夫模型由节点组成，每个节点的值基于有限数量的前一个节点具有概率分布。马尔可夫模型可以用来生成文本。为此，我们在文本上训练模型，然后根据前n个词为每个n-gram的每个*n*-th标记建立概率。例如，使用三元组，在马尔可夫模型有两个词之后，它可以从基于前两个词的概率分布中选择第三个词。然后，它可以从基于第二个和第三个词的概率分布中选择第四个词。要查看使用nltk实现此类模型的示例，请参阅源代码中的generator.py，其中我们的模型学习生成莎士比亚风格的句子。最终，使用马尔可夫模型，我们能够生成通常语法正确且表面上听起来与人类语言输出相似的文本。然而，这些句子缺乏实际的意义和目的。
- en: Bag-of-Words Model
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词袋模型
- en: 'Bag-of-words is a model that represents text as an unordered collection of
    words. This model ignores syntax and considers only the meanings of the words
    in the sentence. This approach is helpful in some classification tasks, such as
    sentiment analysis (another classification task would be distinguishing regular
    email from spam email). Sentiment analysis can be used, for instance, in product
    reviews, categorizing reviews as positive or negative. Consider the following
    sentences:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型是一种将文本表示为无序单词集合的模型。该模型忽略了语法，只考虑句子中单词的意义。这种方法在某些分类任务中很有帮助，例如情感分析（另一个分类任务可能是区分常规电子邮件和垃圾邮件）。情感分析可以用于产品评论，将评论分类为正面或负面。考虑以下句子：
- en: “My grandson loved it! So much fun!”
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “我的孙子很喜欢它！太有趣了！”
- en: “Product broke after a few days.”
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “产品几天后就坏了。”
- en: “One of the best games I’ve played in a long time.”
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “这是我很久以来玩过的最好的游戏之一。”
- en: “Kind of cheap and flimsy, not worth it.”
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “有点便宜且脆弱，不值得。”
- en: Based only on the words in each sentence and ignoring the grammar, we can see
    that sentences 1 and 3 are positive (“loved,” “fun,” “best”) and sentences 2 and
    4 are negative (“broke,” “cheap,” “flimsy”).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 仅基于每个句子中的单词，忽略语法，我们可以看到句子1和3是积极的（“loved”，“fun”，“best”），而句子2和4是消极的（“broke”，“cheap”，“flimsy”）。
- en: Naive Bayes
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单贝叶斯
- en: 'Naive Bayes is a technique that can be used in sentiment analysis with the
    bag-of-words model. In sentiment analysis, we are asking “What is the probability
    that the sentence is positive/negative given the words in the sentence.” Answering
    this question requires computing conditional probability, and it is helpful to
    recall Bayes’ rule from lecture 2:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 简单贝叶斯是一种可以与词袋模型一起用于情感分析的技术。在情感分析中，我们问“给定句子中的单词，句子是积极的/消极的概率是多少。”回答这个问题需要计算条件概率，回忆第2节课中的贝叶斯定理会有所帮助：
- en: '![Bayes'' Rule](../Images/888965c16e6b416fcef400ccf3a470bb.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯定理](../Images/888965c16e6b416fcef400ccf3a470bb.png)'
- en: 'Now, we would like to use this formula to find P(sentiment | text), or, for
    example, P(positive | “my grandson loved it”). We start by tokenizing the input,
    such that we end up with P(positive | “my”, “grandson”, “loved”, “it”). Applying
    Bayes’ ruled directly, we get the following expression: P(“my”, “grandson”, “loved”,
    “it” | positive)*P(positive)/P(“my”, “grandson”, “loved”, “it”). This complicated
    expression will give us the precise answer to P(positive | “my”, “grandson”, “loved”,
    “it”).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想使用这个公式来找到P(sentiment | text)，例如，P(positive | “my grandson loved it”)。我们首先对输入进行标记化，这样我们最终得到P(positive
    | “my”, “grandson”, “loved”, “it”)。直接应用贝叶斯定理，我们得到以下表达式：P(“my”, “grandson”, “loved”,
    “it” | positive)*P(positive)/P(“my”, “grandson”, “loved”, “it”)。这个复杂表达式将给我们P(positive
    | “my”, “grandson”, “loved”, “it”)的精确答案。
- en: 'However, we can simplify the expression if we are willing to get an answer
    that’s not equal, but proportional to P(positive | “my”, “grandson”, “loved”,
    “it”). Later on, knowing that the probability distribution needs to sum up to
    1, we can normalize the resulting value into an exact probability. This means
    that we can simplify the expression above to the numerator only: P(“my”, “grandson”,
    “loved”, “it” | positive)*P(positive). Again, we can simplify this expression
    based on the knowledge that a conditional probability of *a* given *b* is proportional
    to the joint probability of *a* and *b*. Thus, we get the following expression
    for our probability: P(positive, “my”, “grandson”, “loved”, “it”)*P(positive).
    Calculating this joint probability, however, is complicated, because the probability
    of each word is conditioned on the probabilities of the words preceding it. It
    requires us to compute P(positive)*P(“my” | positive)*P(“grandson” | positive,
    “my”)*P(loved | positive, “my”, “grandson”)*P(“it” | positive, “my”, “grandson”,
    “loved”).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们愿意得到一个不等于P(positive | “my”, “grandson”, “loved”, “it”)，但与其成比例的答案，我们就可以简化这个表达式。稍后，我们知道概率分布需要加起来等于1，我们可以将得到的结果值归一化成一个确切的概率。这意味着我们可以将上面的表达式简化为仅包含分子：P(“my”,
    “grandson”, “loved”, “it” | positive)*P(positive)。再次，我们可以根据已知条件概率*a*给定*b*与*a*和*b*的联合概率成比例的知识来简化这个表达式。因此，我们得到以下概率表达式：P(positive,
    “my”, “grandson”, “loved”, “it”)*P(positive)。然而，计算这个联合概率是复杂的，因为每个词的概率都是基于它前面词的概率。这需要我们计算P(positive)*P(“my”
    | positive)*P(“grandson” | positive, “my”)*P(loved | positive, “my”, “grandson”)*P(“it”
    | positive, “my”, “grandson”, “loved”)。
- en: 'Here is where we use Bayes’ rules naively: we assume that the probability of
    each word is independent from other words. This is not true, but despite this
    imprecision, Naive Bayes’ produces a good sentiment estimate. Using this assumption,
    we end up with the following probability: P(positive)*P(“my” | positive)*P(“grandson”
    | positive)*P(“loved” | positive)*P(“it” | positive), which is not that difficult
    to calculate. P(positive) = the number of all positive samples divided by the
    number of total samples. P(“loved” | positive) is equal to the number of positive
    samples with the word “loved” divided by the number of positive samples. Let’s
    consider the example below, with smiling and frowning emojies substituting the
    words “positive” and “negative”:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 正是在这里，我们天真地使用了贝叶斯定理：我们假设每个词的概率与其他词是独立的。这并不正确，但尽管这种不精确，朴素贝叶斯仍然能产生一个好的情感估计。使用这个假设，我们最终得到以下概率：P(positive)*P(“my”
    | positive)*P(“grandson” | positive)*P(“loved” | positive)*P(“it” | positive)，这并不难计算。P(positive)
    = 所有正样本的数量除以总样本的数量。P(“loved” | positive)等于包含单词“loved”的正样本数量除以正样本的数量。让我们考虑以下例子，其中微笑和皱眉表情符号代替了单词“positive”和“negative”：
- en: '![Naive Bayes](../Images/232177d7257ceb0652d513019fbbdcf6.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![朴素贝叶斯](../Images/232177d7257ceb0652d513019fbbdcf6.png)'
- en: On the right we are seeing a table with the conditional probabilities of each
    word on the left occurring in a sentence given that the sentence is positive or
    negative. In the small table on the left we are seeing the probability of a positive
    or a negative sentence. On the bottom left we are seeing the resulting probabilities
    following the computation. At this point, they are in proportion to each other,
    but they don’t tell us much in terms of probabilities. To get the probabilities,
    we need to normalize the values, arriving at P(positive) = 0.6837 and P(negative)
    = 0.3163\. The strength of naive Bayes is that it is sensitive to words that occur
    more often in one type of sentence than in the other. In our case, the word “loved”
    occurs much more often in positive sentences, which makes the whole sentence more
    likely to be positive than negative. To see an implementation of sentiment assessment
    using Naive Bayes with the nltk library, refer to sentiment.py.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在右侧，我们看到一个表格，其中包含左侧每个词在句子中出现的条件概率，前提是句子是积极的或消极的。在左侧的小表格中，我们看到积极或消极句子的概率。在左下角，我们看到计算后的结果概率。在这个阶段，它们之间是成比例的，但它们在概率方面并没有告诉我们太多。为了得到概率，我们需要归一化这些值，得到P(positive)
    = 0.6837和P(negative) = 0.3163。朴素贝叶斯的优势在于它对在一个类型的句子中比另一个类型句子中出现频率更高的词很敏感。在我们的例子中，单词“loved”在积极句子中出现的频率更高，这使得整个句子更有可能被判定为积极而不是消极。要查看使用nltk库实现的朴素贝叶斯情感评估的示例，请参考sentiment.py。
- en: One problem that we can run into is that some words may never appear in a certain
    type of sentence. Suppose none of the positive sentences in our sample had the
    word “grandson.” Then, P(“grandson” | positive) = 0, and when computing the probability
    of the sentence being positive we will get 0\. However, this is not the case in
    reality (not all sentences mentioning grandsons are negative). One way to go about
    this problem is with **Additive Smoothing**, where we add a value α to each value
    in our distribution to smooth the data. This way, even if a certain value is 0,
    by adding α to it we won’t be multiplying the whole probability for a positive
    or negative sentence by 0\. A specific type of additive smoothing, **Laplace Smoothing**
    adds 1 to each value in our distribution, pretending that all values have been
    observed at least once.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会遇到的一个问题是，某些词可能永远不会出现在某种类型的句子中。假设我们样本中的所有积极句子都没有“孙子”这个词。那么，P(“孙子” | 积极)
    = 0，在计算句子为积极的概率时，我们会得到0。然而，在现实中并非如此（提到孙子的句子并不都是消极的）。解决这个问题的方法之一是**加性平滑**，即在我们分布的每个值上添加一个值α来平滑数据。这样，即使某个值是0，通过向其添加α，我们也不会将正句或负句的整个概率乘以0。一种特定的加性平滑方法，**拉普拉斯平滑**，将1加到我们分布的每个值上，假装所有值都至少被观察过一次。
- en: Word Representation
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词表示
- en: 'We want to represent word meanings in our AI. As we’ve seen before, it is convenient
    to provide input to the AI in the form of numbers. One way to go about this is
    by using **One-Hot Representation**, where each word is represented with a vector
    that consists of as many values as we have words. Except for a single value in
    the vector that is equal to 1, all other values are equal to 0\. How we can differentiate
    words is by which of the values is 1, ending up with a unique vector per word.
    For example, the sentence “He wrote a book” can be represented as four vectors:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想在我们的AI中表示词义。正如我们之前看到的，以数字形式向AI提供输入是方便的。解决这个问题的方法之一是使用**独热表示**，其中每个词用一个向量表示，该向量包含与我们有相同数量的值。除了向量中的一个值等于1之外，所有其他值都等于0。我们可以通过哪个值是1来区分单词，最终为每个单词得到一个唯一的向量。例如，句子“他写了一本书”可以表示为四个向量：
- en: '[1, 0, 0, 0] (he)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1, 0, 0, 0] (他)'
- en: '[0, 1, 0, 0] (wrote)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[0, 1, 0, 0] (已写)'
- en: '[0, 0, 1, 0] (a)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[0, 0, 1, 0] (a)'
- en: '[0, 0, 0, 1] (book)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[0, 0, 0, 1] (书)'
- en: 'However, while this representation works in a world with four words, if we
    want to represent words from a dictionary, when we can have 50,000 words, we will
    end up with 50,000 vectors of length 50,000\. This is incredibly inefficient.
    Another problem in this kind of representation is that we are unable to represent
    similarity between words like “wrote” and “authored.” Instead, we turn to the
    idea of **Distributed Representation**, where meaning is distributed across multiple
    values in a vector. With distributed representation, each vector has a limited
    number of values (much less than 50,000), taking the following form:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，虽然这种表示在只有四个词的世界中是有效的，但如果我们想表示词典中的词，当我们有50,000个词时，我们最终会得到50,000个长度为50,000的向量。这是极其低效的。这种表示方式中的另一个问题是，我们无法表示像“wrote”和“authored”这样的词之间的相似性。因此，我们转向**分布式表示**的想法，其中意义分布在向量中的多个值上。在分布式表示中，每个向量有有限数量的值（远少于50,000），其形式如下：
- en: '[-0.34, -0.08, 0.02, -0.18, …] (he)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[-0.34, -0.08, 0.02, -0.18, …] (他)'
- en: '[-0.27, 0.40, 0.00, -0.65, …] (wrote)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[-0.27, 0.40, 0.00, -0.65, …] (写了)'
- en: '[-0.12, -0.25, 0.29, -0.09, …] (a)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[-0.12, -0.25, 0.29, -0.09, …] (a)'
- en: '[-0.23, -0.16, -0.05, -0.57, …] (book)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[-0.23, -0.16, -0.05, -0.57, …] (书中)'
- en: This allows us to generate unique values for each word while using smaller vectors.
    Additionally, now we are able to represent similarity between words by how different
    the values in their vectors are.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够为每个词生成独特的值，同时使用较小的向量。此外，现在我们能够通过它们向量中值的差异来表示词之间的相似性。
- en: “You shall know a word by the company it keeps” is an idea by J. R. Firth, an
    English linguist. Following this idea, we can come to define words by their adjacent
    words. For example, there are limited words that we can use to complete the sentence
    “for ___ he ate.” These words are probably words like “breakfast,” “lunch,” and
    “dinner.” This brings us to the conclusion that by considering the environment
    in which a certain word tends to appear, we can infer the meaning of the word.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: “你将通过与你相伴的词来认识一个词”是J. R. Firth，一位英国语言学家的一个想法。遵循这个想法，我们可以通过定义词的相邻词来定义词。例如，我们可以用有限的词来完成句子“for
    ___ he ate.” 这些词可能是像“breakfast”、“lunch”和“dinner”这样的词。这使我们得出结论，通过考虑某个词倾向于出现的环境，我们可以推断出该词的意义。
- en: word2vec
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: word2vec
- en: word2vec is an algorithm for generating distributed representations of words.
    It does so by **Skip-Gram Architecture**, which is a neural network architecture
    for predicting context given a target word. In this architecture, the neural network
    has an input unit for every target word. A smaller, single hidden layer (e.g.
    50 or 100 units, though this number is flexible) will generate values that represent
    the distributed representations of words. Every unit in this hidden layer is connected
    to every unit in the input layer. The output layer will generate words that are
    likely to appear in a similar context as the target words. Similar to what we
    saw in last lecture, this network needs to be trained with a training dataset
    using the backpropagation algorithm.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec 是一种生成单词分布式表示的算法。它通过 **Skip-Gram 架构** 来实现，这是一种针对给定目标词预测上下文的神经网络架构。在这个架构中，神经网络为每个目标词都有一个输入单元。一个较小的、单一的隐藏层（例如，50
    或 100 个单元，尽管这个数字是灵活的）将生成代表单词分布式表示的值。隐藏层中的每个单元都与输入层中的每个单元相连。输出层将生成与目标词在相似上下文中可能出现的单词。类似于我们在上一节课中看到的，这个网络需要使用训练数据集并通过反向传播算法进行训练。
- en: '![Skip-Gram Architecture](../Images/400ad9c5718ec18cd3f2ceb1cead9358.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![Skip-Gram 架构](../Images/400ad9c5718ec18cd3f2ceb1cead9358.png)'
- en: This neural network turns out to be quite powerful. At the end of the process,
    every word ends up being just a vector, or a sequence of numbers. For example,
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经网络证明非常强大。在处理过程的最后，每个单词最终都变成一个向量，或者一系列数字。例如，
- en: 'book: [-0.226776 -0.155999 -0.048995 -0.569774 0.053220 0.124401 -0.091108
    -0.606255 -0.114630 0.473384 0.061061 0.551323 -0.245151 -0.014248 -0.210003 0.316162
    0.340426 0.232053 0.386477 -0.025104 -0.024492 0.342590 0.205586 -0.554390 -0.037832
    -0.212766 -0.048781 -0.088652 0.042722 0.000270 0.356324 0.212374 -0.188433 0.196112
    -0.223294 -0.014591 0.067874 -0.448922 -0.290960 -0.036474 -0.148416 0.448422
    0.016454 0.071613 -0.078306 0.035400 0.330418 0.293890 0.202701 0.555509 0.447660
    -0.361554 -0.266283 -0.134947 0.105315 0.131263 0.548085 -0.195238 0.062958 -0.011117
    -0.226676 0.050336 -0.295650 -0.201271 0.014450 0.026845 0.403077 -0.221277 -0.236224
    0.213415 -0.163396 -0.218948 -0.242459 -0.346984 0.282615 0.014165 -0.342011 0.370489
    -0.372362 0.102479 0.547047 0.020831 -0.202521 -0.180814 0.035923 -0.296322 -0.062603
    0.232734 0.191323 0.251916 0.150993 -0.024009 0.129037 -0.033097 0.029713 0.125488
    -0.018356 -0.226277 0.437586 0.004913]'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 书籍：[-0.226776 -0.155999 -0.048995 -0.569774 0.053220 0.124401 -0.091108 -0.606255
    -0.114630 0.473384 0.061061 0.551323 -0.245151 -0.014248 -0.210003 0.316162 0.340426
    0.232053 0.386477 -0.025104 -0.024492 0.342590 0.205586 -0.554390 -0.037832 -0.212766
    -0.048781 -0.088652 0.042722 0.000270 0.356324 0.212374 -0.188433 0.196112 -0.223294
    -0.014591 0.067874 -0.448922 -0.290960 -0.036474 -0.148416 0.448422 0.016454 0.071613
    -0.078306 0.035400 0.330418 0.293890 0.202701 0.555509 0.447660 -0.361554 -0.266283
    -0.134947 0.105315 0.131263 0.548085 -0.195238 0.062958 -0.011117 -0.226676 0.050336
    -0.295650 -0.201271 0.014450 0.026845 0.403077 -0.221277 -0.236224 0.213415 -0.163396
    -0.218948 -0.242459 -0.346984 0.282615 0.014165 -0.342011 0.370489 -0.372362 0.102479
    0.547047 0.020831 -0.202521 -0.180814 0.035923 -0.296322 -0.062603 0.232734 0.191323
    0.251916 0.150993 -0.024009 0.129037 -0.033097 0.029713 0.125488 -0.018356 -0.226277
    0.437586 0.004913]
- en: 'By themselves, these numbers don’t mean much. But by finding which other words
    in the corpus have the most similar vectors, we can run a function that will generate
    the words that are the most similar to the word *book*. In the case of this network
    it will be: book, books, essay, memoir, essays, novella, anthology, blurb, autobiography,
    audiobook. This is not bad for a computer! Through a bunch of numbers that don’t
    carry any specific meaning themselves, the AI is able to generate words that really
    are very similar to *book* not in letters or sounds, but in meaning! We can also
    compute the difference between words based on how different their vectors are.
    For example, the difference between *king* and *man* is similar to the difference
    between *queen* and *woman*. That is, if we add the difference between *king*
    and *man* to the vector for *woman*, the closest word to the resulting vector
    is *queen*! Similarly, if we add the difference between *ramen* and *japan* to
    *america*, we get *burritos*. By using neural networks and distributed representations
    for words, we get our AI to understand semantic similarities between words in
    the language, bringing us one step closer to AIs that can understand and produce
    human language.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字本身并没有什么意义。但是，通过找到语料库中与这些数字最相似的词汇，我们可以运行一个函数，生成与单词 *book* 最相似的词汇。在这个网络中，这些词汇将是：book,
    books, essay, memoir, essays, novella, anthology, blurb, autobiography, audiobook。这对于计算机来说已经很不错了！通过一些本身没有特定意义的数字，人工智能能够生成与
    *book* 在意义而非字母或声音上非常相似的词汇！我们还可以根据词汇向量之间的差异来计算词汇之间的差异。例如，*king* 和 *man* 之间的差异类似于
    *queen* 和 *woman* 之间的差异。也就是说，如果我们把 *king* 和 *man* 之间的差异加到 *woman* 的向量上，与结果向量最接近的词汇是
    *queen*！同样地，如果我们把 *ramen* 和 *japan* 之间的差异加到 *america* 上，我们得到 *burritos*。通过使用神经网络和词汇的分布式表示，我们使我们的AI能够理解语言中词汇之间的语义相似性，使我们更接近能够理解和生成人类语言的AI。
- en: Neural Networks
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: Recall that a **neural network** takes some input, passes it to the network,
    and creates some output. By providing the network with training data, it can do
    more and more of an accurate job of translating the input into an output. Commonly,
    machine translation uses neural networks. In practice, when we are translating
    words, we want to translate a sentence or paragraph. Since a sentence is a fixed
    size, we run into the problem of translating a sequence to another sequence where
    sizes are not fixed. If you have ever had a conversation with an AI chatbot, it
    needs to understand a sequence of words and generate an appropriate sequence as
    output.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，**神经网络**接受一些输入，将其传递到网络中，并创建一些输出。通过向网络提供训练数据，它可以越来越准确地翻译输入为输出。通常，机器翻译使用神经网络。在实践中，当我们翻译词汇时，我们希望翻译一个句子或段落。由于句子是固定大小的，我们遇到了将一个序列翻译为另一个序列的问题，其中大小不是固定的。如果你曾经与一个AI聊天机器人交谈过，它需要理解一个词汇序列并生成一个适当的输出序列。
- en: '**Recurrent neural networks** can re-run the neural network multiple times,
    keeping track of a state that holds all relevant information. Input is taken into
    the network, creating a hidden state. Passing a second input into the encoder,
    along with the first hidden state, produces a new hidden state. This process is
    repeated until an end token is passed. Then, a decoding state begins, creating
    hidden state after hidden state until we get the final word and another end token.
    Some problems, however, arise. One problem in the encoder stage where all the
    information from the input stage must be stored in one final state. For large
    sequences, it’s very challenging to store all that information into a single state
    value. It would be useful to somehow combine all the hidden states. Another problem
    is that some of the hidden states in the input sequence are more important than
    others. Could there be some way to know what states (or words) are more important
    than others?'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环神经网络**可以多次运行神经网络，同时跟踪一个包含所有相关信息的状态。输入被输入到网络中，创建一个隐藏状态。将第二个输入传递到编码器，同时带有第一个隐藏状态，产生一个新的隐藏状态。这个过程会重复进行，直到传递一个结束标记。然后，开始解码状态，创建一个隐藏状态接着一个隐藏状态，直到我们得到最终的词汇和另一个结束标记。然而，一些问题也随之而来。编码阶段的一个问题是，所有来自输入阶段的信息必须存储在一个最终状态中。对于长序列，将所有这些信息存储到一个单一的状态值中是非常具有挑战性的。如果能以某种方式组合所有隐藏状态将是有用的。另一个问题是，输入序列中的某些隐藏状态比其他状态更重要。是否有可能知道哪些状态（或词汇）比其他状态更重要？'
- en: Attention
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意
- en: '**Attention** refers to the neural network’s ability to decide what values
    are more important than others. In the sentence “What is the capital of Massachusetts,”
    attention allows the neural network to decide what values it will pay attention
    to at each stage of generating the output sentence. Running such a calculation,
    the neural network will show that when generating the final word of the answer,
    “capital” and “Massachusetts” are the most important to pay attention to. By taking
    the attention score, multiplying them by the hidden state values generated by
    the network, and adding them up, the neural network will create a final context
    vector that the decoder can use to calculate the final word. A challenge that
    arises in calculations such as these is that recurrent neural networks require
    sequential training of word after word. This takes a lot of time. With the growth
    of large language models, they take longer and longer to train. A desire for parallelism
    has steadily grown as larger and larger datasets need to be trained. Hence, a
    new architecture has been introduced.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意力**指的是神经网络决定哪些值比其他值更重要。在句子“马萨诸塞州的首府是什么？”中，注意力使神经网络能够决定在生成输出句子的每个阶段它将关注哪些值。进行这样的计算，神经网络将显示，在生成答案的最后一个词“capital”和“Massachusetts”是最需要关注的。通过取注意力分数，将它们乘以网络生成的隐藏状态值，并将它们相加，神经网络将创建一个解码器可以用来计算最后一个词的最终上下文向量。在这些计算中出现的挑战是，循环神经网络需要逐词顺序训练。这需要花费大量时间。随着大型语言模型的增长，它们的训练时间越来越长。随着需要训练的更大数据集的出现，对并行化的需求稳步增长。因此，引入了一种新的架构。'
- en: Transformers
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换器
- en: '**Transformers** is a new type of training architecture whereby each input
    word is passed through a neural network simultaneously. An input word goes into
    the neural network and is captured as an encoded representation. Because all words
    are fed into the neural network at the same time, word order could easily be lost.
    Accordingly, **position encoding** is added to the inputs. The neural network,
    therefore, will use both the word and the position of the word in the encoded
    representation. Additionally, a **self-attention** step is added to help define
    the context of the word being inputted. In fact, neural networks will often use
    multiple self-attention steps such that they can further understand the context.
    This process is repeated multiple times for each of the words in the sequence.
    What results are encoded representations that will be useful when it’s time to
    decode the information.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformers**是一种新的训练架构，其中每个输入词同时通过神经网络。一个输入词进入神经网络，并被捕获为一个编码表示。由于所有单词同时输入神经网络，单词顺序很容易丢失。因此，**位置编码**被添加到输入中。因此，神经网络将使用单词及其在编码表示中的位置。此外，添加了一个**自注意力**步骤来帮助定义输入单词的上下文。实际上，神经网络通常会使用多个自注意力步骤，以便它们可以进一步理解上下文。这个过程对序列中的每个单词重复多次。结果是编码表示，在解码信息时将非常有用。'
- en: In the decoding step, the previous output word and its positional encoding are
    given to multiple self-attention steps and the neural network. Additionally, multiple
    attention steps are fed the encoded representation from the encoding process and
    provided to the neural network. Hence, words are able to pay attention to each
    other. Further, parallel processing is possible, and the calculations are fast
    and accurate.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在解码步骤中，前一个输出词及其位置编码被提供给多个自注意力步骤和神经网络。此外，多个注意力步骤被输入编码过程中的编码表示，并提供给神经网络。因此，单词能够相互关注。进一步来说，并行处理成为可能，计算既快又准确。
- en: Summing Up
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: We have looked at artificial intelligence in a wide array of contexts. We looked
    at search problems in how AI can look for solutions. We looked at how AI represents
    knowledge and create knowledge. We looked at uncertainty when it does not know
    things for sure. We looked at optimization, maximizing and minimizing function.
    We looked at machine learning, finding patterns by looking at training data. We
    learned about neural networks and how they use weights to go from input to output.
    Today, we looked at language itself and how we can get the computer to understand
    our language. We have just scratched the surface of this process. We truly hope
    you enjoyed this journey with us. This was Introduction to Artificial Intelligence
    with Python.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在多种情境下探讨了人工智能。我们研究了人工智能如何寻找解决方案的搜索问题。我们探讨了人工智能如何表示知识和创造知识。我们研究了当它不确定某些事情时的情况。我们研究了优化、最大化函数和最小化函数。我们研究了通过观察训练数据来寻找模式的机器学习。我们学习了神经网络以及它们如何使用权重从输入到输出。今天，我们探讨了语言本身以及我们如何让计算机理解我们的语言。我们只是刚刚触及了这个过程的表面。我们真心希望您喜欢与我们一同经历的这段旅程。这是《使用Python的人工智能入门》。
