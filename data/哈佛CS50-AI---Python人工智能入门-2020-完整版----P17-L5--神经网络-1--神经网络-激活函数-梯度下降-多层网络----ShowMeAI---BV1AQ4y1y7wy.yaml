- en: 哈佛CS50-AI ｜ Python人工智能入门(2020·完整版) - P17：L5- 神经网络 1 (神经网络，激活函数，梯度下降，多层网络) -
    ShowMeAI - BV1AQ4y1y7wy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 哈佛CS50-AI ｜ Python人工智能入门(2020·完整版) - P17：L5- 神经网络 1 (神经网络，激活函数，梯度下降，多层网络) -
    ShowMeAI - BV1AQ4y1y7wy
- en: '![](img/7967c64b75ab3f79ac5ab1a77fcf79aa_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7967c64b75ab3f79ac5ab1a77fcf79aa_0.png)'
- en: '[Music]，all right welcome back everyone to an，introduction to artificial intelligence。with
    Python now last time we took a look，at machine learning a set of techniques。that
    computers can use in order to take，a set of data and learn some patterns。inside
    of that data learn how to perform，a task even if we the programmers didn''t。'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[音乐] 好的，欢迎大家回到《Python人工智能导论》。上次我们看到了机器学习，这是一组计算机可以用来处理数据并学习数据内模式的技术，学习如何执行任务，即使我们程序员并没有。'
- en: give the computer explicit instructions，for how to perform that task today we。transition
    to one of the most popular，techniques and tools within machine。learning that of
    neural networks and，neural networks were inspired as early。as the 1940s by researchers
    who were，thinking about how it is that humans。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 给计算机明确的指令，说明如何执行这个任务，今天我们转向机器学习中最流行的技术和工具之一——神经网络，神经网络早在1940年代就受到研究人员的启发，他们思考人类是如何学习的。
- en: learn studying neuroscience in the human，brain and trying to see whether or
    not。we could apply those same ideas to，computers as well and model computer。learning
    off of human learning so how is，the brain structured well very simply。put the
    brain is consists of a whole，bunch of neurons and those neurons are。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 学习人类神经科学的大脑，并尝试看看我们是否可以将这些相同的思想应用于计算机，模拟计算机学习以人类学习为基础。那么，大脑是如何结构的呢？简单来说，大脑由许多神经元组成，这些神经元相互连接。
- en: connected to one another and communicate，with one another in some way in。particular
    if you think about the，structure of a biological neural network。something like
    this there are a couple，of key properties of scientists observed。one was that
    these neurons are connected，to each other and receive electrical。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这些神经元以某种方式相互交流，特别是如果你考虑生物神经网络的结构，科学家们观察到了一些关键特性。其中之一是这些神经元彼此连接，并接收来自其他神经元的电信号。
- en: signals from one another that one neuron，can propagate electrical signals to。another
    neuron and another point is that，neurons process those input signals and。then
    can be activated that a neuron，becomes activated at a certain point and。then can
    propagate further signals onto，neurons in the future and so the。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 一个神经元可以将电信号传播给另一个神经元，另一个要点是神经元处理这些输入信号，然后可以被激活，神经元在某个时刻被激活，随后可以向其他神经元传播进一步的信号。
- en: question then became could we take this，biological idea of how it is that humans。learn
    with brains and with neurons and，apply that to a machine as well in。effect designing
    an artificial neural，network or an A and n which will be a。mathematical model
    for learning that is，inspired by these biological neural。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 问题随之而来，我们能否将这种人类如何用大脑和神经元学习的生物学理念应用于机器，设计出一个人工神经网络（ANN），这将是一个受到这些生物神经网络启发的学习数学模型。
- en: networks and what artificial neural，networks will allow us to do is they。will
    first be able to model some sort of，mathematical function every time you。look
    at a neural network which we'll see，more of later today each one of them is。really
    just a mathematical function that，is mapping certain inputs to particular。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 网络以及人工神经网络将允许我们做的事情是，它们首先能够模拟某种数学函数。每次你查看神经网络时，我们今天稍后会看到，它们实际上只是将某些输入映射到特定输出的数学函数。
- en: outputs based on the structure of the，network that depending on where we place。particularly
    units inside of the sonoran。![](img/7967c64b75ab3f79ac5ab1a77fcf79aa_2.png)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 基于网络的结构输出，取决于我们在神经元内部放置的特定单元的位置。![](img/7967c64b75ab3f79ac5ab1a77fcf79aa_2.png)
- en: network that's going to determine how it，is that the network is going to function。and
    in particular artificial neural，networks are going to lend themselves to。a way
    that we can learn what the，network's parameters should be we'll see。more on that
    in just a moment but in，effect we want to model such that it is。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 网络将决定网络如何功能，特别是人工神经网络将让我们学习网络参数应该是什么，我们稍后将深入探讨，但实际上我们想要模拟的是网络结构。
- en: easy for us to be able to，some code that allows for the network to。be able to
    figure out how to model the，right mathematical function given a。particular set
    of input data so in order，to create our artificial neural network。instead of using
    biological neurons，we're just going to use what we're gonna。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 使我们能够容易地得到一些代码，以便网络能够确定如何建模给定特定输入数据的正确数学函数，因此为了创建我们的人工神经网络，不使用生物神经元，我们将使用我们要使用的。
- en: call unit units inside of a neural，network which we can represent kind of。like
    a node in a graph which will here，be represented just by a blue circle。like this
    and these artificial units，these artificial neurons can be。connected to one another
    so here for，instance we have two units that are。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络内部称为单位的单元，我们可以把它表示为图中的一个节点，这里用一个蓝色圆圈来表示。像这样，这些人工单元，这些人工神经元可以彼此连接，因此这里，例如，我们有两个单元。
- en: connected by this edge inside of this。![](img/7967c64b75ab3f79ac5ab1a77fcf79aa_4.png)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个边连接在这个。![](img/7967c64b75ab3f79ac5ab1a77fcf79aa_4.png)
- en: graph effectively and so what we're，going to do now is think of this idea as。some
    sort of mapping from inputs to，outputs so we have one unit that is。connected to
    another unit that we might，think of this side of the input and that。side of the
    output and what we're trying，to do then is to figure out how to solve。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 有效地表示图形，所以我们现在要做的是将这个想法视为某种从输入到输出的映射，因此我们有一个单元，它连接到另一个单元，我们可以把这边看作输入，那边看作输出，我们试图做的是弄清楚如何解决。
- en: a problem how to model some sort of，mathematical function and this might。take
    the form of something we saw last。![](img/7967c64b75ab3f79ac5ab1a77fcf79aa_6.png)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个问题是如何建模某种数学函数，这可能以我们之前看到的形式出现。![](img/7967c64b75ab3f79ac5ab1a77fcf79aa_6.png)
- en: time which was something like we have，certain inputs like variables x1 and x2。and
    given those inputs we want to，perform some sort of task a task like。predicting
    whether or not it's going to，rain and ideally we'd like some way。given these inputs
    x1 and x2 which stand，for some sort of variables to do with。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 时间上，输入有变量x1和x2，给定这些输入，我们想要执行某种任务，比如预测是否会下雨，理想情况下，我们希望有某种方式，基于这些输入x1和x2，它们代表与之相关的某种变量。
- en: the weather we would like to be able to，predict in this case a boolean。classification
    is it going to rain or is，it not going to rain and we did this。last time by way
    of a mathematical，function we define some function H for。our hypothesis function
    that took as，input x1 and x2 the two inputs that we。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 天气，我们希望能够预测，在这种情况下是布尔分类，它会下雨还是不会下雨，我们上次通过一种数学函数来完成这一点，我们为我们的假设函数定义了某个函数H，它以x1和x2作为输入，这两个输入我们。
- en: cared about processing in order to，determine whether we thought it was。going
    to rain or whether we thought it，was not going to rain the question then。becomes
    what does this hypothesis，function do in order to make that。determination and
    we decided last time，to use a linear combination of these。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 关注处理，以确定我们是否认为会下雨，或者我们认为不会下雨，问题变成了这个假设函数如何做出这个判断，我们上次决定使用这些的线性组合。
- en: input variables to determine what the，output should be so our hypothesis。function
    was equal to something like，this weight 0 plus weight 1 times x1。plus weight 2
    times x2 so what's going，on here is that x1 and x2。those are input variables the
    inputs to，this hypothesis function and each of。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 输入变量，以确定输出应该是什么，因此我们的假设函数等于像这样的东西，权重0加权重1乘以x1，加权重2乘以x2，因此这里发生的事情是x1和x2。这些是输入变量，输入到这个假设函数中。
- en: those input variables is being，multiplied by some weight which is just。some
    number so x1 is being multiplied by，weight 1 x2 is being multiplied by。weight
    2 and we have this，wait wait zero that doesn't get，multiplied by an input variable
    at all。that just serves to either move the，function up or move the functions value。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些输入变量正在被某些权重相乘，这些权重只是一些数字，因此x1被乘以权重1，x2被乘以权重2，而我们有这个，等等，零并没有被输入变量相乘。它只是用来移动函数的值。
- en: down you can think of this as either a，weight that's just multiplied by some。dummy
    value like the number one it's，multiplied by one and so it's not。multiplied by
    anything or sometimes，you'll see in the literature people call。this variable weight
    zero a bias so that，you can think of these variables are。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 向下看，你可以把它视为一个权重，它只是与某个虚拟值相乘，比如数字1，它乘以1，因此没有被乘以任何东西，或者有时在文献中，人们称这个变量权重0为偏差，因此你可以将这些变量视为。
- en: slightly different we have weights that，are multiplied by the input and we。separately
    add some bias to the result，as well you'll hear both of those。terminologies used
    when people talk，about neural networks and machine。learning so in effect what
    we've done，here is that in order to define a。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 略有不同的是，我们有权重乘以输入，然后单独将某些偏差加到结果中，你会听到在谈论神经网络和机器学习时使用这两种术语，因此实际上我们在这里做的就是为了定义一个。
- en: hypothesis function we just need to，decide and figure out what these weights。should
    be to determine what values to，multiply by our inputs to get some sort。of result
    of course at the end of this，what we need to do is make some sort of。classification
    like rainy or not rainy，and to do that we use some sort of。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于假设函数，我们只需要决定和计算这些权重应该是什么，以确定将哪些值乘以我们的输入，从而得到某种结果，当然在这一切的最后，我们需要进行某种分类，比如**下雨或不下雨**，为此我们使用某种方法。
- en: function so that defines some sort of，threshold and so we saw for instance the。step
    function which is defined as 1 if，the weight if the result of multiplying。the
    weights by the inputs is at least，zero otherwise it's zero and you can。think of
    this line down the middle as，kind of like a dotted line effectively。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这定义了一种阈值，我们看到例如**阶跃函数**的定义是，如果将权重与输入相乘的结果至少为零，则定义为 1，否则为零，你可以将这条中间的线看作是一条虚线。
- en: it stays at zero all the way up to one，point and then the function steps or。jumps
    up to one so it's zero before it，reaches some threshold and then it's one。after
    it reaches a particular threshold。![](img/7967c64b75ab3f79ac5ab1a77fcf79aa_8.png)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 它在达到一个点之前保持在零，然后函数在达到某个阈值后跳跃到 1，因此在到达某个阈值之前为零，然后在达到特定阈值后为 1。![](img/7967c64b75ab3f79ac5ab1a77fcf79aa_8.png)
- en: and so this was one way we could define，what will come to call an activation。function
    a function that determines when，it is this output becomes active changes。to a
    z1 instead of being a zero but we，also saw that if we didn't just want a。purely
    binary classification we didn't，want purely one or a zero but we wanted。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这是一种我们可以定义**激活函数**的方法，激活函数决定何时其输出变为活跃，变为 z1 而不是零，但我们也看到，如果我们不想要纯粹的二元分类，不希望只是
    0 或 1，而是想要。
- en: to allow for some in-between real，numbered values we could use a different。function
    and there are a number of，choices but the one that we looked at。was the logistic
    sigmoid function that，has sort of an s-shaped curve where we。could represent this
    as a probability，that maybe somewhere in between the。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了允许一些介于真实、编号值之间的值，我们可以使用不同的函数，有很多选择，但我们关注的是**逻辑 sigmoid 函数**，它呈现一种 S 形曲线，我们可以将其表示为一个概率，也许在这之间的某个地方。
- en: probability of rain of something like，0。5 maybe a little bit later the，probability
    of rain is 0。8 and so rather，than just have a binary classification，of 0 or 1
    we can allow for numbers that。are in between as well and it turns out，there are
    many other different types of。activation functions where an activation，function
    just takes the output of。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 比如雨的概率为 0.5，也许稍后雨的概率是 0.8，因此与其仅有 0 或 1 的二元分类，我们可以允许介于这之间的数值，结果显示有许多不同类型的激活函数，激活函数仅仅是输出某种函数。
- en: multiplying the weights together and out，that bias and then figuring out what
    the。actual output should be another popular，one is the rectified linear unit。otherwise
    known as relu and the way that，works is that it just takes its input。and takes
    the maximum of that input and，0 so if it's positive it remains。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 将权重相乘，加上偏差，然后确定实际输出，另一个流行的函数是**整流线性单元**，也称为 relu，运作方式是它取其输入与 0 的最大值，因此如果它是正值，它保持。
- en: unchanged but if it's 0 if it's negative，it goes ahead and levels out at zero
    and。there are other activation functions，that we can choose as well but in short。each
    of these activation functions you，can just think of as a function that。gets applied
    to the result of all of，this computation we take some function G。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 依然保持不变，但如果它为 0 或负值，则将其平滑到零，还有其他激活函数可供选择，但简而言之，每个激活函数可以看作是应用于所有计算结果的函数，我们取某个函数
    G。
- en: and apply it to the result of all of，that calculation and this then is what。we
    saw last time the way of defining，some hypothesis function that takes in。inputs
    calculate some linear combination，of those inputs and then passes it。through some
    sort of activation function，to get our output and this actually。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 并将其应用于所有计算的结果，这就是我们上次看到的定义某些假设函数的方法，该函数接收输入，计算这些输入的某种线性组合，然后通过某种激活函数得到输出，这实际上。
- en: turns out to be the model for the，simplest of neural networks that we're。going
    to instead represent this，mathematical idea graphically by using a。structure like
    this here then as a，neural network that has two inputs we。can think of this as
    x1 and this is x2，and then one output which you can think。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，这是最简单的神经网络模型，我们将用这样的结构来图形化表示这个数学概念，作为一个有两个输入的神经网络，我们可以把这看作是 x1 和 x2，然后有一个输出，你可以认为。
- en: of a classifying whether or not we think，it's going to rain or not rain for。example
    in this particular instance and，so how exactly does this model work well。each
    of these two inputs represents one，of our input variables x1 and x2 and。notice
    that these inputs are connected，to this output via these edges which are。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，判断我们是否认为会下雨，在这个特定实例中。那么这个模型到底是如何工作的呢？这两个输入代表我们的输入变量 x1 和 x2，注意这些输入通过这些边连接到这个输出。
- en: going to be defined by their weights so，these edges each have a weight。associated
    with them weight 1 and weight，- and then this output unit what it's。going to do
    is it is going to calculate，an output based on those inputs and。based on those
    weights this output unit，is going to multiply all the inputs by。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 将通过他们的权重来定义这些边，每条边都有一个权重，权重 1 和权重 - ，然后这个输出单元将根据这些输入和权重计算输出，这个输出单元将所有输入相乘。
- en: their weights add in this bias term，which you can think of as an extra w0。term
    they get to add it into it and then，we pass it through an activation。function
    so this then is just a，graphical way of representing the same，idea we saw last
    time just。mathematically and we're gonna call this，a very simple neural network
    and we'd。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的权重中添加这个偏置项，你可以把它看作是一个额外的 w0 项，然后我们通过激活函数传递它。因此，这只是一个图形化表示上次我们看到的同样想法的数学形式，我们将称之为一个非常简单的神经网络。
- en: like for this a neural network to be，able to learn how to calculate some。function
    that we want some function for，the neural network to learn and the。neural network
    is going to learn what，should the values of w0 w1 and w2 be。what should the activation
    function，in order to get the result that we would。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这个神经网络能够学习如何计算我们想要的某个函数，而神经网络将学习 w0、w1 和 w2 的值应该是什么。激活函数应该是什么，以获得我们想要的结果。
- en: expect so we can actually take a look at，an example of this what then is a very。simple
    function that we might calculate，well if we recall back from when we were。looking
    at propositional logic one of，the simplest functions we looked at with。something
    like the or function that，takes two inputs X and y and outputs one。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看看这个示例，实际上是一个非常简单的函数，我们可能会计算，如果我们回顾一下在命题逻辑中，我们看到的最简单的函数之一就是类似于或函数的，它接受两个输入
    X 和 y 并输出一个。
- en: otherwise known is true if either one of，the inputs or both of them are 1 and。outputs
    a 0 if both of the inputs are 0，or false so this then is the or function。and this
    was the truth table for the or，function that as long as either of the。inputs are
    1 the output of the function，is 1 and the only case where the output。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当任一输入或两个输入都是 1 时，这被称为真；而当两个输入都是 0 时，输出为 0，即假。因此，这就是或函数，而这是或函数的真值表，只要任一输入为 1，函数的输出就是
    1，唯一的情况是输出。
- en: is 0 is where both of the inputs are 0，so the question is how could we take。this
    and train a neural network to be，able to learn this particular function。what would
    those weights look like well，we could do something like this here's。our neural
    network and I'll propose that，in order to calculate the or function。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个输入都是 0 时，输出为 0。那么问题是，我们如何能够训练一个神经网络以学习这个特定的函数？那些权重会是什么样子呢？我们可以这样做，这是我们的神经网络，我提议为了计算或函数。
- en: we're going to use a value of 1 for each，of the weights and we'll use a bias
    of。negative 1 and then we'll just use this，step function as our activation function。how
    then does this work well if I wanted，to calculate something like 0 or 0 which。we
    know to be 0 because false or false，is false then what are we going to do。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为每个权重使用值 1，并将使用负 1 作为偏置，然后我们将使用这个阶跃函数作为我们的激活函数。那么这如何运作呢？如果我想计算 0 或 0，显然结果是
    0，因为假或假为假，那么我们该怎么做呢？
- en: well our output unit is going to，calculate this input multiplied by the。weight
    0 times 1 that's 0 same thing，here 0 times 1 that's 0 and we'll add to。that the
    bias minus 1 so that'll give us，a result of negative 1 if we plot that。on our
    activation function negative 1 is，here it's before the threshold which。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输出单元将计算这个输入乘以权重，0 乘以 1 是 0，同样，这里 0 乘以 1 也是 0，我们还会加上偏置负 1，所以我们得到的结果是负 1，如果我们将其绘制在激活函数上，负
    1 在这里，处于阈值之前。
- en: means either 0 or 1 it's only one after，the threshold since negative 1 is before。the
    threshold the output that this unit，provides is going to be 0 and that's。what
    we would expect it to be that 0 or，0 should be 0 what if instead we had had。1
    or 0 where this is the number 1 well，in this case in order to calculate what。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着要么是 0，要么是 1，只有在阈值之后，因为负 1 在阈值之前，这个单元提供的输出将是 0，这就是我们期望的，0 或 0 应该为 0。如果我们改为
    1 或 0，这个数字是 1，那么在这种情况下，为了计算超出阈值的值，输出将是 1。
- en: the output is going to be we again have，to do this weighted sum 1 times 1 that's。1
    0 times 1 that's 0 sum of that so far，is 1 add negative 1 to that well then。the
    output is 0 and if we plot 0 on the，step function 0 ends up being here it's。just
    at the threshold and，the output here is going to be one。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将是我们再次需要做加权和，1 乘以 1 是 1，0 乘以 1 是 0，到目前为止的总和是 1，加上负 1，那么输出为 0，如果我们将 0 在阶跃函数上绘制，0
    就在这里，正好在阈值上，输出将在这里为 1。
- en: because the output of one or zero that's，what so that's what we would expect
    as。well and just for one more example if I，had one or one what would the result
    be。well 1 times 1 is 1 1 times 1 is 1 some，of those is 2 I add the bias term to。that
    I get the number 1 1 plotted on，this graph is way over there that's well。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因为输出为 0 或 1，这就是我们所期望的结果，再举一个例子，如果我有 1 或 1，结果会是什么呢？嗯，1 乘以 1 等于 1，1 乘以 1 也是 1，加起来是
    2，我再加上偏置项，得到的数是 1，1 在这个图表上的位置非常远。
- en: beyond the threshold and so this output，is going to be 1 as well the output
    is。always 0 or 1 depending on whether or，not we're past the threshold and this。neural
    network then models the or，function a very simple function。definitely but it still
    is able to model，it correctly if I give it the inputs it。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 输出总是 0 或 1，取决于我们是否超过了阈值，因此这个神经网络建模了或函数，虽然是一个非常简单的函数，但仍能正确建模，如果我给它输入。
- en: will tell me what x1 or x2 happens to be，and you could imagine trying to do
    this。for other functions as well a function，like the and function for instance
    that。takes two inputs and calculates whether，both x and y are true so if X is
    1 and Y。is 1 then the output of X and y is 1 but，in all the other cases the output
    is 0。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 将告诉我 x1 或 x2 的值是什么，你可以想象尝试对其他函数做同样的事情，例如与函数，它接受两个输入并计算是否 x 和 y 都为真，所以如果 X 为
    1 且 Y 为 1，则 X 和 Y 的输出为 1，但在所有其他情况下，输出为 0。
- en: how could we model that inside of a，neural network as well well it turns out。we
    could do it in the same way except，instead of negative 1 as the bias we can。use
    negative 2 as the bias instead what，does that end up looking like well if I。had
    1 and 1 that should be one because，one true in true is equal to true well I。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何在神经网络中建模这一点呢？事实证明，我们可以以相同的方式进行，除了将偏置从负 1 改为负 2。那么结果看起来会是什么呢？如果我有 1 和 1，应该为
    1，因为真和真等于真。
- en: take 1 times 1 that's 1 1 times 1 is 1 I，get a total sum of 2 so far now I add。the
    bias of negative 2 and I get the，value 0 and 0 when I plotted on the。activation
    function is just past that，threshold and so the output is going to。be 1 but if
    I add any other input for，example like 1 and 0 will the weighted。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 取 1 乘以 1 是 1，1 乘以 1 是 1，到目前为止总和为 2，现在我加上偏置负 2，得到的值是 0，而 0 在激活函数上绘制时正好超过那个阈值，因此输出为
    1。但如果我添加其他输入，例如 1 和 0，权重会是什么？
- en: sum of these is 1 plus 0 it's going to，be 1 minus 2 is going to give us。negative
    1 and negative 1 is not past，that threshold and so the output is。going to be 0
    so those then are some，very simple functions that we can model。using a neural
    network that has two，inputs and one output where our goal is。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这些和是1加0，它将是1减2，结果为负1，而负1未通过阈值，因此输出将是0。因此，这些是我们可以使用具有两个输入和一个输出的神经网络来建模的一些非常简单的函数，我们的目标是。
- en: to be able to figure out what those，weights should be in order to determine。what
    the output should be and you could，imagine generalizing this to calculate。more
    complex functions as well that may，be given the humidity and the pressure。we want
    to calculate what's the，probability that it's going to rain for。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定这些权重应该是什么，以便确定输出应该是什么，你可以想象将其推广到计算更复杂的函数，比如给定湿度和压力，我们想计算下雨的概率。
- en: example or we might want to do a，russian-style problem were given some。amount
    of advertising and given what，month it is maybe we want to predict。what our expected
    sales are going to be，for that particular month so you could。imagine these inputs
    and outputs being，different as well and it turns out that。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可能想做一个俄罗斯风格的问题，给定一些广告量和当前的月份，也许我们想预测该月份的预期销售额，因此你可以想象这些输入和输出也是不同的，结果表明。
- en: in some problems we're not just going to，have two inputs and the nice thing
    about。these neural networks is that we can，compose multiple units together make
    our。networks more complex just by adding，more units into this particular neural。network
    so the network we've been，looking at has two inputs and one output。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些问题中，我们不仅会有两个输入，而这些神经网络的好处在于，我们可以将多个单元组合在一起，使我们的网络更复杂，仅通过向这个特定的神经网络添加更多单元。因此，我们正在查看的网络有两个输入和一个输出。
- en: but we could just as easily say let's go，ahead and have three inputs in there
    or。have even more inputs where we could，arbitrarily decide however many inputs。there
    are to our problem all going to be，calculating some sort of output that we。care
    about figuring out the value of how，then does the math work for figuring out。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们同样可以轻松地说，继续添加三个输入，或有更多输入，我们可以任意决定有多少个输入到我们的问题中，所有这些都将计算我们关心的某种输出，如何进行数学运算。
- en: that output well it's going to work in a，very similar way in the case of two。inputs
    we had two weights indicated by，these edges and we multiplied the。weights by the
    numbers adding this bias，term and we'll do the same thing in the。other cases as
    well if I have three，inputs you'll imagine multiplying each。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 输出方式与两个输入的情况非常相似，我们有两个权重，通过这些边表示，并将权重与数字相乘，添加这个偏差项，在其他情况下我们也会做同样的事情，如果我有三个输入，你可以想象将每个输入相乘。
- en: of these three inputs by each of these，weights if I had five inputs instead。we're
    gonna do the same thing here I'm，saying sum up from 1 to 5 X I multiplied。by weight
    I so take each of the five，input variables multiply them by their。corresponding
    weight and then add the，bias to that so this would be a case。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对这三个输入的每个权重进行运算，如果我有五个输入，我们会在这里做同样的事情，我说从1到5求和，将每个输入变量乘以其对应的权重，然后将偏差添加到结果中，这就是一个情况。
- en: where there are five inputs into this，internal network for example but there。could
    be more arbitrarily many nodes，that we want inside of this neural。network where
    each time we're just going，to sum up all of those input variables。multiplied by
    their weight and then add，the bias term at the very end and so。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这个内部网络有五个输入，但我们可以在这个神经网络中有任意多个节点，每次我们只需将所有输入变量乘以它们的权重求和，然后在最后添加偏差项。
- en: this allows us to be able to represent，problems that have even more inputs just。by
    growing the size of our neural，network now the next question we might。ask is a
    question about how it is that，we train these neural networks in the。case of the
    or function in the an，function they were simple enough。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够表示具有更多输入的问题，只需通过增大神经网络的规模。接下来我们可能会问一个关于如何训练这些神经网络的问题，在“与”函数和“或”函数的情况下，它们都简单得多。
- en: functions that I could just tell you，like here what the weights should be and，yourself。what
    the weights should be in order to，calculate the output that you want but。in general
    with functions like，predicting sales or predicting whether。or not it's going to
    rain these are much，trickier functions to be able to figure。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以告诉你一些函数，比如这里的权重应该是什么，而你自己。权重应该如何设置，以便计算你想要的输出，但一般来说，像预测销售或预测是否会下雨这样的函数，都是更棘手的函数。
- en: out we would like the computer to have，some mechanism of calculating what it
    is，that the way。should be how it is to set the weights，so that our neural network
    is able to。accurately model the function that we，care about trying to estimate
    and it。turns out that the strategy for doing，this inspired by the domain of calculus。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望计算机有某种机制来计算权重应该如何设置，以便我们的神经网络能够准确建模我们关心的函数。结果证明，进行这项工作的策略受到了微积分领域的启发。
- en: is a technique called gradient descent。![](img/7967c64b75ab3f79ac5ab1a77fcf79aa_10.png)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种称为梯度下降的技术。![](img/7967c64b75ab3f79ac5ab1a77fcf79aa_10.png)
- en: and what gradient descent is it is an，algorithm for minimizing loss when。you're
    training a neural network and。![](img/7967c64b75ab3f79ac5ab1a77fcf79aa_12.png)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种用于最小化损失的算法，尤其是在训练神经网络时。![](img/7967c64b75ab3f79ac5ab1a77fcf79aa_12.png)
- en: recall that loss refers to how bad our，hypothesis function happens to be that。we
    can define certain loss functions and，we saw some examples of loss functions。last
    time that just give us a number for，any particular hypothesis saying how。poorly
    does it model the data how many，examples does it get wrong how are they。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，损失是指我们的假设函数有多糟糕。我们可以定义某些损失函数，之前我们看到了一些损失函数的例子，它们给我们一个数字，表示任何特定假设模型数据的效果如何，有多少例子被错误分类，它们的表现如何。
- en: worse or less bad as compared to other，hypothesis functions that we might。define
    and this loss function is just a，mathematical function and when you have。a mathematical
    function in calculus what，you could do is calculate something。known as the gradient
    which you can，think of it's like a slope it's the。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们可能定义的其他假设函数相比，它更差或稍微好一点，这个损失函数只是一个数学函数。当你在微积分中有一个数学函数时，你可以计算一些被称为梯度的东西，你可以把它想象成斜率。
- en: direction the loss function is moving at，any particular point and what it's
    going。to tell us is in which direction should，we be moving these weights in order
    to。minimize the amount of loss and so，generally speaking we won't get into the。calculus
    event but the high-level idea，for gradient descent is going to look。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何特定点，损失函数的方向在移动，告诉我们应该如何调整这些权重，以最小化损失量。一般来说，我们不会深入讨论微积分，但梯度下降的高层次思想看起来是这样的。
- en: something like this if we want to train，a neural network we'll go ahead and。start
    just by choosing the weights，randomly just pick random weights for。all of the
    weights in the neural network，and then we'll use the input data that。we have access
    to in order to train the，network in order to figure out what the。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要训练一个神经网络，我们将先随机选择权重，为神经网络中的所有权重随机选取，然后我们将使用我们可以访问的输入数据来训练网络，以便找出什么。
- en: weights should actually be so we'll，repeat this process again and again the。first
    step is we're going to calculate，the gradient based on all of the data。points
    so we'll look at all the data and，figure out what the gradient is at the。place
    where we currently are for the，current setting of the weights which。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 权重实际应该是什么，因此我们将一遍又一遍地重复这个过程。第一步是根据所有数据点计算梯度，我们将查看所有数据并找出当前权重设置下的梯度。
- en: means that in which direction should we，move the weights in order to minimize。the
    total amount of loss in order to，make our solution better and once we've。calculated
    that gradient which direction，we should move in the loss function well。then we
    can just update those weights，according to the gradient take a small。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们应该朝哪个方向移动权重，以最小化总的损失，使我们的解决方案更好。一旦我们计算出梯度，确定应该朝哪个方向移动损失函数，我们就可以根据梯度更新权重，采取一个小的步骤。
- en: step in the direction of those weights，in order to try to make our solution
    a。little bit better and the size of the，step that we take that's going to vary。and
    you can choose that when you're，training a particular neural network but。in short
    the idea is going to be take，all the data points figure out based on。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 向这些权重的方向迈出一步，以试图使我们的解决方案稍微更好，而我们迈出的步伐大小会有所不同。你可以在训练特定神经网络时选择这个步伐，但简而言之，这个想法是基于所有数据点进行确定。
- en: those data points in what direction the，weights should move and。and then move
    the weights one small step，in that direction and if you repeat that。process over
    and over again adjusting，the weights a little bit at a time based。on all the data
    points eventually you，should end up with a pretty good，problem。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据点中权重应该朝哪个方向移动，然后在那个方向上小步移动权重。如果你不断重复这个过程，逐渐调整权重，最终你应该能得到一个相当好的解决方案。
- en: at least that's what we would hope to，happen now if you look at this algorithm。a
    good question to ask，anytime you're analyzing an algorithm is。what is going to
    be the expensive part，of doing the calculation what's going to。take a lot of work
    to try to figure out，what is going to be expensive to。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 至少这是我们希望发生的事情，现在如果你看看这个算法，任何时候分析一个算法都要问一个好问题：进行计算时最昂贵的部分是什么？什么需要大量工作来搞清楚，什么将会很昂贵。
- en: calculate and in particular in the case，of gradient descent the really expensive。part
    is this all data points part right，here having to take all of the data。points
    and using all of those data，points figure out what the gradient is。at this particular
    setting of all of the，weights because odds are in a big。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是在梯度下降的情况下，真正昂贵的部分是所有数据点的部分。在这里，需要使用所有的数据点来确定在所有权重的特定设置下梯度的值，因为在大数据集中这种情况的概率很高。
- en: machine learning problem when you're，trying to solve a big problem with a lot。of
    data you have a lot of data points in，order to calculate and figuring out the。gradient
    based on all of those data，points is going to be expensive and。you'll have to
    do it many times you'll，likely repeat this process again and。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习问题中，当你试图解决一个有大量数据的大问题时，你有很多数据点。在计算和确定基于所有这些数据点的梯度时将是昂贵的，而且你需要多次进行这个过程。
- en: again and again going through all the，data points taking one small step over。and
    over as you try and figure out what，the optimal setting of those weights。happens
    to be it turns out that we would，ideally like to be able to train our。neural networks
    faster to be able to，more quickly converge to some sort of。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一遍又一遍地查看所有数据点，反复进行小步伐，试图找出那些权重的**最佳设置**是什么。事实证明，我们理想情况下希望能够更快地训练我们的神经网络，以便更快地收敛到某种解决方案。
- en: solution that is going to be a good，solution to the problem so in that case。there
    are alternatives to just standard，gradient descent which looks at all of。the data
    points at once，we can employ a method like stochastic，gradient descent which will
    randomly。just choose one data point at a time to，calculate the gradient based
    on instead。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是一个良好的解决方案，因此在这种情况下，有比标准**梯度下降**更好的选择，后者一次性查看所有数据点。我们可以采用像**随机梯度下降**这样的方法，它将随机选择一个数据点来计算梯度。
- en: of calculating it based on all of the，data points so the idea there is that
    we。have some setting of the weights we pick，a data point and based on that one
    data。point we figure out in which direction，should we move all of the weights
    and。move the weights in that small direction，then take another data point and
    do that。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 基于所有数据点的计算，因此这里的想法是我们有一些权重设置，我们选择一个数据点，并基于那个数据点确定应该如何调整所有权重，并在那个小方向上移动权重，然后再选择另一个数据点并这样做。
- en: again and repeat this process again and，again maybe looking at each of the data。points
    multiple times but each time only，using one data point to calculate the。gradient
    to calculate which direction we，should move in now just using one data。point instead
    of all of the data points，probably gives us a less accurate。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 再次重复这个过程，可能多次查看每个数据点，但每次只使用一个数据点来计算梯度，以确定我们应该朝哪个方向移动。现在，仅使用一个数据点而不是所有数据点，可能会使我们得出的结论不够准确。
- en: estimate of what the gradient actually，is but on the plus side it's going to
    be。much faster to be able to calculate the，can much more quickly calculate what
    the。gradient ISM based on one data point，instead of calculating based on all of。the
    data points and having to do all of，that computational work again and again。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对梯度的估计，但从好的一面来看，基于一个数据点计算梯度将会更快，而不是基于所有数据点进行计算，并且一次又一次地进行所有计算工作。
- en: so they're trade-offs here between，looking at all of the data points and。just
    looking at one data point and it，turns out that a middle ground that was。also
    quite popular is a technique called，mini-batch gradient descent where the。idea
    there is instead of looking at all，of the data versus just a single point。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看所有数据点与仅查看一个数据点之间存在权衡，结果表明，一个也相当流行的中间选择是称为小批量梯度下降的技术，其中的理念是与查看所有数据相比，仅查看一个数据点。
- en: we instead divide our data set up into，small batches groups of data points。where
    you can decide how big a，particular batch is but in short you're。just gonna look
    at a small number of，points at any given time hopefully。getting a more accurate
    estimate of the，gradient but also not requiring all of。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据集分成小批量的数据点组。你可以决定每个批次的大小，但简而言之，你只是会在任何给定时间查看少量的数据点，希望能够更准确地估计梯度，同时也不需要所有的计算。
- en: the computational effort needed to look，at every single one of these data points。so
    gradient descent then is this，technique that we can use in order to。train these
    a neural networks in order，to figure out what the setting of all of。these weights
    should be if we want some，way to try and get an accurate notion of。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 计算每一个数据点所需的计算量是巨大的。因此，梯度下降是一种我们可以用来训练神经网络的技术，以便确定所有权重的设置，如果我们想要某种方式来获得准确的概念。
- en: how it is that this function should work，some way of modeling how to transform。the
    inputs into particular outputs now，so far the networks that we've taken a。look
    at have all been structured similar，to this we have some number of inputs。maybe
    two or three or five or more and，then we have one output that is just。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数应该如何工作，以及如何建模将输入转换为特定输出的方式。目前我们查看的网络结构都类似，我们有一些输入，可能是两个、三个、五个或更多，然后我们有一个输出。
- en: predicting like rain or no rain or just，predicting one particular value but。often
    in machine learning problems we，don't just care about one output we。might care
    about an output that has，multiple different values associated。with it so in the
    same way that we could，take a neural network and add units to。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 但在机器学习问题中，我们通常不仅关心一个输出，而是可能关心一个与多个不同值相关的输出。因此，正如我们可以取一个神经网络并添加单元一样。
- en: the input layer we can likewise add，input or out outputs to the output layer。as
    well instead of just one output you，could imagine we have two outputs or we。could
    have like four outputs for example，we're in each case as we add more inputs。or
    add more outputs if we want to keep，this network fully connected between。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层，我们同样可以在输出层添加输入或输出。除了一个输出外，你可以想象我们有两个输出，或者我们可能有四个输出，例如，在每种情况下，随着输入或输出的增加，如果我们希望保持这个网络的完全连接。
- en: these two layers we just need to add，more weights that now each of these。input
    nodes has four weights associated，with each of the four outputs and that's。true
    for each of these various different，input nodes so as we add nodes we add。more
    weights in order to make sure that，each of the inputs can somehow be。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个层我们只需要添加更多的权重，现在每个输入节点都有四个权重与四个输出相关联，并且对这些不同的输入节点都是如此，因此随着节点的增加，我们添加更多的权重，以确保每个输入都能以某种方式。
- en: connected to each of the outputs so that，each output value can be calculated。based
    on what the valley，of the input happens to be I said what。made a case be where
    we want multiple，different output values well you might。consider that in the case
    of weather，predicting for example we might not just。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 与每个输出相连接，以便每个输出值可以根据输入的值进行计算。我提到过，为什么我们想要多个不同的输出值的情况，比如天气预测。例如，我们可能不仅仅是在预测雨或不雨，或者只是预测一个特定的值。
- en: care whether it's raining or not raining，there might be multiple different。categories
    of weather that we would like，to categorize the weather into with just。a single
    output variable we can do a，binary classification like rain or no。rain for instance
    1 or 0 but it doesn't，allow us to do much more than that with。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是下雨还是不下雨，可能会有多种不同的天气类别，我们希望将天气分类为单一的输出变量，我们可以进行二元分类，例如下雨或不下雨（1或0），但这并不能让我们做到更多。
- en: multiple output variables I might be，able to use each one to predict。something
    a little different maybe I，want to categorize the weather into one。of four different
    categories something，like is it going to be raining or sunny。or cloudy or snowy
    and I now have 4，output variables that can be used to。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多个输出变量，我可能能够使用每一个来预测一些略有不同的内容，可能我想将天气分类为四个不同的类别，比如说，它会是下雨、晴天、多云还是下雪，现在我有4个可以用来预测的输出变量。
- en: represent maybe the probability that it，is rainy as opposed to sunny as opposed。to
    cloudy or as opposed to snowy how，then with this neural network work where。we
    have some input variables that，represent some data that we have。collected about
    the weather each of，those inputs gets multiplied by each of。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这些输入可能代表下雨的概率，与晴天、多云或下雪相对，那么这个神经网络是如何工作的呢？我们有一些输入变量，这些变量代表我们收集到的天气数据。
- en: these various different weights we have，more multiplications to do but these
    are。fairly quick mathematical operations to，perform and then what we get is after。passing
    them through some sort of，activation function in the outputs we。end up getting
    some sort of number where，that number you might imagine you can。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不同的权重，我们还有更多的乘法运算要做，但这些都是相当快速的数学运算，然后我们得到的结果是在通过某种激活函数后，在输出中得到的某种数字，你可以想象这个数字。
- en: interpret it as like a probability like，a probability then it is one category
    as。opposed to another category so here，we're saying that based on the inputs we。think
    there is a 10% chance that it's，raining a 60% chance that it's sunny a。20% chance
    of cloudy a 10% chance that，it's snowy and given that output if。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将其解释为概率，就像一个类别的概率与另一个类别的概率相比，所以在这里，我们根据输入认为下雨的概率是10%，晴天的概率是60%，多云的概率是20%，下雪的概率是10%，如果给定这个输出。
- en: these represent like a probability，distribution well then you could just。pick
    whichever one has the highest value，in this case sunny and say that well。most
    likely we think that this，categorization the inputs means that the。output should
    be snowy and or it should，be sunny and that is what we would。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这些表示概率分布，那么你可以选择值最高的那个，在这种情况下是晴天，并且说我们最可能认为这些输入的分类意味着输出应该是下雪的，或者应该是晴天，这就是我们要的结果。
- en: expect the weather to be in this，particular instance and so this allows。us to
    do these sort of multi-class，classification we're instead of just。having a binary
    classification 1 or 0 we，can have as many different categories as。we want and
    we can have our neural，network output these probabilities over。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 期望天气在这个特定的实例中是怎样的，这使我们能够进行这种多类别分类，而不仅仅是二元分类（1或0），我们可以有任意多的类别，并且我们的神经网络可以输出这些概率。
- en: which categories are most more likely，than other categories and using that，inference。what
    it is that we should do so this was，sort of the idea of supervised machine。learning
    I can give the minister neural，network a whole bunch of data a whole。bunch of
    input data corresponding to，some label some output data like we know。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 哪些类别比其他类别更可能，并利用这种推断来决定我们应该做什么，这就是监督机器学习的想法。我可以给神经网络提供大量数据以及与某些标签对应的输入数据。
- en: that it was raining on this day we know，that was sunny on that day and using
    all。of that data the algorithm can use，gradient descent to figure out what all。the
    weights should be in order to create，some sort of model that hopefully allows。us
    a way to predict what we think the，weather is going to be but neural，networks
    have a lot of other。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道这一天是下雨的，那一天是晴天，利用所有这些数据，算法可以使用梯度下降来计算所有权重，以创建某种模型，希望能够让我们预测我们认为天气会怎样，但神经网络还有很多其他。
- en: applications as well you can imagine，applying the same sort of idea to a。reinforcement
    learning sort of example，as well where you remember that in。reinforcement learning
    what we wanted to，do is train some sort of agent to learn。what action to take
    depending on what，state they currently happen to be in so。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的应用，你可以想象将这种思想应用于强化学习的例子，记住在强化学习中我们想要做的是训练某种代理来学习根据其当前所处的状态采取什么行动。
- en: depending on the current state of the，world we wanted the agent to pick from。one
    of the available actions that is，available to them and you might model。that by
    having each of these input，variables represent some information。about the state
    some data about what，state our agent is currently in and then。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 根据世界的当前状态，我们希望代理从可用的可采取的行动中选择一个，你可能会通过让每个输入变量代表一些关于状态的信息来进行建模，关于我们代理当前所处状态的一些数据，然后。
- en: the output for example could be each of，the various different actions that our。agent
    could take action one two three，and four and you might imagine that this。network
    would work in the same way the，based on these particular inputs we go。ahead and
    calculate values for each of，these outputs and those outputs could。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 输出例如可以是我们代理可以采取的各种不同动作，动作一、二、三和四，你可能想象这个网络的工作方式是，根据这些特定的输入，我们继续计算每个输出的值，而这些输出可能。
- en: model which action is better than other，actions and we could just choose based。on
    looking at those outputs which action，should with we should take and so these。neural
    networks are very broadly，applicable that all they're really doing。is modeling
    some mathematical function，so anything that we can frame as a。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 模型评估哪个动作优于其他动作，我们可以仅仅根据查看这些输出来选择应该采取的行动，因此这些神经网络具有广泛的适用性，它们所做的实际上就是建模某种数学函数，所以任何我们能框定为一个。
- en: mathematical function something like，classifying inputs into various。different
    categories or figuring out，based on some input state what action we。should take
    these are all mathematical，functions that we could attempt to model。by taking
    advantage of this neural，network structure and in particular。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 数学函数类似于将输入分类为各种不同类别，或者基于某些输入状态来确定我们应该采取的行动，这些都是我们可以通过利用神经网络结构进行建模的数学函数，特别是。
- en: taking advantage of this technique，gradient descent that we can use in。order
    to figure out what the weights，should be in order to do this sort of。calculation
    now how is it that you would，go about training a neural network that。has multiple
    outputs instead of just one，well with just a single output we could。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这种技术，梯度下降，我们可以用来确定权重应该是什么，以便进行这种计算，那么你将如何训练一个有多个输出的神经网络，而不仅仅是一个呢？对于单个输出，我们可以。
- en: see what the output for that value，should be and then you update all the。weights
    that correspond into it，and when we have multiple outputs at。least in this particular
    case we can，really think of this as four separate。neural networks that really
    we just have，one network here that has these three。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 查看该值的输出应该是什么，然后更新与其对应的所有权重，当我们在这个特定情况下有多个输出时，我们可以将其真正视为四个独立的神经网络，实际上我们这里只有一个网络，有这三个。
- en: inputs corresponding with these three，weights corresponding to this one output。value
    and the same thing is true for，this output value this output value。effectively
    defines yet another neural，network that has these same three inputs。but a different
    set of weights that，correspond to this output and likewise。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 输入与这三个权重对应，这些权重对应于这个单一输出值，同样的事情也适用于这个输出值，这个输出值实际上定义了另一个神经网络，该网络具有这三个相同的输入，但对应于这个输出的权重集不同。
- en: this output has its own set of weights，as well and same thing for the fourth。output
    too and so if you wanted to train，a neural network that had four outputs。instead
    of just one in this case where，the inputs are directly connected to the。outputs
    you could really think of this，as just training for independent neural。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出也有自己的权重集，第四个输出也是如此，因此如果你想训练一个有四个输出的神经网络，而不是仅仅一个，在这种情况下输入直接连接到输出，你可以真的把它看作是为独立的神经。
- en: networks we know what the outputs for，each of these four should be based on。our
    input data and using that data we，can begin to figure out what all of。these individual
    weights should be and，maybe there's an additional step at the。end to make sure
    that we turn these，values into a probability distribution。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道这些四个输出应基于我们的输入数据，并利用这些数据，我们可以开始确定这些个别权重应是什么，也许在最后还有一个额外步骤，以确保我们将这些值转化为概率分布。
- en: such that we can interpret which one is，better than another or more likely than。another
    as a category or something like，that so this then seems like it does a。pretty
    good job of taking inputs and，trying to predict what outputs should be。and we'll
    see some real examples of this，in just a moment as well but it's。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这样我们可以解释哪个类别比另一个更好或更可能，因此这似乎在处理输入并试图预测输出时表现得相当不错，我们将很快看到一些实际例子，但这。
- en: important then to think about what the，limitations of this sort of approach
    is。of just taking some linear combination，of inputs and passing it into some sort。of
    activation function and it turns out，that when we do this in the case of。binary
    classification of trying to，predict like does it belong to one。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅是取某些输入的线性组合，并将其传递到某种激活函数中。结果发现，当我们在二元分类的情况下进行此操作时，试图预测某个点是否属于某个类别。
- en: category or another we can only predict，things that are linearly separable。because
    we're taking a linear，combination of inputs and using that to。define some decision
    boundary or，threshold then what we get is a。situation where if we have this set
    of，data we can predict like a line that。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 是某个类别或另一个类别，我们只能预测线性可分的事物，因为我们正在取输入的线性组合并用它来定义某个决策边界或阈值，因此我们得到的情况是，如果我们有这组数据，我们可以预测像一条线那样。
- en: separates linearly the red points from，the blue points but a single unit that。is
    making a binary classification，otherwise known as a perceptron can't。deal with
    a situation like this where，we've seen this type of situation before。where there
    is no straight line that，just goes straight through the data that。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 它线性分隔红点与蓝点，但一个进行二元分类的单元，即感知器，无法处理这种情况，我们曾见过这种情况，其中没有一条直线可以直接穿过数据。
- en: will divide the red points away from the，blue points it's a more complex decision。boundary
    the decision boundary somehow，needs to capture the things inside of。Circle and
    there isn't really a line，that will allow us to deal with that so。this is the
    limitation of the perceptron，these units that just make these binary。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 将红点与蓝点分开，这是一个更复杂的决策边界，决策边界必须捕捉圆圈内部的事物，而实际上并没有一条线能让我们处理这个问题。因此，这就是感知器的局限性，这些单元仅根据输入做出二元决策，单个感知器只能学习线性可分的决策边界，它所能做的只是定义一条线。
- en: decisions based on their inputs that a，single perceptron is only capable of。learning
    a linearly separable decision，boundary all it can do is define a line。and sure
    it can give us probabilities，based on how close to that decision。boundary we are
    but it can only really，decide based on a linear decision。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，它可以根据我们距离该决策边界的远近给出概率，但它只能真正基于线性决策做出判断。
- en: boundary and so this doesn't seem like，it's going to generalize well to。situations
    where real-world data is，involved because real-world data often。isn't linearly
    separable it often isn't，the case that we can just draw a line。through the data
    and be able to divide，it up into multiple groups so what then。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 边界，因此这似乎不太可能推广到涉及真实世界数据的情况，因为真实世界数据通常不是线性可分的，往往不能仅仅通过一条线将数据划分成多个组。那么重要的是要考虑这种方法的局限性。
- en: is the solution to this well what was，proposed with the idea of a multi-layer。neural
    network that's so far all the，neural networks we've seen have had a。set of inputs
    and a set of outputs and，the inputs are connected to those。outputs but in a multi-layer
    neural，network this is going to be an。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的解决方案是提出了多层神经网络的概念，到目前为止，我们看到的所有神经网络都有一组输入和一组输出，而输入与这些输出连接，但在多层神经网络中，这将是一个。
- en: artificial neural network that has an，input layer still it has an output layer。but
    also has one or more hidden layers，in between other layers of artificial。neurons
    or units that are going to，calculate their own values as well so。instead of a
    neural network that looks，like this with three inputs and one。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络，仍然有一个输入层，还有一个输出层，但中间还有一个或多个隐藏层，其他层的人工神经元或单元也将计算它们自己的值。因此，神经网络看起来并不是像这样有三个输入和一个。
- en: output you might imagine in the middle，here injecting a hidden layer something。like
    this this is a hidden layer that，has four nodes you could choose how many。nodes
    or units end up going into the，hidden layer you can have multiple。hidden layers
    as well and so now each of，these inputs isn't directly connected to。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 输出时，你可能会想象在中间，注入一个隐藏层，像这样，这是一个有四个节点的隐藏层。你可以选择进入隐藏层的节点或单元的数量，你也可以有多个。隐藏层，因此现在每个输入并不是直接连接到。
- en: the output each of the inputs is，connected to this hidden layer and then。all
    of the nodes in the hidden layer，those are connected to the one output。and so
    this is just another step that we，can take towards calculating more。complex functions
    each of these hidden，units will calculate its output value。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的，每个输入都连接到这个隐藏层，然后隐藏层中的所有节点，都连接到一个输出。这只是我们可以迈向计算更复杂函数的又一步，这些隐藏单元将计算它的输出值。
- en: otherwise known as its activation based，on a linear combination of all the。inputs
    and once we have values for all，these nodes as opposed to this just。being the
    output we do the same thing，again calculate the output for this node。based on
    multiplying each of the values，for these units by their weights as well。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 也被称为其激活值，基于所有输入的线性组合。一旦我们对所有这些节点有了值，而不是这仅仅是输出，我们再次做同样的事情，根据将这些单元的值与它们的权重相乘来计算这个节点的输出。
- en: so in effect the way this works is we，start with inputs they get multiplied
    by，weights。to calculate values for the hidden nodes，those get multiplied by weights
    in order。to figure out what the ultimate output，is going to be and the advantage
    of。layering things like this is it gives us，an ability to model more complex。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，实际上它的工作方式是我们从输入开始，这些输入乘以权重。计算隐藏节点的值，这些值再次乘以权重，以确定最终输出将是什么，像这样分层的优点是它让我们能够建模更复杂的。
- en: functions but instead of just having a，single decision boundary a single line。dividing
    the red points from the blue，points each of these hidden nodes can。learn a different
    decision boundary and，we can combine those decision boundaries。to figure out what
    the ultimate output，is going to be and as we begin to。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 函数，但不是仅仅有一个单一的决策边界，一条线。将红点与蓝点分开，每个隐藏节点都可以。学习一个不同的决策边界，我们可以将这些决策边界结合起来。来确定最终的输出将会是什么，随着我们开始。
- en: imagine more complex situations you，could imagine each of these nodes。learning
    some useful property or，learning some useful feature of all of。the inputs and
    us somehow learning how，to combine those features together in。order to get the
    output that we actually，want now the natural question will when。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 想象更复杂的情况，你可以想象这些节点学习某种有用的属性或，学习所有输入的某种有用特征，而我们以某种方式学习如何将这些特征结合在一起，以获得我们实际上想要的输出，现在自然会问。
- en: you begin to look at this now is to ask，the question of how do we train a neural。network
    that has hidden layers inside of，it and this turns out to initially be a。bit of
    a tricky question because the，input data we are given is we are given。values for
    all of the inputs and we're，given what the value of the output。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你开始查看这个问题，现在是要问，如何训练一个内部有隐藏层的神经网络。这最初是一个有点棘手的问题，因为我们给定的输入数据是，我们得到了所有输入的值，以及输出的值。
- en: should be what the category is for，example but the input data doesn't tell。us
    what the values for all of these，nodes should be so we don't know how far。off
    each of these nodes actually is，because we're only given data for the。inputs and
    the outputs the reason this，is called a hidden layer is because the。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，输入数据并没有告诉我们这些节点的值应该是什么，因此我们不知道这些节点实际上离目标有多远，因为我们仅仅得到了输入和输出的数据。这就是为什么这被称为隐藏层。
- en: data that is made available to us，doesn't tell us what the values for all。of
    these intermediate nodes should。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 提供给我们的数据，并没有告诉我们所有这些中间节点的值应该是什么。
