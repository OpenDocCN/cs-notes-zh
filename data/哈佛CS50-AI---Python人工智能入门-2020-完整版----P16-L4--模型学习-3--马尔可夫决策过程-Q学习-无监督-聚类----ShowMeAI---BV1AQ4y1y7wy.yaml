- en: 哈佛CS50-AI ｜ Python人工智能入门(2020·完整版) - P16：L4- 模型学习 3 (马尔可夫决策过程，Q学习，无监督，聚类) -
    ShowMeAI - BV1AQ4y1y7wy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 哈佛CS50-AI ｜ Python人工智能入门(2020·完整版) - P16：L4- 模型学习 3 (马尔可夫决策过程，Q学习，无监督，聚类) -
    ShowMeAI - BV1AQ4y1y7wy
- en: learn what to do in the future and what，not to do so in order to begin to。formalize
    this the first thing we need，to do is formalize this notion of what。we mean about
    states and actions and，like，and oftentimes we'll formulate this。world and as what's
    known as a Markov，decision process similar in spirit to。
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 学习未来该做什么和不该做什么，因此为了开始正式化这一点，我们首先需要正式化我们所说的状态和行动的概念，像往常一样，我们将这种世界形式化为一个被称为马尔可夫决策过程的东西，与。
- en: Markov chains which you might recall，from before but a Markov decision。process
    is a model that we can use for，decision making for an agent trying to。make decisions
    in its environment and，it's a model that allows us to represent。the various different
    states that an，agent can be in the various different。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能记得之前的马尔可夫链，但马尔可夫决策过程是一个我们可以用来为代理在其环境中做决策的模型，这是一个允许我们表示代理可以处于的各种不同状态的模型。
- en: actions that they can take and also what，the reward is，for taking one action
    as opposed to。another action so what then does it，actually look like well if you
    recall a，Markov chain from。for a Markov chain looked a little，something like this
    where we had a whole。bunch of these individual states and，each state immediately
    transitioned to。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 他们可以采取的行动以及采取一种行动与采取另一种行动的奖励是什么，那么这实际上是什么样子呢？如果你还记得马尔可夫链，从之前的马尔可夫链看起来有点像这样，我们有一大堆这些个体状态，每个状态立即过渡到。
- en: another state based on some probability，distribution we saw this in the context。of
    the weather before where if it was，sunny we said with some probability。you'd only
    be sunny the next day with，some other probability you'll be rainy。for example
    but we could also imagine，rain anymore，we just had these states where one state。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个基于某些概率的状态，分布我们在之前天气的上下文中看到过。如果天气是晴朗的，我们说在某种概率下，第二天也只有在某种其他概率下才会是雨天。例如，我们也可以想象不再下雨，我们只有这些状态，其中一个状态。
- en: leads to another state according to some，probability distribution but in this。original
    model there was no agent that，had any control over this process it was。just entirely
    probability based where，with some probability we move to this。next state but maybe
    it's going to be，some other state with some other。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 根据某些概率分布会导致另一个状态，但在这个原始模型中，没有代理可以控制这个过程，完全是基于概率的，在某种概率下我们移动到这个下一个状态，但也许它会是某种其他状态。
- en: probability what will now have is the，ability for the agent in this state to。choose
    from a set of actions where maybe，instead of just one path forward they。have three
    different choices of actions，that each lead up down different paths。and even this
    is a bit of an，oversimplification because in each of。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 概率现在我们将拥有的是代理在这个状态下选择一组行动的能力，可能与仅有一条前进路径不同，他们有三种不同的行动选择，每种选择都通向不同的路径。即便这也有些过于简化，因为在每个状态下，
- en: these states you might imagine more，branching points where there are more。decisions
    that can be taken as well so，we've extended the Markov chain to say。that from
    a state you now have available，action choices and each of those actions。might
    be associated with its own，probability distribution of going to。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些状态中，你可以想象更多的分支点，那里有更多的决策可以被采取，因此我们扩展了马尔可夫链，表示从一个状态你现在有可用的行动选择，而每个这些行动可能与其自身的概率分布相关联，去。
- en: various different states then in，addition we'll add another extension。where
    anytime you move from a state，taking an action going into this other。state we
    can associate a reward with，that outcome saying either R is positive。meaning some
    positive reward or R as，negative meaning there was some sort of。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 各种不同的状态，然后我们还会添加另一个扩展，每当你从一个状态采取行动进入另一个状态时，我们可以将奖励与这个结果相关联，表明R是正值，意味着某种正奖励，或者R是负值，意味着有某种形式的。
- en: punishment and this then is what we'll，consider to be a Markov decision process。that
    a Markov decision process has some，initial set of states of states in the。world
    that we can be in we have some set，of actions that given a state I can say。what
    are the actions that are available，to me in that state and action that I。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 惩罚，这就是我们将考虑的马尔可夫决策过程。马尔可夫决策过程有一些初始状态的集合，我们可以处于这些状态中，我们有一些行动的集合，给定一个状态，我可以说在那个状态下有哪些可用的行动。
- en: can choose from then we have some，transition model the transition model。before
    just said that given my current，state what is the probability that I end。up in
    that next state or this other，state the transition model now has，effectively two
    things we're。conditioning on we're saying given that，I'm in this state and that
    I take this。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们有一些转移模型。转移模型之前提到，给定我当前的状态，我到达下一个状态或另一个状态的概率是什么？转移模型现在有效地有两个条件：我们在说，给定我处于这个状态，并且我采取这个行动，我到达下一个状态的概率是什么。
- en: action what's the probability that I end，up in this next state。now maybe we
    live in a very determined，mystic world in this Markov decision。process we're given
    a state and given an，action we know for sure what next state。will end up in but
    maybe there's some，randomness in the world that when you。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们生活在一个非常确定的神秘世界中，在这个马尔可夫决策过程中，给定一个状态和一个行动，我们可以确定会到达哪个下一个状态，但也许世界中存在一些随机性，当你采取行动并意识到受到惩罚和经历时。
- en: take in a state and you take an action，you might not always end up in the exact。same
    state there might be some，probabilities involved there as well the。Markov decision
    process can handle both，of those possible cases and then finally。we have a reward
    function generally，called R that in this case says what is。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当你处于一个状态并采取行动时，你可能不会总是回到完全相同的状态，可能还涉及一些概率。马尔可夫决策过程可以处理这两种可能情况，最后我们有一个通常称为R的奖励函数，它在这种情况下说明了什么。
- en: the reward for being in this state，taking this action and then getting to s。prime
    this next state so I'm in this，original state I take this action I get。to this
    next state what is the reward，for doing that process and you can add。up these
    rewards every time you take an，action to get the total amount of。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个状态中，采取这个行动然后到达s'这个下一个状态的奖励，因此我在这个原始状态中，采取这个行动，我到达这个下一个状态，这个过程的奖励是什么？你可以在每次采取行动时将这些奖励加起来，得到总奖励。
- en: rewards that an agent might get from，interacting in a particular environment。modeled
    using this Markov decision，process so what might this actually look，like in practice。well
    let's just create a little，simulated world here where I have this。agent that is
    just trying to navigate，its way this agent is this yellow dot。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 代理可能从与特定环境的交互中获得的奖励，可以使用这个马尔可夫决策过程进行建模。那么这在实践中可能是什么样子呢？好吧，让我们在这里创建一个小模拟世界，我有这个试图导航的代理，这个代理是这个黄色点。
- en: here like a robot in the world trying to，navigate its way through this grid
    and。ultimately it's trying to find its way，to the goal and if it gets to the green。goal
    then it's going to get some sort of，reward but then we might also have some。you
    know red squares that are places，where you get some sort of punishment。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这里像一个在世界中试图穿越这个网格的机器人，最终它试图找到通往目标的路，如果它到达绿色目标，它就会得到某种奖励，但我们也可能会有一些红色方块，那是会受到惩罚的地方。
- en: some bad place where we don't want the，Square，then our agent is going to get
    some sort。of punishment as a result of that but，the agent originally doesn't know
    all of。these details it doesn't know that these，states are associated with punishments。but
    maybe it does know that this state，is associated with a reward maybe it。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个我们不想要的地方，如果我们的代理到达了方块，它将会受到某种惩罚，但代理最初并不知道所有这些细节，它不知道这些状态与惩罚相关。但是也许它知道这个状态与奖励相关。
- en: doesn't but it just needs to sort of，interact with the environment to try and。figure
    out what to do and what not to do，so the first thing the agent might do is。given
    no additional information if it，doesn't know what the punishments are it。doesn't
    know where the rewards are it，just might try and take an action and it。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着它必须与环境进行交互，以尝试弄清楚该做什么和不该做什么，因此，代理可能首先会在没有额外信息的情况下采取行动。如果它不知道惩罚是什么，也不知道奖励在哪里，它可能只是尝试采取某种行动。
- en: takes an action and ends up realizing，that it got some sort of punishment and，experience。well
    it might learn that when you're in，this state in the future don't take the。action
    move to the right that that is a，bad action to take that in the future if。you
    ever find yourself back in the state，don't take this action of going to the。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 那么它可能会学到，当你在将来处于这个状态时，不要采取这个向右移动的行动，这是一个错误的行动。在将来，如果你再次回到这个状态，就不要采取去奖励的行动。
- en: right when you're in this particular，state because that leads to punishment。that
    might be the intuition at least and，so you could try，doing other actions you move
    up alright。that didn't lead to any immediate，rewards and maybe try something else。then
    maybe try something else and，alright now you found that you got。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当你处于这个特定状态时，因为那会导致惩罚。这可能是直觉，至少可以这么说，因此你可以尝试进行其他行动，你向上移动，好吧，那没有导致任何立即的奖励，也许可以尝试其他事情。然后也许再尝试其他事情，好的，现在你发现你得到了。
- en: another punishment and so you learn，something from that experience so the。next
    time you do this whole process you，know that if you ever end up in this。square
    you shouldn't take the down，action because being in this state and。taking that
    action ultimately leads to，some sort of punishment a negative。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种惩罚，因此你从那次经历中学到了一些东西，所以下次你进行这个完整的过程时，你知道如果你最终处于这个状态，你不应该采取这种行动，因为处于这个状态并采取那个行动最终会导致某种惩罚，负面的结果。
- en: reward in other words and this process，repeats that you might imagine just。letting
    our agent explore the world，learning over time what states tend to。correspond
    with poor actions and，learning over time what states，correspond with poor actions
    until。eventually if it tries enough things，randomly it might find that eventually。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，这个过程会重复，你可以想象让我们的代理探索世界，随着时间的推移学习哪些状态往往对应于糟糕的行动，并随着时间的推移学习哪些状态对应于糟糕的行动，直到最终如果它随机尝试足够多的事情，它可能最终会发现。
- en: when you get to this state if you take，the up action in this state it might。find
    that you actually get a reward from，that and what it can learn from that is。that
    if you're in this state you should，take the up action because that leads to。a
    reward and over time you can also，learn that if you're in this state you。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当你处于这个状态时，如果你采取向上的行动，可能会发现你实际上从中得到了奖励，而从中可以学到的是，如果你在这个状态下，你应该采取向上的行动，因为那会导致奖励，随着时间的推移，你还可以学到如果你在这个状态下。
- en: should take the left action because that，leads to this state that also lets
    you。eventually get to the reward so you，begin to learn over time not only which。actions
    are good in particular States，but also which actions are bad such that。once you
    know with some sequence of good，actions that leads you to some sort of。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 应该采取左侧的行动，因为那会导致这个状态，这也让你最终得到奖励，因此你随着时间的推移开始学习，不仅哪些行动在特定状态下是好的，还学习哪些行动是不好的，这样一来，一旦你知道一系列好的行动会引导你走向某种结果。
- en: reward our agent can just follow those，instructions follow the experience that。it
    has learned we didn't tell the agent，what the goal was we didn't tell the。agent
    where the punishments were but the，agent can begin to learn from this。experience
    and learn to begin to perform，these sorts of tasks better in the。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代理可以遵循这些指令，遵循它所学到的经验，我们没有告诉代理目标是什么，也没有告诉代理惩罚在哪里，但代理可以开始从这个经验中学习，并开始更好地执行这些任务。
- en: future and so let's now try to formalize，this idea formalize the idea that we。would
    like to be able to learn in this，state taking this action is that a good。thing
    or a bad thing there are lots of，different models from reinforcement。learning
    we're just going to look at one，of them today and the one that we're。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 未来，所以现在让我们尝试形式化这个想法，形式化这个我们想要能够在这个状态下学习的想法，采取这种行动是好事还是坏事。强化学习有很多不同的模型，今天我们只看其中一个模型，而我们要看的那个。
- en: gonna look at is a method known as cue，learning and what cue learning is all。about
    it's about learning a function a，function Q that takes inputs s and a。where s
    is a state and a is an action，that you take in that state and what Q。is this q
    function is going to do is it，is going to estimate the value how much。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要看的方法称为 Q 学习，Q 学习的核心在于学习一个函数 Q，这个函数以状态 s 和行动 a 为输入，其中 s 是状态，a 是你在该状态下采取的行动，而这个
    Q 函数将做的就是估算出奖励的值。
- en: reward will I get from taking this，action in this state originally we don't。know
    what this q function should be，but over time based on experience based。on trying
    things out and seeing what the，result is I would like to try and learn。what q
    of Si is for any particular state，and any particular action that I might。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这个状态下采取这种行动能得到什么奖励，起初我们不知道这个 Q 函数应该是什么，但随着时间的推移，基于经验，基于尝试和观察结果，我想尝试学习任意特定状态
    Si 和我可能采取的任意特定行动的 Q 值。
- en: take in that state so what is the，approach well the approach originally is。we'll
    start with Q si equal to zero for，all states s and for all actions a that。initially
    before I've ever started，anything before I've had any experiences。I don't know
    the value of taking any，action in any given state so I'm going。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 那么方法是什么呢？最初的方法是，我们将对所有状态s和所有动作a的Q的si设定为零。在我开始任何事情之前，或者在我拥有任何经验之前，我不知道在任何给定状态下采取任何行动的价值，所以我将假设所有价值都是零。
- en: to assume that the value is just zero，all across the board but then as I。interact
    with the world as I experience，rewards or punishments or maybe I go to。a cell
    where I don't get either a reward，or a punishment I want to somehow update。my
    estimate of Q si I want to，continually update my estimate of Q si。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 但是当我与世界互动时，当我经历奖励或惩罚，或者我去一个既没有奖励也没有惩罚的状态时，我希望以某种方式更新我对Q的si的估计。我希望不断更新我对Q的si的估计。
- en: based on the experiences and rewards and，punishments that I've received such
    that。in the future my knowledge of what，actions are good and what states will
    be。better so when we take an action and，receive some sort of reward I want to。estimate
    the new value of Q si and I，estimate that based on a couple of，different things。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我所获得的经验、奖励和惩罚，使我在未来对什么动作是好的、什么状态会更好有了了解。当我们采取一个动作并获得某种奖励时，我想估计Q的si的新值，而这个估计是基于几个不同的因素。
- en: I estimate it based on the reward that，I'm getting from taking this action and。getting
    into the next state but assuming，like the situation isn't over assuming。there
    are still future actions that I，might take as well I also need to take。into account
    the expected future rewards，that if you imagine an agent interacting。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我根据从这个动作中获得的奖励以及进入下一个状态时的奖励来进行估计，但假设情况并没有结束，假设我可能还会采取未来的其他动作，我还需要考虑预期的未来奖励。
- en: with the environment and sometimes，you'll take an action and get a reward。but
    then you can keep taking more，actions and get more rewards that these。both are
    relevant both the current，reward I'm getting from this current。step and also my
    future reward and it，might be the case that I'll want to take。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与环境互动，有时你会采取行动并获得奖励。但随后你可以继续采取更多行动，并获得比这些更多的奖励。这两者都与当前我从这一步获得的奖励相关，也与我未来可能想采取的步骤相关。
- en: a step that doesn't immediately lead to，a reward because later on down the line。I
    know it will lead to more rewards as，well so there's a balancing act between。current
    rewards that the agent，experiences and future rewards that the。agent experiences
    as well and then we，need to update q si so we estimate the。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会有这样的情况，我会想采取一个不立即导致奖励的步骤，因为我知道在未来这将导致更多的奖励。因此，代理体验的当前奖励和未来奖励之间需要找到一个平衡。
- en: value of Q si based on the current，reward and the expected future rewards。and
    then we need to update this q，function to take into account this new。estimate
    now we already as we go through，this process will already have an。estimate for
    what we think the value is，now we have a new estimate and then。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Q的si的值基于当前的奖励和预期的未来奖励。然后我们需要更新这个q函数，以考虑这个新的估计。现在在我们经过这个过程时，我们已经有了一个我们认为的价值的估计，现在我们有了一个新的估计。
- en: somehow we need to combine these two，estimates together and we'll look at。more
    formal ways that we can actually，begin to do that so to actually show you。what
    this formula looks like here is the，approach will take with q-learning。we're going
    to again start with Q of SN，a being equal to 0 for all states and。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将这两种估计结合起来，我们将查看更正式的方法来实际开始这样做。为了向你展示这个公式的样子，这里是我们将采用的q-learning方法。我们将再次从Q的SN开始，a在所有状态下都等于0。
- en: then every time we take an action a in，state s and observe a reward R we're。going
    to update our value our estimate，for Q of Si and the idea is that we're。going
    to figure out what the new value，estimate is minus what our existing。value estimate
    is so we have some，preconceived notion for what the value。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后每次我们在状态s下采取一个动作a并观察到奖励R时，我们将更新我们的价值估计，Q的Si。这个想法是，我们将弄清楚新的价值估计减去我们现有的价值估计。
- en: is for taking this action in this state，maybe our expectation is we currently。think
    the value is 10 but then we're，going to estimate what we now think it's。going
    to be maybe the new value estimate，is something like 20 so there's a delta。of
    like 10 that our new value estimate，is 10 points higher than what our。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个状态下采取这个动作的期望也许是我们当前认为的价值是 10，但我们现在将估计我们认为的价值，也许新的价值估计是像 20 这样的东西，因此我们的新价值估计比我们。
- en: current value estimate happens to be，here，we need to decide how much we want
    to。adjust our current expectation of what，the value is of taking this action in。this
    particular state and what that a，difference is how much we add or。subtract from
    our existing notion of how，much do we expect the value to be is。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的值估计恰好在这里，我们需要决定我们想要调整我们当前的期望，这个动作在这个特定状态下的价值是什么，这个差异是我们增加或减少多少我们对预期价值的看法。
- en: dependent on this parameter alpha also，called the learning rate and alpha。represents
    in effect how much we value，new information compared to how much we。value old
    information an alpha value of，1 means we really value new information。but if we
    have a new estimate then it，doesn't matter what our old estimate is。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖于这个参数 alpha，也叫学习率，alpha 实际上表示我们对新信息的重视程度与我们对旧信息的重视程度之间的关系。alpha 值为 1 意味着我们真的重视新信息，但如果我们有一个新的估计，那么旧的估计就无关紧要。
- en: we're only going to consider our new，estimate because we always just want to。can't
    Kea take into consideration our，new information so the way that works is。that
    if you imagine alpha being 1 well，then we're taking the old value of Q si。and
    then adding one times the new value，minus the old value and that just leaves。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只考虑我们的新估计，因为我们总是只想这样做。不能考虑我们的新信息，所以这工作的方式是。如果你想象 alpha 为 1，那么我们就取 Q 的旧值，然后加上
    1 乘以新值减去旧值，这样就只剩下。
- en: us with the new value so when alpha is 1，all we take into consideration is what。our
    new estimate happens to be but over，time as we go through a lot of。experiences
    we already have some，existing information we might have tried。taking this action
    nine times already，and now we just tried it a tenth time，and we don't only want。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 所以当 alpha 为 1 时，我们只考虑我们的新估计是什么，但随着时间的推移，随着我们经历很多经历，我们已经有一些现有信息，我们可能已经尝试过这个动作九次，而现在我们只是在第十次尝试，而且我们不想。
- en: consider this tent experience I also，want to consider the fact that my prior。nine
    experiences those were meaningful，to and that's data I don't necessarily。want
    to lose them and so this alpha，controls that decision controls how。important is
    the new information zero，would mean ignore all the new。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个帐篷体验，我还想考虑我之前九次体验的事实，这对我来说是有意义的，而这些数据我不一定想丢失。因此，alpha 控制这个决策，控制新信息的重要性，零则意味着忽略所有新信息。
- en: information just keep this Q value the，same one it means replace the old。information
    entirely with the new，information and somewhere in between。keep some sort of balance
    between these，two values we can put this equation a。little bit more formally as
    well the old，value estimate is our old estimate for。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 信息只是保持这个 Q 值不变，这意味着完全用新信息替换旧信息，而在这两者之间保持某种平衡。我们可以更正式地将这个方程表达为，旧值估计是我们对。
- en: what the value is of taking this action，in a particular state that's just Q
    of。sna so we have it once here and we're，gonna add something to it we're gonna。add
    alpha times the new value estimate，minus the old value estimate but the old。value
    estimate we just look up by，calling this Q function and what then is。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在特定状态下采取这个动作的价值，这就是 Q 的意思。所以我们在这里有一次，我们要增加一些东西，我们将添加 alpha 乘以新价值估计减去旧价值估计，但旧价值估计我们只需通过调用这个
    Q 函数来查找。
- en: the new value estimate based on this，experience we have just taken what is。our
    new estimate for the value of taking，this action in this particular state。well
    it's going to be composed of two，parts it's going to be composed of what。reward
    did I just get from taking this，action in this state and then it's going。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们刚刚采取的这个经验的新价值估计是什么，在这个特定状态下采取这个动作的价值的新估计将由两个部分组成。它将由我在这个状态下采取这个动作获得的奖励组成，然后它将。
- en: to be what can I expect my future，rewards to be from this point forward so。it's
    going to be our sum reward I'm，getting right now plus whatever is。tomato I'm going
    to get in the future，and how do I estimate what I'm going to。get in the future
    well it's a bit of a，another call to this Q function it's。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以从这一点开始期望未来的奖励是什么。因此，这将是我现在获得的总奖励，加上未来我将获得的*奖励*，我怎么估计将来会获得什么呢？这有点像对这个Q函数的另一次调用。
- en: going to be take the maximum across all，possible actions I could take next and。say
    all right of all of these possible，actions I could take which one is going。to
    have the highest reward so this then，looks a little bit complicated this is。going
    to be our notion for how we're，going to perform this kind of update。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我将从所有可能的下一步行动中选取最大值，看看在所有这些可能的行动中，哪一个会有最高的奖励。因此这看起来有点复杂，这将是我们进行这种更新的概念。
- en: then I have some estimate some old，estimate for what the value is of taking。this
    action in the state and I'm going，to update it based on new information。that I
    experienced some reward I predict，what my future reward is going to be and。using
    that I update what what is tomato，reward will be for taking this action in。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我有一些旧的估计，估计采取这一行动在该状态下的价值。我将根据我经历的新信息更新它。我预测我将获得的未来奖励是什么，并且利用这一点更新我对采取此行动的*奖励*的估计。
- en: this particular state and there are，other additions you might make to this。algorithm
    as well sometimes it might not，be the case that future，you want to wait equally
    to current。rewards maybe you want an agent that，values like reward now over a
    reward。later and so sometimes you can even add，another term in here some other。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定状态下，您可能还会对这个算法进行其他补充，有时候可能并不是所有的未来奖励都与当前奖励同等重要，也许你想要一个代理，重视现在的奖励而不是稍后的奖励。因此，有时您甚至可以在这里添加另一个项。
- en: parameter where you discount future，rewards and say future rewards are not。as
    valuable as rewards immediately the，getting reward in the current time step。is
    better than waiting a year and，getting rewards later but that's。something up to
    the programmer to decide，what that parameter ought to be but the。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一个参数是折现未来的奖励，认为未来的奖励没有当前奖励那么有价值。在当前时间步获得奖励比等待一年后再获得奖励要好，但这要由程序员决定这个参数应该是什么。
- en: big picture idea of this entire formula，is to say that every time we experience。some
    new reward we take that into，account we update our estimate of how。good is this
    action and then in the，future we can make decisions based on。that algorithm once
    we have some good，estimate for every state and for every。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式的总体思路是说，每当我们经历一些新的奖励时，我们会考虑这一点，更新我们对这一行动有多好的估计。然后在未来，我们可以基于该算法做出决策，一旦我们对每个状态和每个行动都有了较好的估计。
- en: action what the value is of taking that，action then we can do something like。implement
    a greedy decision-making，policy that if I am in a state and I。want to know what
    action should I take，in that state well then I consider for。all of my possible
    actions what is the，value of Q si what is my estimated value。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们知道采取这一行动的价值，那么我们可以做一些事情，比如实施一个贪婪决策政策。如果我处于某个状态，想知道在这个状态下应该采取什么行动，那么我会考虑所有可能的行动，看看Q(s,
    a)的价值是什么。
- en: of taking that action in that state and，I will just pick the action that has
    the。highest value after I evaluate that，expression so I picked the action that。has
    the highest value and based on that，that tells me what action I should take。at
    any given state that I'm in i can，just greedily say across all my actions。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在该状态下采取这一行动，我将只选择在评估那个表达式后具有最高值的行动。所以我选择了具有最高值的行动，这告诉我在我处于的任何给定状态下，我应该采取什么行动，我可以贪婪地说在我所有的行动中。
- en: this action gives me the highest，expected value and so I'll go ahead and。choose
    that action as the action that I，take as well but there is a downside to。this
    kind of approach and the downside，comes up in a situation like this where。we know
    that you know there is some，solution that gets me to the reward and，out。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个行动给我带来最高的期望值，因此我会选择这个行动作为我采取的行动。但这种方法有一个缺点，缺点出现在这样的情况下：我们知道存在某种解决方案可以带我到达奖励。
- en: but it might not necessarily be the best，way or the fastest way if the agent
    is。allowed to explore a little bit more it，might find that it can get the reward。faster
    by taking some other route，instead by going through this particular。path that
    is a faster way to get to that，ultimate goal and maybe we would like。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 但这不一定是最佳方式或最快方式，如果代理允许多探索一点，可能会发现通过某条其他路线获得奖励的速度更快，而不是通过这条特定的路径，这是更快到达最终目标的方式，或许我们想要。
- en: for the agent to be able to figure that，out as well but if the agent always。takes
    the actions that it knows to be，best well when it gets to this。particular square
    it doesn't know that，this is a good action because。never really tried it but it
    knows that，going down eventually leads its way to。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 代理也能找出这一点，但如果代理总是采取它知道的最佳动作，当它到达这个特定方格时，它并不知道这是一个好动作，因为它从未真正尝试过，但它知道向下走最终会引导它。
- en: this reward so what might learn in the，future that it should just always take。this
    route and it's never going to，explore and go along that route instead。so when
    reinforcement learning there's，this tension between exploration and。exploitation
    and exploitation generally，refers to using knowledge that the AI。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个奖励，因此将来可能学到的就是它应该总是选择这条路线，而不会探索并走这条路线。因此在强化学习中，探索与利用之间存在这种紧张关系，而利用通常指的是使用AI所掌握的知识。
- en: already has the AI already knows that，this is a move that leads to reward so。we'll
    go ahead and use that move an，exploration is all about exploring other。actions
    that we may not have explored as，thoroughly before because maybe one of。these
    actions even if I don't know，anything about it might lead to better。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: AI已经知道这是一个导致奖励的动作，因此我们会使用该动作，探索就是关于探索其他可能我们之前没有充分探索的动作，因为这些动作中的一个，即使我对此一无所知，也可能导致更好的结果。
- en: rewards faster or two more rewards in，the future and so an agent that only。ever
    exploits information and never，explores might be able to get reward but。it might
    not maximize its forwards，because it doesn't know what other，possibilities are
    out there。possibilities that would only know about，by taking advantage of exploration
    and。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 更快地获得奖励或未来获得更多奖励，因此一个仅仅利用信息而不进行探索的代理可能能够获得奖励，但它可能无法最大化其未来，因为它不知道还有其他可能性，这些可能性只能通过利用探索来了解。
- en: so how can we try and address this well，one possible solution is known as the。epsilon
    greedy algorithm where we set，epsilon equal to how often we want to。just make
    a random move where，occasionally we will just make a random。move in order to say
    let's try to，explore and see what happens and then。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何尝试解决这个问题呢？一个可能的解决方案称为epsilon贪婪算法，我们设定epsilon为我们希望多频繁随机选择一次，有时我们会随机选择一次，以便说“让我们尝试探索，看看会发生什么”，然后。
- en: the logic of the algorithm will be with，probability 1 minus Epsilon choose the。estimated
    best move in a greedy case，we'd always choose the best move but in。epsilon greedy
    were most of the time，going to choose the best movement or。sometimes going to
    choose the best move，but sometimes with probability Epsilon。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的逻辑将是以概率1减去Epsilon选择估计的最佳动作，在贪婪情况下，我们总是选择最佳动作，但在epsilon贪婪中，我们大多数时间会选择最佳动作，或者有时会以概率Epsilon选择最佳动作。
- en: we're going to choose a random move，instead so every time we're faced with。the
    ability to take an action sometimes，we're gonna choose the best move。sometimes
    we're just going to choose a，random move so this type of algorithm n。can be quite
    powerful in a reinforcement，learning context by not always just。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将选择一个随机动作，因此每次面临采取行动的能力时，有时我们会选择最佳动作，有时我们只会选择一个随机动作，因此这种类型的算法在强化学习背景下可能非常强大，而不仅仅是。
- en: choosing the best possible move right，now but sometimes especially early on。allowing
    yourself to make random moves，that allow you to explore various。different possible
    states and actions，more and maybe over time you might。decrease your value of Epsilon
    more and，more often choosing the best move after。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 选择现在可能的最佳动作，但有时，尤其是在早期，允许自己进行随机移动，让你探索各种不同的可能状态和动作，随着时间的推移，你可能会逐渐减少Epsilon的值，更频繁地选择最佳动作。
- en: you're more confident that you've，explored what all of the possibility is，actually。so
    we can put this into practice and one，very common application of reinforcement。learning
    is in game playing that if you，want to teach an agent how to play a。game you just
    let the agent play the，game a whole bunch and then the reward。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你更有信心已经探索了所有可能性，所以我们可以将其付诸实践，强化学习的一个非常常见的应用是在游戏中，如果你想教一个代理如何玩一个游戏，你只需让代理玩这个游戏很多次，然后奖励信号会在游戏结束时发生。
- en: signal happens at the end of the game，when the game is over if our a I won the。game
    it gets a reward of like 1 for，example and if it lost the game it gets。a reward
    of negative 1 and from that it，begins to learn what actions are good。and what
    actions are bad you don't have，to tell the AI what's good and what's。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当游戏结束时，如果我们的 AI 赢得了游戏，它会获得像 1 这样的奖励，例如，如果它输了游戏，它会得到负 1 的奖励，基于此，它开始学习哪些行动是好的，哪些行动是坏的，你不必告诉
    AI 什么是好，什么是坏。
- en: bad but the AI figures it out based on，that reward winning the game is some。signal
    losing the game is some signal，and based on all of that it begins to。figure out
    what decisions it should，actually make so one very simple game。which you may have
    played before is a，game called NIM and in the game of NIM。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 不好的结果，但是 AI 会根据这个奖励来判断，赢得游戏是某种信号，输掉游戏也是某种信号，基于所有这些信息，它开始弄清楚应该做出什么决策。一个你可能玩过的非常简单的游戏是
    NIM，在 NIM 游戏中。
- en: you've got a whole bunch of objects and，a whole bunch of different piles where。here
    I've represented each pile as an，individual row so you've got one object。in the
    first pile three in the second，pile five in the third pile seven in the。fourth
    pile and the game of NIM is a，two-player game where players take turns。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一堆物体和许多不同的堆，这里我将每个堆表示为一个独立的行，所以你在第一堆有一个物体，在第二堆有三个物体，在第三堆有五个物体，在第四堆有七个物体，NIM
    游戏是一个双人游戏，玩家轮流进行。
- en: removing objects from piles and the rule，is that on any given turn you were。allowed
    to remove as many objects as you，want from any one of these piles any one。of these
    rows you have to remove at，least one object but you can remove as。many as you
    want from exactly one of the，piles and whoever takes the last object。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从堆中移除物体的规则是，在任何给定的回合，你可以从这些堆中的任意一个移除任意数量的物体，任意一行你必须至少移除一个物体，但你可以从其中一个堆中移除任意数量的物体，而谁移走最后一个物体。
- en: loses so player one might like remove，four from this pile here player two。might
    remove four from this pile here so，now we've got four piles left one three。one
    and three and player one might，remove you know the entirety of the。second pile
    player two if they're being，strategic might remove all might remove。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 所以玩家一可能会从这个堆中移除四个物体，玩家二可能会从这个堆中移除四个物体，现在我们还有四堆，分别是三、一个和三，玩家一可能会从第二堆中移除全部物体，玩家二如果策略得当，可能会移除全部，或者只移除部分。
- en: two from the third pile，now we've got three piles left each with。one object
    left player one might remove，one from one pile player two removes one。from the
    other pile and now player one，is left with choosing this one object。from the last
    pile at which point player，one loses the game so fairly simple game。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从第三堆中移除两个，现在我们还有三堆，每堆剩下一个物体，玩家一可能从一堆中移除一个物体，玩家二从另一堆中移除一个物体，现在玩家一剩下的选择是从最后一堆中选择这个物体，玩家一在此时输掉了游戏，所以这是一个相当简单的游戏。
- en: piles of objects any turn you choose how，many objects to remove from a pile。whoever
    removes the last object loses，and this is the type of game you could。encode into
    an AI fairly easily because，the states are really just four numbers。every state
    is just how many objects in，each of the four piles and。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 物体的堆，每次轮到你选择从一个堆中移除多少个物体，移除最后一个物体的人会输掉，这是一种你可以相对容易地编码成 AI 的游戏，因为状态实际上只是四个数字。每个状态就是四个堆中每个堆的物体数量。
- en: actions are things like how many am I，going to remove from each one of these。individual
    piles and the reward happens，at the end that if you were the player。that had to
    remove the last object then，you get some sort of punishment but if。you were not
    and the other player had to，remove the last object well then you get。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 行动是指我将从每一个个别堆中移除多少个物体，而奖励则发生在最后，如果你是那个必须移除最后一个物体的玩家，那么你会受到某种惩罚，但如果你不是，另一个玩家必须移除最后一个物体，那么你就会获得奖励。
- en: some sort of reward so we could actually，try and show a demonstration of this。that
    I've implemented an AI to play the，game of NIM all right so here what we're。going
    to do is create an AI as a result，of training the AI on some number of。games that
    the AI is going to play，against itself where the idea is the AI。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一种奖励，因此我们实际上可以展示这一点。我已经实现了一个AI来玩NIM游戏。好吧，所以我们要做的是创建一个AI，通过训练该AI进行一些场次的游戏，让AI与自己对战，想法是AI将会。
- en: will play games against itself learn，from each of those experiences and learn。what
    to do in the future and then I the，human will play against the AI so。initially
    we'll say train zero times。![](img/b7a5235e5a53a7df848b73225d711ede_1.png)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在与自己对战的游戏中学习，从每一次经历中学习，了解未来该怎么做，然后我作为人类将与AI对战。所以最开始我们说训练零次。![](img/b7a5235e5a53a7df848b73225d711ede_1.png)
- en: meaning we're not gonna let the AI play，any practice games against itself in。order
    to learn from its experiences，we're just gonna see how well it plays。and it looks
    like there are four piles I，can choose how many I remove from any。one of the pile
    ISM so maybe from pile，three I will remove five objects for。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们不会让AI与自己进行任何练习游戏，以便从它的经验中学习，我们只会看看它表现得多好。看起来有四堆，我可以选择从其中任何一堆中移走多少物品。所以，也许我会从三堆中移走五个物品。
- en: example so now a I chose to take one，item from pile zero so I'm left with。these
    piles now for example and so here，I could choose maybe to say I would like。to
    remove and from pile two I'll remove，all five of them for example and so AI。chose
    to take two away from pile one now，I'm left with one pile that has one。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我选择从零堆中拿走一个物品，所以我现在留下这些堆。在这里，我可能会选择从二堆中移除所有五个物品。AI选择从一堆中移走两个，现在我剩下的有一堆只有一个物品。
- en: object one pile that has two objects so，from pile three I will remove two。objects
    and now I've left the ayah with，no choice but to take that last one。![](img/b7a5235e5a53a7df848b73225d711ede_3.png)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一堆有两个物品的堆，所以从三堆中我将移走两个物品，现在我让AI没有选择，只能拿走最后一个。![](img/b7a5235e5a53a7df848b73225d711ede_3.png)
- en: until the game is over and I was able to，win but I did so because the AI was。really
    just playing randomly it didn't，have any prior experience that it was。using in
    order to make these sorts of，judgments now let me let the AI train。itself on like
    ten thousand games I'm，gonna let the AI play ten thousand games。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 直到游戏结束，我才能赢，但这是因为AI实际上是在随机玩，它没有任何先前的经验来进行这些判断。现在让我让AI自己训练，大约进行一万场游戏，我会让AI玩一万场游戏。
- en: of NIMH against itself every time it，wins or loses it's going to learn from。![](img/b7a5235e5a53a7df848b73225d711ede_5.png)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次NIMH对战中，无论是赢还是输，它都会从中学习。![](img/b7a5235e5a53a7df848b73225d711ede_5.png)
- en: that experience and learn in the future，what to do and what not to do so here。then
    I'll go ahead and run this again，and now you see the AI running through a。whole
    bunch of training games ten，thousand training games against itself。and now it's
    going to let me make these，sorts of decisions so now I'm gonna play。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个经验中学习，未来该怎么做，未来不该做什么。接下来我将再次运行这个程序，现在你看到AI正在进行大量的训练游戏，大约一万场与自己对战的训练游戏。现在它将让我做这些决定，所以现在我准备好玩。
- en: against the AI and maybe I'll remove one，from pile three and the AI took。everything
    from pile three so I'm left，with three piles I'll go ahead and from。pile two maybe
    remove three items and，the AI removes one item from pile zero。and left with two
    piles each of which，has two items in it I'll remove one from。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在与AI对战时，也许我会从三堆中移走一个物品，AI则从三堆中移走所有物品，因此我剩下三堆。然后我会继续从二堆中移走三个物品，而AI从零堆中移走一个物品，留下两堆，每堆各有两个物品。我会从中移走一个物品。
- en: pile one I guess and the AI took two，from pile two leaving me with no choice，but
    to take one away。from pile one so it seems like after，playing 10，000 games of
    NIMH against。itself the AI has learned something，about what states and what actions
    tend。to be good and has begun to learn some，sort of pattern for how to predict
    what。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我想是从一堆中，AI从二堆中拿走两个，留下我没有选择，只能从一堆中移走一个物品。因此，看起来在玩了一万场NIMH游戏之后，AI已经学会了一些关于哪些状态和哪些动作是好的，并开始学习一些预测模式。
- en: actions are going to be good and what，actions are going to be bad in any given。state
    so reinforcement learning can be a，very powerful technique for achieving。these
    sorts of game playing agents，agents that are able to play a game well。just by
    learning from experience whether，that's playing against other people or。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 行为在任何给定状态下会是什么好的，什么行为会是坏的。因此，强化学习可以是一种非常强大的技术，用于实现这些类型的游戏代理，这些代理能够通过学习经验玩得很好，无论是对抗其他人还是。
- en: by playing against itself and learning，from those experiences as well now NIM。is
    a bit of an easy game to use，reinforcement learning for because there。are so few
    states there are only states，that are as many as how many different。objects are
    in each of these various，different piles you might imagine that。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通过与自己对弈并从那些经验中学习，现在NIM是一个相对简单的游戏，适合用于强化学习，因为状态太少，只有状态数等于这些不同堆中对象的数量。你可能想象。
- en: it's going to be harder if you think of，a game like chess or a games where there。are
    many many more States and many many，more actions that you can imagine taking。where
    it's not going to be as easy to，learn for every state and for every。action what
    the value is going to be so，oftentimes in that case we can't。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想到像棋类游戏这样的游戏，或者有很多状态和许多你可以想象的动作，那将会更难。对于每个状态和每个动作学习其值并不容易。因此，通常在这种情况下，我们无法。
- en: necessarily learn exactly what the value，is for every state and for every action。but
    we can approximate it so much as we，saw with minimax that we could use a。depth
    limiting approach to stop，calculating at a certain point in time，we can do a similar
    type of。approximation known as function，approximation in a reinforcement，learning
    context where instead of。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 不一定要准确学习每个状态和每个动作的值。但我们可以近似，就像我们在极小化中看到的那样，我们可以使用深度限制的方法在某个时间点停止计算。我们可以做一种称为函数近似的类似类型的近似，在强化学习上下文中，而不是。
- en: learning a value of Q for every state，and every action we just have some。function
    that estimates what the value，is for taking this action in this。particular state
    that might be based on，various different features of the state。that the environment
    that the agent，happens to be in where you might have to。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 学习每个状态和每个动作的Q值，我们只是有一些函数来估计在这个特定状态下采取这个动作的值，这可能基于该状态的各种不同特征。代理所处的环境，你可能必须。
- en: choose what those features actually are，but you can begin to learn some patterns。that
    generalize beyond one specific，state and one specific action that you。can begin
    to learn if certain features，tend to be good things or bad things。reinforcement
    learning can allow you，using a very，similar mechanism to generalize beyond。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 选择这些特征究竟是什么，但你可以开始学习一些模式，这些模式超越一个特定的状态和一个特定的动作。你可以开始学习某些特征是否倾向于是好的事情或坏的事情。强化学习允许你，使用非常，类似的机制来超越。
- en: one particular state and say if this，other state looks kind of like this。state
    then maybe the similar types of，actions that worked in one state will。also work
    in another state as well and，so this type of approach can be quite。helpful as
    you begin to deal with，reinforcement learning that exists in。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 某个特定状态并说如果这个，其他状态看起来有点像这样。那么，可能在一个状态下有效的，类似类型的行为也会在另一个状态下。随着你开始处理存在于的强化学习，这种方法可能会非常有帮助。
- en: larger and larger state spaces where，it's just not feasible to explore all of。the
    possible states that could actually，exist so there then are two of the main。categories
    of reinforcement learning，supervised learning where you have。labeled input and
    output pairs and，reinforcement learning where an agent。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 较大且越来越大的状态空间，其中探索所有可能存在的状态并不可行。因此，强化学习的两个主要类别是，有监督学习，即你有标记的输入和输出对，以及强化学习，即一个代理。
- en: learns from rewards or punishments and，it receives the third major category
    of。machine learning that we'll just touch，on briefly is known as unsupervised。learning
    and unsupervised learning，happens when we have data without any。additional feedback
    without labels that，in the supervised learning case all of。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从奖励或惩罚中学习，它接收的机器学习的第三个主要类别，我们将简要触及，称为无监督学习。无监督学习发生在我们有数据而没有任何额外反馈，没有标签的情况下，在有监督学习的情况下则是。
- en: our data had labels we label the data，point with whether that was a rainy day。or
    not rainy day and using those labels，we were able to infer what the pattern，banknote。and
    using those labels we were able to，draw inferences and patterns to figure。out
    what it does a bank note look like，versus not in unsupervised learning we。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据有标签，我们对数据进行标记，标记是否为下雨天。或者不是下雨天，使用这些标签，我们能够推断出模式，钞票。并且利用这些标签，我们能够，推导出推论和模式，以判断钞票的样子，与非钞票在无监督学习中的区别。
- en: don't have any access to any of those，labels but we still would like to learn。some
    of those patterns and one of the，tasks that you might want to perform an。unsupervised
    learning is something like，clustering we're clustering it's just a。task if given
    some set of objects，organize it into distinct clusters。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有访问任何这些标签的权限，但我们仍然希望学习一些模式。在无监督学习中，你可能想要执行的任务之一就是聚类，聚类只是一个任务，如果给定一组对象，将其组织成不同的簇。
- en: groups of objects that are similar to，one another and there's lots of。applications
    for clustering it comes up，in genetic research where you might have。a whole bunch
    of different genes and you，want to cluster them into similar genes。if you're trying
    to analyze across a，population or across species and it。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一组相似的对象，有很多聚类的应用，它出现在基因研究中，你可能有许多不同的基因，而你想将它们聚类为相似的基因。如果你尝试分析整个种群或跨物种，它。
- en: comes up in an image if you want to take，all the pixels of an image cluster
    them。and in different parts of the image，comes a lot a lot up in market research。if
    you want to divide your consumers，into different groups so you know which。groups
    to target with certain types of，product advertisements for example and a。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像中，如果你想将，图像的所有像素进行聚类，并且在图像的不同部分，会出现很多市场研究。如果你想将消费者，分成不同的组，以便知道哪些，组应该以某些类型的产品广告为目标。
- en: number of other contexts as well in，which clustering can be very applicable。one
    technique for clustering is an，algorithm known as k-means clustering。and what
    k-means clustering is going to，do is it is going to divide all of our。data points
    into K different clusters，and it's going to do so by repeating。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他许多上下文中，聚类也非常适用。一种聚类技术是，称为K均值聚类的算法。K均值聚类将会，做的是将我们的所有数据点划分为K个不同的簇，并通过重复来实现。
- en: this process of assigning points to。![](img/b7a5235e5a53a7df848b73225d711ede_7.png)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 将点分配给的过程。![](img/b7a5235e5a53a7df848b73225d711ede_7.png)
- en: clusters and then moving around those，clusters at centers we're going to。define
    a cluster by its center the，middle of the cluster and then assign。points to that
    cluster based on which，center is closest to that point and I'll。show you an example
    of that now here for，example I have a whole bunch of。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在这些簇的中心移动，我们将通过中心来定义一个簇，簇的中间，然后根据哪个中心离该点最近，将点分配给该簇，我现在给你一个示例，例如我有一大堆。
- en: unlabeled data just various data points，that are in some sort of graphical space。and
    I would like to group them into，various different clusters but I don't。know how
    to do that originally and let's，say I want to put assign like three。clusters to
    this group and you have to，choose how many clusters you want in。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 无标签数据只是各种在某种图形空间中的数据点。我想将它们分组为不同的簇，但我最初不知道如何做到这一点，假设我想给这个组分配三个簇，你必须选择你想要的簇的数量。
- en: k-means clustering that you could try，multiple and see how well those values。perform
    but I'll start just by randomly，picking some places to put the Centers。of those
    clusters and maybe I have a，blue cluster a red cluster in a green。cluster and
    I'm going to start with the，Centers of those clusters just being in。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: K均值聚类可以尝试多次，查看这些值的表现，但我将先随机选择一些位置，来放置这些簇的中心，可能我有一个，蓝色簇，一个红色簇和一个绿色簇，我将从这三个位置开始，作为这些簇的中心。
- en: these three locations here and what，k-means clustering tells us to do is。once
    I have the Centers of the clusters，of assign every point to a cluster based。on
    which cluster Center it is closest to，so we end up with something like this。where
    all of these points are closer to，the blue cluster Center than any other。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: K均值聚类告诉我们做的是，一旦我有了簇的中心，就要根据哪个簇的中心离每个点最近，来将每个点分配给一个簇，因此我们最终得到类似这样的结果，所有这些点都更靠近，蓝色簇的中心，而不是其他。
- en: cluster Center all of these points here，are closer to the green cluster Center。than
    any other cluster Center and then，these two points plus these points over。here
    those are all closest to the red，cluster Center instead so here then is。one possible
    assignment of all these，points to three different clusters but。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类中心，这里所有的点距离绿色聚类中心比任何其他聚类中心都更近，然后这两个点加上这边的这些点都是离红色聚类中心最近的。因此，这里是将所有这些点分配到三个不同聚类的一种可能方式。
- en: it's not great that it seems like in，this red cluster these points are kind。of
    far apart in this green cluster these，points are kind of far apart it might。not
    be my ideal choice of how I would，cluster these various different data。points
    but k-means clustering is an，iterative process that after I do this。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是很好，因为在这个红色聚类中，这些点似乎有点分散，而在这个绿色聚类中，这些点也有点分散，这可能不是我理想的聚类方式来处理这些不同的数据点，但k均值聚类是一个迭代过程，完成这一步之后。
- en: there's a next step which is that after，I've assigned all of the points to the。cluster
    Center that it is nearest to we，are going to recenter the clusters。meaning take
    the cluster centers these，diamond shapes here and move them to the。middle or the
    average effectively of all，of the points that are in that cluster。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 还有下一步，那就是在我将所有点分配到离它最近的聚类中心后，我们将重新确定聚类，意味着将这些菱形的聚类中心移动到该聚类中所有点的中间或平均值。
- en: so we'll take this blue point this blue，Center and go ahead and move it to the，middle
    or to the。Center of all of the points that were，assigned to the blue cluster moving
    it。slightly to the right in this case and，we'll do the same thing for red we'll。move
    the cluster Center to the middle of，all of these points weighted by how many。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们将这个蓝点和蓝色中心移动到分配给蓝色聚类的所有点的中心，稍微向右移动，在这种情况下，我们对红色做同样的事情，将聚类中心移动到这些点的中间，按数量加权。
- en: points there are there are more points，over here so the red centre ends up。moving
    a little bit further that way and，likewise for the Green Center there are。many
    more points on this plate this side，of the Green Center so the Green Center。ends
    up being pulled a little bit，further in this direction so we recenter。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有更多的点，所以红色中心会向那边移动一点，同样，绿色中心在这边的点也更多，因此绿色中心会稍微向这个方向移动，因此我们重新确定所有聚类的中心。
- en: all of the clusters and then we repeat，the process we go ahead and now reassign。all
    of the points to the cluster center，that they are now closest to and now。that
    we've moved around the cluster，centers these cluster assignments might。change
    that this point originally was，closer to the red cluster Center but now。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们重复这个过程，现在重新分配所有点到它们最近的聚类中心，现在因为我们已经移动了聚类中心，这些聚类分配可能会改变，原本这个点是离红色聚类中心最近的，但现在。
- en: it's actually closer to the blue cluster，Center same goes for this point as
    well。and these three points that were，originally closer to the green cluster。Center
    are now closer to the red cluster，Center instead so we can reassign what。colors
    are what's which clusters each of，these data points belongs to and then。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 它实际上离蓝色聚类中心更近，这个点也是如此，而这三个最初离绿色聚类中心更近的点，现在则离红色聚类中心更近，因此我们可以重新分配这些数据点属于哪个聚类。
- en: repeat the process again moving each of，these cluster means from the middles
    of。the cluster ism to the mean the average，of all of the other points that happen。to
    be there and repeat the process again，go ahead and assign each of the points。to
    the cluster that they are closest to，so once we reach a point where we've。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 再次重复这个过程，将每个聚类的均值从聚类的中心移动到所有其他点的均值，并再次重复这个过程，继续将每个点分配到离它们最近的聚类，因此一旦我们达到一个点。
- en: assigned all the points to clusters to，the cluster that they are nearest to
    and。nothing changed we've reached a sort of，equilibrium in this situation where
    no。points are changing their allegiance and，as a result we can declare this。algorithm
    is now over and we now have，some assignment of each of these points。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有点分配到离它们最近的聚类，并且没有变化，我们达到了某种平衡状态，在这种情况下没有点改变其归属，因此我们可以宣布这个算法现在结束，我们现在有了每个点的分配。
- en: into three different clusters and it，looks like we did a pretty good job of。trying
    to identify which points are more，similar to one another than they are two。points
    in other groups so we have the，green cluster down here this blue。cluster here
    and then this red cluster，over there as well and we did so without。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据分为三个不同的集群，看起来我们在识别哪些点彼此更相似而不是与其他组的点相似方面做得相当不错。因此我们有了下方的绿色集群、这里的蓝色集群，以及那边的红色集群，我们在没有。
- en: any access to some labels to tell us，what these various different clusters。were
    we just used an algorithm in an，unsupervised sense without any of those。labels
    to figure out which points belong，to which categories and again lots of。applications
    for this type of clustering，technique and there are many more。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并没有任何标签来告诉我们这些不同的集群是什么，而是以无监督的方式使用算法，来确定哪些点属于哪些类别，再次强调，这种聚类技术有很多应用，且还有更多。
- en: algorithms in each of these various，different fields within machine learning，supervised
    D'Andrea。foresman and unsupervised but those are，many of the big-picture foundational。ideas
    that underlie a lot of these，techniques the word that these are the。problems that
    we're trying to solve them，and we try and solve those problems。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的不同领域中，**D'Andrea**、**Foresman**所提到的监督与非监督算法是许多基础性大局观念的组成部分，这些理念支撑着我们要解决的许多问题。
- en: using a number of different methods of，trying to take data and learn patterns。in
    that data whether that's trying to，find neighboring data points that are。similar
    or trying to minimize some sort，of loss function or any number of other。techniques
    that allow us to begin to try，to solve these sorts of problems that。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多种不同的方法尝试从数据中学习模式，无论是寻找相似的邻近数据点，还是试图最小化某种损失函数，或其他许多技术，这些都使我们开始尝试解决这些问题。
- en: then was a look at some of the，principles that are at the foundation of。modern
    machine learning this ability to，take data and learn from that data so。that the
    computer can perform a task，even if they haven't explicitly been。given instructions
    in order to do so，next time we'll continue this。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们看看一些现代机器学习的基础原则，这种能力是从数据中学习，以便计算机可以执行任务，即使没有明确的指令来做到这一点，下次我们将继续这个主题。
- en: conversation about machine learning，looking in other techniques we can use。![](img/b7a5235e5a53a7df848b73225d711ede_9.png)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 关于机器学习的对话，探索我们可以使用的其他技术。![](img/b7a5235e5a53a7df848b73225d711ede_9.png)
