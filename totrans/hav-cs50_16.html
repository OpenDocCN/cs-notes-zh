<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Lecture 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Lecture 2</h1>
<blockquote>原文：<a href="https://cs50.harvard.edu/ai/notes/2/">https://cs50.harvard.edu/ai/notes/2/</a></blockquote>

                    

<h2 id="uncertainty">Uncertainty</h2>

<p>Last lecture, we discussed how AI can represent and derive new knowledge. However, often, in reality, the AI has only partial knowledge of the world, leaving space for uncertainty. Still, we would like our AI to make the best possible decision in these situations. For example, when predicting weather, the AI has information about the weather today, but there is no way to predict with 100% accuracy the weather tomorrow. Still, we can do better than chance, and today’s lecture is about how we can create AI that makes optimal decisions given limited information and uncertainty.</p>

<h2 id="probability">Probability</h2>

<p>Uncertainty can be represented as a number of events and the likelihood, or probability, of each of them happening.</p>

<p><strong>Possible Worlds</strong></p>

<p>Every possible situation can be thought of as a world, represented by the lowercase Greek letter omega ω. For example, rolling a die can result in six possible worlds: a world where the die yields a 1, a world where the die yields a 2, and so on. To represent the probability of a certain world, we write P(<em>ω</em>).</p>

<p><strong>Axioms in Probability</strong></p>

<ul>
  <li data-marker="*">0 &lt; P(<em>ω</em>) &lt; 1: every value representing probability must range between 0 and 1.
    <ul>
      <li data-marker="-">Zero is an impossible event, like rolling a standard die and getting a 7.</li>
      <li data-marker="-">One is an event that is certain to happen, like rolling a standard die and getting a value less than 10.</li>
      <li data-marker="-">In general, the higher the value, the more likely the event is to happen.</li>
    </ul>
  </li>
  <li data-marker="*">The probabilities of every possible event, when summed together, are equal to 1.</li>
</ul>

<p><img src="../Images/9e8aae5f3b3ce3d92df7dfd9290cf079.png" alt="Summing Probabilities" data-original-src="https://cs50.harvard.edu/ai/notes/2/lotp.png"/></p>

<p>The probability of rolling a number <em>R</em> with a standard die can be represented as P(<em>R</em>). In our case, P(<em>R</em>) = 1/6, because there are six possible worlds (rolling any number from 1 through 6) and each is equally likely to happen. Now, consider the event of rolling two dice. Now, there are 36 possible events, which are, again, equally as likely.</p>

<p><img src="../Images/7acb0bf3fad4d177724794bcad78abcf.png" alt="36 Events" data-original-src="https://cs50.harvard.edu/ai/notes/2/36events1.png"/></p>

<p>However, what happens if we try to predict the sum of the two dice? In this case, we have only 11 possible values (the sum has to range from 2 to 12), and they do not occur equally as often.</p>

<p><img src="../Images/0239d1fb8e6d4f8ad96b46b33e9ae6e8.png" alt="Sum of Two Dice" data-original-src="https://cs50.harvard.edu/ai/notes/2/sumdice.png"/></p>

<p>To get the probability of an event, we divide the number of worlds in which it occurs by the number of total possible worlds. For example, there are 36 possible worlds when rolling two dice. Only in one of these worlds, when both dice yield a 6, do we get the sum of 12. Thus, P(<em>12</em>) = 1/36, or, in words, the probability of rolling two dice and getting two numbers whose sum is 12 is 1/36. What is P(<em>7</em>)? We count and see that the sum 7 occurs in 6 worlds. Thus, P(<em>7</em>) = 6/36 = 1/6.</p>

<p><strong>Unconditional Probability</strong></p>

<p>Unconditional probability is the degree of belief in a proposition in the absence of any other evidence. All the questions that we have asked so far were questions of unconditional probability, because the result of rolling a die is not dependent on previous events.</p>

<h2 id="conditional-probability">Conditional Probability</h2>

<p>Conditional probability is the degree of belief in a proposition given some evidence that has already been revealed. As discussed in the introduction, AI can use partial information to make educated guesses about the future. To use this information, which affects the probability that the event occurs in the future, we rely on conditional probability.</p>

<p>Conditional probability is expressed using the following notation: P(<em>a | b</em>), meaning “the probability of event <em>a</em> occurring given that we know event <em>b</em> to have occurred,” or, more succinctly, “the probability of <em>a</em> given <em>b</em>.” Now we can ask questions like what is the probability of rain today given that it rained yesterday P(<em>rain today | rain yesterday</em>), or what is the probability of the patient having the disease given their test results P(<em>disease | test results</em>).</p>

<p>Mathematically, to compute the conditional probability of <em>a</em> given <em>b</em>, we use the following formula:</p>

<p><img src="../Images/3ed75a84fd781e595d28e8290b332c65.png" alt="Conditional Probability Formula" data-original-src="https://cs50.harvard.edu/ai/notes/2/conditional.png"/></p>

<p>To put it in words, the probability that <em>a</em> given <em>b</em> is true is equal to the probability of <em>a</em> and <em>b</em> being true, divided by the probability of <em>b</em>. An intuitive way of reasoning about this is the thought “we are interested in the events where both <em>a</em> and <em>b</em> are true (the numerator), but only from the worlds where we know <em>b</em> to be true (the denominator).” Dividing by <em>b</em> restricts the possible worlds to the ones where <em>b</em> is true. The following are algebraically equivalent forms to the formula above:</p>

<p><img src="../Images/c8f319de3f5b737991e1870e387f0f1d.png" alt="Equivalent Formulas" data-original-src="https://cs50.harvard.edu/ai/notes/2/conditionalequivalent.png"/></p>

<p>For example, consider P(<em>sum 12 | roll six on one die</em>), or the probability of rolling two dice and getting a sum of twelve, given that we have already rolled one die and got a six. To calculate this, we first restrict our worlds to the ones where the value of the first die is six:</p>

<p><img src="../Images/c6db2185d70a90243e95999a4e211c65.png" alt="Restricting the Worlds" data-original-src="https://cs50.harvard.edu/ai/notes/2/sumconditional1.png"/></p>

<p>Now we ask how many times does the event <em>a</em> (the sum being 12) occur in the worlds that we restricted the question to (dividing by P(<em>b</em>), or the probability of the first die yielding 6).</p>

<p><img src="../Images/b8ad79a4f9df28af93781db43fe37a08.png" alt="Conditioned Probability" data-original-src="https://cs50.harvard.edu/ai/notes/2/sumconditional2.png"/></p>

<h2 id="random-variables">Random Variables</h2>

<p>A random variable is a variable in probability theory with a domain of possible values that it can take on. For example, to represent possible outcomes when rolling a die, we can define a random variable <em>Roll</em>, that can take on the values {<em>1, 2, 3, 4, 5, 6</em>}. To represent the status of a flight, we can define a variable <em>Flight</em> that takes on the values {<em>on time, delayed, canceled</em>}.</p>

<p>Often, we are interested in the probability with which each value occurs. We represent this using a probability distribution. For example,</p>

<ul>
  <li data-marker="*">P(<em>Flight = on time</em>) = 0.6</li>
  <li data-marker="*">P(<em>Flight = delayed</em>) = 0.3</li>
  <li data-marker="*">P(<em>Flight = canceled</em>) = 0.1</li>
</ul>

<p>To interpret the probability distribution with words, this means that there is a 60% chance that the flight is on time, 30% chance that it is delayed, and 10% chance that it is canceled. Note that, as shown previously, the sum the probabilities of all possible outcomes is 1.</p>

<p>A probability distribution can be represented more succinctly as a vector. For example, <strong>P</strong>(<em>Flight</em>) = &lt;<em>0.6, 0.3, 0.1</em>&gt;. For this notation to be interpretable, the values have a set order (in our case, <em>on time, delayed, canceled</em>).</p>

<p><strong>Independence</strong></p>

<p>Independence is the knowledge that the occurrence of one event does not affect the probability of the other event. For example, when rolling two dice, the result of each die is independent from the other. Rolling a 4 with the first die does not influence the value of the second die that we roll. This is opposed to dependent events, like clouds in the morning and rain in the afternoon. If it is cloudy in the morning, it is more likely that it will rain in the afternoon, so these events are dependent.</p>

<p>Independence can be defined mathematically: events <em>a</em> and <em>b</em> are independent if and only if the probability of <em>a</em> and <em>b</em> is equal to the probability of <em>a</em> times the probability of <em>b</em>: P(<em>a ∧ b</em>) = P(<em>a</em>)P(<em>b</em>).</p>

<h2 id="bayes-rule">Bayes’ Rule</h2>

<p>Bayes’ rule is commonly used in probability theory to compute conditional probability. In words, Bayes’ rule says that the probability of <em>b</em> given <em>a</em> is equal to the probability of <em>a</em> given <em>b</em>, times the probability of <em>b</em>, divided by the probability of <em>a</em>.</p>

<p><img src="../Images/43f7f72a2ec8f7e902766aa8a8af95dd.png" alt="Bayes' Rule" data-original-src="https://cs50.harvard.edu/ai/notes/2/bayesrule.png"/></p>

<p>For example, we would like to compute the probability of it raining in the afternoon if there are clouds in the morning, or P(<em>rain | clouds</em>). We start with the following information:</p>

<ul>
  <li data-marker="*">80% of rainy afternoons start with cloudy mornings, or P(<em>clouds | rain</em>).</li>
  <li data-marker="*">40% of days have cloudy mornings, or P(<em>clouds</em>).</li>
  <li data-marker="*">10% of days have rainy afternoons, or P(<em>rain</em>).</li>
</ul>

<p>Applying Bayes’ rule, we compute (0.1)(0.8)/(0.4) = 0.2. That is, the probability that it rains in the afternoon given that it was cloudy in the morning is 20%.</p>

<p>Knowing P(<em>a | b</em>), in addition to P(<em>a</em>) and P(<em>b</em>), allows us to calculate P(<em>b | a</em>). This is helpful, because knowing the conditional probability of a visible effect given an unknown cause, P(<em>visible effect | unknown cause</em>), allows us to calculate the probability of the unknown cause given the visible effect, P(<em>unknown cause | visible effect</em>). For example, we can learn P(<em>medical test results | disease</em>) through medical trials, where we test people with the disease and see how often the test picks up on that. Knowing this, we can calculate P(<em>disease | medical test results</em>), which is valuable diagnostic information.</p>

<h2 id="joint-probability">Joint Probability</h2>

<p>Joint probability is the likelihood of multiple events all occurring.</p>

<p>Let us consider the following example, concerning the probabilities of clouds in the morning and rain in the afternoon.</p>

<table>
  <thead>
    <tr>
      <th>C = <em>cloud</em></th>
      <th>C = <em>¬cloud</em></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.4</td>
      <td>0.6</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th>R = <em>rain</em></th>
      <th>R = <em>¬rain</em></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0.1</td>
      <td>0.9</td>
    </tr>
  </tbody>
</table>

<p>Looking at these data, we can’t say whether clouds in the morning are related to the likelihood of rain in the afternoon. To be able to do so, we need to look at the joint probabilities of all the possible outcomes of the two variables. We can represent this in a table as follows:</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>R = <em>rain</em></th>
      <th>R = <em>¬rain</em></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>C = <em>cloud</em></td>
      <td>0.08</td>
      <td>0.32</td>
    </tr>
    <tr>
      <td>C = <em>¬cloud</em></td>
      <td>0.02</td>
      <td>0.58</td>
    </tr>
  </tbody>
</table>

<p>Now we are able to know information about the co-occurrence of the events. For example, we know that the probability of a certain day having clouds in the morning and rain in the afternoon is 0.08. The probability of no clouds in the morning and no rain in the afternoon is 0.58.</p>

<p>Using joint probabilities, we can deduce conditional probability. For example, if we are interested in the probability distribution of clouds in the morning given rain in the afternoon. P(<em>C | rain</em>) = P(<em>C, rain</em>)/P(<em>rain</em>) (a side note: in probability, commas and ∧ are used interchangeably). Thus, P(<em>C, rain</em>) = P(<em>C ∧ rain</em>)). In words, we divide the joint probability of rain and clouds by the probability of rain.</p>

<p>In the last equation, it is possible to view P(<em>rain</em>) as some constant by which P(<em>C, rain</em>) is multiplied. Thus, we can rewrite P(<em>C, rain</em>)/P(<em>rain</em>) = αP(<em>C, rain</em>), or α&lt;0.08, 0.02&gt;. Factoring out α leaves us with the proportions of the probabilities of the possible values of C given that there is rain in the afternoon. Namely, if there is rain in the afternoon, the proportion of the probabilities of clouds in the morning and no clouds in the morning is 0.08:0.02. Note that 0.08 and 0.02 don’t sum up to 1; however, since this is the probability distribution for the random variable C, we know that they should sum up to 1. Therefore, we need to normalize the values by computing α such that α0.08 + α0.02 = 1. Finally, we can say that P(<em>C | rain</em>) = &lt;0.8, 0.2&gt;.</p>

<h2 id="probability-rules">Probability Rules</h2>

<ul>
  <li data-marker="*"><strong>Negation</strong>: P(<em>¬a</em>) = 1 - P(<em>a</em>). This stems from the fact that the sum of the probabilities of all the possible worlds is 1, and the complementary literals <em>a</em> and <em>¬a</em> include all the possible worlds.</li>
  <li data-marker="*"><strong>Inclusion-Exclusion</strong>: P(<em>a ∨ b</em>) = P(<em>a</em>) + P(<em>b</em>) - P(<em>a ∧ b</em>). This can interpreted in the following way: the worlds in which <em>a</em> or <em>b</em> are true are equal to all the worlds where <em>a</em> is true, plus the worlds where <em>b</em> is true. However, in this case, some worlds are counted twice (the worlds where both <em>a</em> and <em>b</em> are true)). To get rid of this overlap, we subtract once the worlds where both <em>a</em> and <em>b</em> are true (since they were counted twice).
    <blockquote>
      <p>Here is an example from outside lecture that can elucidate this. Suppose I eat ice cream 80% of days and cookies 70% of days. If we’re calculating the probability that today I eat ice cream or cookies P(<em>ice cream ∨ cookies</em>) without subtracting P(<em>ice cream ∧ cookies</em>), we erroneously end up with <del>0.7 + 0.8 = 1.5</del>. This contradicts the axiom that probability ranges between 0 and 1. To correct for counting twice the days when I ate both ice cream and cookies, we need to subtract P(<em>ice cream ∧ cookies</em>) once.</p>
    </blockquote>
  </li>
  <li data-marker="*"><strong>Marginalization</strong>: P(<em>a</em>) = P(<em>a, b</em>) + P(<em>a, ¬b</em>). The idea here is that <em>b</em> and <em>¬b</em> are disjoint probabilities. That is, the probability of <em>b</em> and <em>¬b</em> occurring at the same time is 0. We also know <em>b</em> and <em>¬b</em> sum up to 1. Thus, when <em>a</em> happens, <em>b</em> can either happen or not. When we take the probability of both <em>a</em> and <em>b</em> happening in addition to the probability of <em>a</em> and <em>¬b</em>, we end up with simply the probability of <em>a</em>.</li>
</ul>

<p>Marginalization can be expressed for random variables the following way:</p>

<p><img src="../Images/f16133fa0d9aad9784df103985812661.png" alt="Marginalization" data-original-src="https://cs50.harvard.edu/ai/notes/2/marginalization.png"/></p>

<p>The left side of the equation means “The probability of random variable X having the value xᵢ.” For example, for the variable C we mentioned earlier, the two possible values are <em>clouds in the morning</em> and <em>no clouds in the morning</em>. The right part of the equation is the idea of marginalization. P(<em>X = xᵢ</em>) is equal to the sum of all the joint probabilities of xᵢ and every single value of the random variable Y. For example, P(<em>C = cloud</em>) = P(<em>C = cloud, R = rain</em>) + P(<em>C = cloud, R = ¬rain</em>) = 0.08 + 0.32 = 0.4.</p>

<ul>
  <li data-marker="*"><strong>Conditioning</strong>: P(<em>a</em>) = P(<em>a | b</em>)P(<em>b</em>) + P(<em>a | ¬b</em>)P(<em>¬b</em>). This is a similar idea to marginalization. The probability of event <em>a</em> occurring is equal to the probability of <em>a</em> given <em>b</em> times the probability of <em>b</em>, plus the probability of <em>a</em> given <em>¬b</em> time the probability of <em>¬b</em>.</li>
</ul>

<p><img src="../Images/eff11ee634038b28e8728fea53276b00.png" alt="Conditioning" data-original-src="https://cs50.harvard.edu/ai/notes/2/conditioning.png"/></p>

<p>In this formula, the random variable X takes the value xᵢ with probability that is equal to the sum of the probabilities of xᵢ given each value of the random variable Y multiplied by the probability of variable Y taking that value. This makes sense if we remember that P(<em>a | b</em>) = P(<em>a, b</em>)/P(<em>b</em>). If we multiply this expression by P(<em>b</em>), we end up with P(<em>a, b</em>), and from here we do the same as we did with marginalization.</p>

<h2 id="bayesian-networks">Bayesian Networks</h2>

<p>A Bayesian network is a data structure that represents the dependencies among random variables. Bayesian networks have the following properties:</p>

<ul>
  <li data-marker="*">They are directed graphs.</li>
  <li data-marker="*">Each node on the graph represent a random variable.</li>
  <li data-marker="*">An arrow from X to Y represents that X is a parent of Y. That is, the probability distribution of Y depends on the value of X.</li>
  <li data-marker="*">Each node X has probability distribution P(<em>X | Parents(X)</em>).</li>
</ul>

<p>Let’s consider an example of a Bayesian network that involves variables that affect whether we get to our appointment on time.</p>

<p><img src="../Images/6f5cda1e10d03dac442fc6b714c0d513.png" alt="Bayesian Network" data-original-src="https://cs50.harvard.edu/ai/notes/2/bayesiannetwork.png"/></p>

<p>Let’s describe this Bayesian network from the top down:</p>

<ul>
  <li data-marker="*">
    <p>Rain is the root node in this network. This means that its probability distribution is not reliant on any prior event. In our example, Rain is a random variable that can take the values {<em>none, light, heavy</em>} with the following probability distribution:</p>

    <table>
      <thead>
        <tr>
          <th><em>none</em></th>
          <th><em>light</em></th>
          <th><em>heavy</em></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>0.7</td>
          <td>0.2</td>
          <td>0.1</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li data-marker="*">
    <p>Maintenance, in our example, encodes whether there is train track maintenance, taking the values {<em>yes, no</em>}. Rain is a parent node of Maintenance, which means that the probability distribution of Maintenance is affected by Rain.</p>

    <table>
      <thead>
        <tr>
          <th>R</th>
          <th><em>yes</em></th>
          <th><em>no</em></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><em>none</em></td>
          <td>0.4</td>
          <td>0.6</td>
        </tr>
        <tr>
          <td><em>light</em></td>
          <td>0.2</td>
          <td>0.8</td>
        </tr>
        <tr>
          <td><em>heavy</em></td>
          <td>0.1</td>
          <td>0.9</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li data-marker="*">
    <p>Train is the variable that encodes whether the train is on time or delayed, taking the values {<em>on time, delayed</em>}. Note that Train has arrows pointing to it from both Maintenance and Rain. This means that both are parents of Train, and their values affect the probability distribution of Train.</p>

    <table>
      <thead>
        <tr>
          <th>R</th>
          <th>M</th>
          <th><em>on time</em></th>
          <th><em>delayed</em></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><em>none</em></td>
          <td>yes</td>
          <td>0.8</td>
          <td>0.2</td>
        </tr>
        <tr>
          <td><em>none</em></td>
          <td>no</td>
          <td>0.9</td>
          <td>0.1</td>
        </tr>
        <tr>
          <td><em>light</em></td>
          <td>yes</td>
          <td>0.6</td>
          <td>0.4</td>
        </tr>
        <tr>
          <td><em>light</em></td>
          <td>no</td>
          <td>0.7</td>
          <td>0.3</td>
        </tr>
        <tr>
          <td><em>heavy</em></td>
          <td>yes</td>
          <td>0.4</td>
          <td>0.6</td>
        </tr>
        <tr>
          <td><em>heavy</em></td>
          <td>no</td>
          <td>0.5</td>
          <td>0.5</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li data-marker="*">
    <p>Appointment is a random variable that represents whether we attend our appointment, taking the values {<em>attend, miss</em>}. Note that its only parent is Train. This point about Bayesian network is noteworthy: parents include only direct relations. It is true that maintenance affects whether the train is on time, and whether the train is on time affects whether we attend the appointment. However, in the end, what directly affects our chances of attending the appointment is whether the train came on time, and this is what is represented in the Bayesian network. For example, if the train came on time, it could be heavy rain and track maintenance, but that has no effect over whether we made it to our appointment.</p>

    <table>
      <thead>
        <tr>
          <th>T</th>
          <th><em>attend</em></th>
          <th><em>miss</em></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><em>on time</em></td>
          <td>0.9</td>
          <td>0.1</td>
        </tr>
        <tr>
          <td><em>delayed</em></td>
          <td>0.6</td>
          <td>0.4</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>For example, if we want to find the probability of missing the meeting when the train was delayed on a day with no maintenance and light rain, or P(<em>light, no, delayed, miss</em>), we will compute the following: P(<em>light</em>)P(<em>no | light</em>)P(<em>delayed | light, no</em>)P(<em>miss | delayed</em>). The value of each of the individual probabilities can be found in the probability distributions above, and then these values are multiplied to produce P(<em>no, light, delayed, miss</em>).</p>

<p><strong>Inference</strong></p>

<p>At the last lecture, we looked at inference through entailment. This means that we could definitively conclude new information based on the information that we already had. We can also infer new information based on probabilities. While this does not allow us to know new information for certain, it allows us to figure out the probability distributions for some values. Inference has multiple properties.</p>

<ul>
  <li data-marker="*">Query <strong>X</strong>: the variable for which we want to compute the probability distribution.</li>
  <li data-marker="*">Evidence variables <strong>E</strong>: one or more variables that have been observed for event <strong>e</strong>. For example, we might have observed that there is light rain, and this observation helps us compute the probability that the train is delayed.</li>
  <li data-marker="*">Hidden variables <strong>Y</strong>: variables that aren’t the query and also haven’t been observed. For example, standing at the train station, we can observe whether there is rain, but we can’t know if there is maintenance on the track further down the road. Thus, Maintenance would be a hidden variable in this situation.</li>
  <li data-marker="*">The goal: calculate <strong>P</strong>(<em>X | e</em>). For example, compute the probability distribution of the Train variable (the query) based on the evidence <strong>e</strong> that we know there is light rain.</li>
</ul>

<p>Let’s take an example. We want to compute the probability distribution of the Appointment variable given the evidence that there is light rain and no track maintenance. That is, we know that there is light rain and no track maintenance, and we want to figure out what are the probabilities that we attend the appointment and that we miss the appointment, <strong>P</strong>(<em>Appointment | light, no</em>). from the <a href="#joint-probability">joint probability</a> section, we know that we can express the possible values of the Appointment random variable as a proportion, rewriting <strong>P</strong>(<em>Appointment | light, no</em>) as αP(<em>Appointment, light, no</em>). How can we calculate the probability distribution of Appointment if its parent is only the Train variable, and not Rain or Maintenance? Here, we will use marginalization. The value of <strong>P</strong>(<em>Appointment, light, no</em>) is equal to α[<strong>P</strong>(<em>Appointment, light, no, delayed</em>) + <strong>P</strong>(<em>Appointment, light, no, on time</em>)].</p>

<p><strong>Inference by Enumeration</strong></p>

<p>Inference by enumeration is a process of finding the probability distribution of variable X given observed evidence e and some hidden variables Y.</p>

<p><img src="../Images/fae66d8c519d7a9a9ee418baea0ad29b.png" alt="Inference by Enumeration" data-original-src="https://cs50.harvard.edu/ai/notes/2/inferencebyenumeration.png"/></p>

<p>In this equation, X stand for the query variable, e for the observed evidence, y for all the values of the hidden variables, and α normalizes the result such that we end up with probabilities that add up to 1. To explain the equation in words, it is saying that the probability distribution of X given e is equal to a normalized probability distribution of X and e. To get to this distribution, we sum the normalized probability of X, e, and y, where y takes each time a different value of the hidden variables Y.</p>

<p>Multiple libraries exist in Python to ease the process of probabilistic inference. We will take a look at the library <em>pomegranate</em> to see how the above data can be represented in code.</p>

<p>First, we create the nodes and provide a probability distribution for each one.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pomegranate</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Rain node has no parents
</span><span class="n">rain</span> <span class="o">=</span> <span class="nc">Node</span><span class="p">(</span><span class="nc">DiscreteDistribution</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">none</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">light</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">heavy</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.1</span>
<span class="p">}),</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">rain</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Track maintenance node is conditional on rain
</span><span class="n">maintenance</span> <span class="o">=</span> <span class="nc">Node</span><span class="p">(</span><span class="nc">ConditionalProbabilityTable</span><span class="p">([</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">none</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">yes</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">none</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">no</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">light</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">yes</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">light</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">no</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">heavy</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">yes</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">heavy</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">no</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]</span>
<span class="p">],</span> <span class="p">[</span><span class="n">rain</span><span class="p">.</span><span class="n">distribution</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">maintenance</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Train node is conditional on rain and maintenance
</span><span class="n">train</span> <span class="o">=</span> <span class="nc">Node</span><span class="p">(</span><span class="nc">ConditionalProbabilityTable</span><span class="p">([</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">none</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">yes</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">on time</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">none</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">yes</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">delayed</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">none</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">no</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">on time</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">none</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">no</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">delayed</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">light</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">yes</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">on time</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">light</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">yes</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">delayed</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">light</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">no</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">on time</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">light</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">no</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">delayed</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">heavy</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">yes</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">on time</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">heavy</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">yes</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">delayed</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">heavy</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">no</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">on time</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">heavy</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">no</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">delayed</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
<span class="p">],</span> <span class="p">[</span><span class="n">rain</span><span class="p">.</span><span class="n">distribution</span><span class="p">,</span> <span class="n">maintenance</span><span class="p">.</span><span class="n">distribution</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Appointment node is conditional on train
</span><span class="n">appointment</span> <span class="o">=</span> <span class="nc">Node</span><span class="p">(</span><span class="nc">ConditionalProbabilityTable</span><span class="p">([</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">on time</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">attend</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">on time</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">miss</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">delayed</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">attend</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">delayed</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">miss</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]</span>
<span class="p">],</span> <span class="p">[</span><span class="n">train</span><span class="p">.</span><span class="n">distribution</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">appointment</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p>Second, we create the model by adding all the nodes and then describing which node is the parent of which other node by adding edges between them (recall that a Bayesian network is a directed graph, consisting of nodes with arrows between them).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create a Bayesian Network and add states
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">BayesianNetwork</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add_states</span><span class="p">(</span><span class="n">rain</span><span class="p">,</span> <span class="n">maintenance</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">appointment</span><span class="p">)</span>

<span class="c1"># Add edges connecting nodes
</span><span class="n">model</span><span class="p">.</span><span class="nf">add_edge</span><span class="p">(</span><span class="n">rain</span><span class="p">,</span> <span class="n">maintenance</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add_edge</span><span class="p">(</span><span class="n">rain</span><span class="p">,</span> <span class="n">train</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add_edge</span><span class="p">(</span><span class="n">maintenance</span><span class="p">,</span> <span class="n">train</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add_edge</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">appointment</span><span class="p">)</span>

<span class="c1"># Finalize model
</span><span class="n">model</span><span class="p">.</span><span class="nf">bake</span><span class="p">()</span>
</code></pre></div></div>

<p>Now, to ask how probable a certain event is, we run the model with the values we are interested in. In this example, we want to ask what is the probability that there is no rain, no track maintenance, the train is on time, and we attend the meeting.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Calculate probability for a given observation
</span><span class="n">probability</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">probability</span><span class="p">([[</span><span class="sh">"</span><span class="s">none</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">no</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">on time</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">attend</span><span class="sh">"</span><span class="p">]])</span>

<span class="nf">print</span><span class="p">(</span><span class="n">probability</span><span class="p">)</span>
</code></pre></div></div>

<p>Otherwise, we could use the program to provide probability distributions for all variables given some observed evidence. In the following case, we know that the train was delayed. Given this information, we compute and print the probability distributions of the variables Rain, Maintenance, and Appointment.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Calculate predictions based on the evidence that the train was delayed
</span><span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">delayed</span><span class="sh">"</span>
<span class="p">})</span>

<span class="c1"># Print predictions for each node
</span><span class="k">for</span> <span class="n">node</span><span class="p">,</span> <span class="n">prediction</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">states</span><span class="p">,</span> <span class="n">predictions</span><span class="p">):</span>
    <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">node</span><span class="p">.</span><span class="n">name</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">prediction</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">node</span><span class="p">.</span><span class="n">name</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">value</span><span class="p">,</span> <span class="n">probability</span> <span class="ow">in</span> <span class="n">prediction</span><span class="p">.</span><span class="n">parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">items</span><span class="p">():</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">    </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">probability</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p>The code above used inference by enumeration. However, this way of computing probability is inefficient, especially when there are many variables in the model. A different way to go about this would be abandoning <strong>exact inference</strong> in favor of <strong>approximate inference</strong>. Doing this, we lose some precision in the generated probabilities, but often this imprecision is negligible. Instead, we gain a scalable method of calculating probabilities.</p>

<h2 id="sampling">Sampling</h2>

<p>Sampling is one technique of approximate inference. In sampling, each variable is sampled for a value according to its probability distribution. We will start with an example from outside lecture, and then cover the example from lecture.</p>

<blockquote>
  <p>To generate a distribution using sampling with a die, we can roll the die multiple times and record what value we got each time. Suppose we rolled the die 600 times. We count how many times we got 1, which is supposed to be roughly 100, and then repeat for the rest of the values, 2-6. Then, we divide each count by the total number of rolls. This will generate an approximate distribution of the values of rolling a die: on one hand, it is unlikely that we get the result that each value has a probability of 1/6 of occurring (which is the exact probability), but we will get a value that’s close to it.</p>
</blockquote>

<p>Here is an example from lecture: if we start with sampling the Rain variable, the value <em>none</em> will be generated with probability of 0.7, the value <em>light</em> will be generated with probability of 0.2, and the value <em>heavy</em> will be generated with probability of 0.1. Suppose that the sampled value we get is <em>none</em>. When we get to the Maintenance variable, we sample it, too, but only from the probability distribution where Rain is equal to <em>none</em>, because this is an already sampled result. We will continue to do so through all the nodes. Now we have one sample, and repeating this process multiple times generates a distribution. Now, if we want to answer a question, such as what is P(<em>Train = on time</em>), we can count the number of samples where the variable Train has the value <em>on time</em>, and divide the result by the total number of samples. This way, we have just generated an approximate probability for P(<em>Train = on time</em>).</p>

<p>We can also answer questions that involve conditional probability, such as P(<em>Rain = light | Train = on time</em>). In this case, we ignore all samples where the value of Train is not <em>on time</em>, and then proceed as before. We count how many samples have the variable Rain = <em>light</em> among those samples that have Train = <em>on time</em>, and then divide by the total number of samples where Train = <em>on time</em>.</p>

<p>In code, a sampling function can look like <code class="language-plaintext highlighter-rouge">generate_sample</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pomegranate</span>

<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="kn">from</span> <span class="n">model</span> <span class="kn">import</span> <span class="n">model</span>

<span class="k">def</span> <span class="nf">generate_sample</span><span class="p">():</span>

    <span class="c1"># Mapping of random variable name to sample generated
</span>    <span class="n">sample</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Mapping of distribution to sample generated
</span>    <span class="n">parents</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Loop over all states, assuming topological order
</span>    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">states</span><span class="p">:</span>

        <span class="c1"># If we have a non-root node, sample conditional on parents
</span>        <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">state</span><span class="p">.</span><span class="n">distribution</span><span class="p">,</span> <span class="n">pomegranate</span><span class="p">.</span><span class="n">ConditionalProbabilityTable</span><span class="p">):</span>
            <span class="n">sample</span><span class="p">[</span><span class="n">state</span><span class="p">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="p">.</span><span class="n">distribution</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">parent_values</span><span class="o">=</span><span class="n">parents</span><span class="p">)</span>

        <span class="c1"># Otherwise, just sample from the distribution alone
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="n">sample</span><span class="p">[</span><span class="n">state</span><span class="p">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="p">.</span><span class="n">distribution</span><span class="p">.</span><span class="nf">sample</span><span class="p">()</span>

        <span class="c1"># Keep track of the sampled value in the parents mapping
</span>        <span class="n">parents</span><span class="p">[</span><span class="n">state</span><span class="p">.</span><span class="n">distribution</span><span class="p">]</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[</span><span class="n">state</span><span class="p">.</span><span class="n">name</span><span class="p">]</span>

    <span class="c1"># Return generated sample
</span>    <span class="k">return</span> <span class="n">sample</span>
</code></pre></div></div>

<p>Now, to compute P(<em>Appointment | Train = delayed</em>), which is the probability distribution of the Appointment variable given that the train is delayed, we do the following:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Rejection sampling
# Compute distribution of Appointment given that train is delayed
</span><span class="n">N</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Repeat sampling 10,000 times
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>

    <span class="c1"># Generate a sample based on the function that we defined earlier
</span>    <span class="n">sample</span> <span class="o">=</span> <span class="nf">generate_sample</span><span class="p">()</span>

    <span class="c1"># If, in this sample, the variable of Train has the value delayed, save the sample. Since we are interested interested in the probability distribution of Appointment given that the train is delayed, we discard the sampled where the train was on time.
</span>    <span class="k">if</span> <span class="n">sample</span><span class="p">[</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">]</span> <span class="o">==</span> <span class="sh">"</span><span class="s">delayed</span><span class="sh">"</span><span class="p">:</span>
        <span class="n">data</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="sh">"</span><span class="s">appointment</span><span class="sh">"</span><span class="p">])</span>

<span class="c1"># Count how many times each value of the variable appeared. We can later normalize by dividing the results by the total number of saved samples to get the approximate probabilities of the variable that add up to 1.
</span><span class="nf">print</span><span class="p">(</span><span class="nc">Counter</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>

</code></pre></div></div>

<p><strong>Likelihood Weighting</strong></p>

<p>In the sampling example above, we discarded the samples that did not match the evidence that we had. This is inefficient. One way to get around this is with likelihood weighting, using the following steps:</p>

<ul>
  <li data-marker="*">Start by fixing the values for evidence variables.</li>
  <li data-marker="*">Sample the non-evidence variables using conditional probabilities in the Bayesian network.</li>
  <li data-marker="*">Weight each sample by its <strong>likelihood</strong>: the probability of all the evidence occurring.</li>
</ul>

<p>For example, if we have the observation that the train was on time, we will start sampling as before. We sample a value of Rain given its probability distribution, then Maintenance, but when we get to Train - we always give it the observed value, in our case, <em>on time</em>. Then we proceed and sample Appointment based on its probability distribution given Train = <em>on time</em>. Now that this sample exists, we weight it by the conditional probability of the observed variable given its sampled parents. That is, if we sampled Rain and got <em>light</em>, and then we sampled Maintenance and got <em>yes</em>, then we will weight this sample by P(<em>Train = on time | light, yes</em>).</p>

<h2 id="markov-models">Markov Models</h2>

<p>So far, we have looked at questions of probability given some information that we observed. In this kind of paradigm, the dimension of time is not represented in any way. However, many tasks do rely on the dimension of time, such as prediction. To represent the variable of time we will create a new variable, X, and change it based on the event of interest, such that Xₜ is the current event, Xₜ₊₁ is the next event, and so on. To be able to predict events in the future, we will use Markov Models.</p>

<p><strong>The Markov Assumption</strong></p>

<p>The Markov assumption is an assumption that the current state depends on only a finite fixed number of previous states. This is important to us. Think of the task of predicting weather. In theory, we could use all the data from the past year to predict tomorrow’s weather. However, it is infeasible, both because of the computational power this would require and because there is probably no information about the conditional probability of tomorrow’s weather based on the weather 365 days ago. Using the Markov assumption, we restrict our previous states (e.g. how many previous days we are going to consider when predicting tomorrow’s weather), thereby making the task manageable. This means that we might get a more rough approximation of the probabilities of interest, but this is often good enough for our needs. Moreover, we can use a Markov model based on the information of the one last event (e.g. predicting tomorrow’s weather based on today’s weather).</p>

<p><strong>Markov Chain</strong></p>

<p>A Markov chain is a sequence of random variables where the distribution of each variable follows the Markov assumption. That is, each event in the chain occurs based on the probability of the event before it.</p>

<p>To start constructing a Markov chain, we need a <strong>transition model</strong> that will specify the the probability distributions of the next event based on the possible values of the current event.</p>

<p><img src="../Images/02c1ff11b8f76b491b14bcf0634aae7f.png" alt="Transition Model" data-original-src="https://cs50.harvard.edu/ai/notes/2/transitionmodel.png"/></p>

<p>In this example, the probability of tomorrow being sunny based on today being sunny is 0.8. This is reasonable, because it is more likely than not that a sunny day will follow a sunny day. However, if it is rainy today, the probability of rain tomorrow is 0.7, since rainy days are more likely to follow each other. Using this transition model, it is possible to sample a Markov chain. Start with a day being either rainy or sunny, and then sample the next day based on the probability of it being sunny or rainy given the weather today. Then, condition the probability of the day after tomorrow based on tomorrow, and so on, resulting in a Markov chain:</p>

<p><img src="../Images/33e9dec509c66164093b95d786019dd2.png" alt="Markov Chain" data-original-src="https://cs50.harvard.edu/ai/notes/2/markovchain.png"/></p>

<p>Given this Markov chain, we can now answer questions such as “what is the probability of having four rainy days in a row?” Here is an example of how a Markov chain can be implemented in code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pomegranate</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Define starting probabilities
</span><span class="n">start</span> <span class="o">=</span> <span class="nc">DiscreteDistribution</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">sun</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">rain</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.5</span>
<span class="p">})</span>

<span class="c1"># Define transition model
</span><span class="n">transitions</span> <span class="o">=</span> <span class="nc">ConditionalProbabilityTable</span><span class="p">([</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">sun</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">sun</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">sun</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">rain</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">rain</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">sun</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">rain</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">rain</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]</span>
<span class="p">],</span> <span class="p">[</span><span class="n">start</span><span class="p">])</span>

<span class="c1"># Create Markov chain
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">MarkovChain</span><span class="p">([</span><span class="n">start</span><span class="p">,</span> <span class="n">transitions</span><span class="p">])</span>

<span class="c1"># Sample 50 states from chain
</span><span class="nf">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="mi">50</span><span class="p">))</span>
</code></pre></div></div>

<h2 id="hidden-markov-models">Hidden Markov Models</h2>

<p>A hidden Markov model is a type of a Markov model for a system with hidden states that generate some observed event. This means that sometimes, the AI has some measurement of the world but no access to the precise state of the world. In these cases, the state of the world is called the <strong>hidden state</strong> and whatever data the AI has access to are the <strong>observations</strong>. Here are a few examples for this:</p>

<ul>
  <li data-marker="*">For a robot exploring uncharted territory, the hidden state is its position, and the observation is the data recorded by the robot’s sensors.</li>
  <li data-marker="*">In speech recognition, the hidden state is the words that were spoken, and the observation is the audio waveforms.</li>
  <li data-marker="*">When measuring user engagement on websites, the hidden state is how engaged the user is, and the observation is the website or app analytics.</li>
</ul>

<p>For our discussion, we will use the following example. Our AI wants to infer the weather (the hidden state), but it only has access to an indoor camera that records how many people brought umbrellas with them. Here is our <strong>sensor model</strong> (also called <strong>emission model</strong>) that represents these probabilities:</p>

<p><img src="../Images/807a4fc8eb5bb432577d55934b18fe77.png" alt="Sensor Model" data-original-src="https://cs50.harvard.edu/ai/notes/2/sensormodel.png"/></p>

<p>In this model, if it is sunny, it is most probable that people will not bring umbrellas to the building. If it is rainy, then it is very likely that people bring umbrellas to the building. By using the observation of whether people brought an umbrella or not, we can predict with reasonable likelihood what the weather is outside.</p>

<p><strong>Sensor Markov Assumption</strong></p>

<p>The assumption that the evidence variable depends only on the corresponding state. For example, for our models, we assume that whether people bring umbrellas to the office depends only on the weather. This is not necessarily reflective of the complete truth, because, for example, more conscientious, rain-averse people might take an umbrella with them everywhere even when it is sunny, and if we knew everyone’s personalities it would add more data to the model. However, the sensor Markov assumption ignores these data, assuming that only the hidden state affects the observation.</p>

<p>A hidden Markov model can be represented in a Markov chain with two layers. The top layer, variable X, stands for the hidden state. The bottom layer, variable E, stands for the evidence, the observations that we have.</p>

<p><img src="../Images/85559a1ad1374bfd81aa2f164ac0eb6e.png" alt="Hidden Markov Chain" data-original-src="https://cs50.harvard.edu/ai/notes/2/hiddenmarkovchain.png"/></p>

<p>Based on hidden Markov models, multiple tasks can be achieved:</p>

<ul>
  <li data-marker="*">Filtering: given observations from start until now, calculate the probability distribution for the current state. For example, given information on when people bring umbrellas form the start of time until today, we generate a probability distribution for whether it is raining today or not.</li>
  <li data-marker="*">Prediction: given observations from start until now, calculate the probability distribution for a future state.</li>
  <li data-marker="*">Smoothing: given observations from start until now, calculate the probability distribution for a past state. For example, calculating the probability of rain yesterday given that people brought umbrellas today.</li>
  <li data-marker="*">Most likely explanation: given observations from start until now, calculate most likely sequence of events.</li>
</ul>

<p>The most likely explanation task can be used in processes such as voice recognition, where, based on multiple waveforms, the AI infers the most likely sequence of words or syllables that brought to these waveforms. Next is a Python implementation of a hidden Markov model that we will use for a most likely explanation task:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pomegranate</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Observation model for each state
</span><span class="n">sun</span> <span class="o">=</span> <span class="nc">DiscreteDistribution</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">umbrella</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">no umbrella</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.8</span>
<span class="p">})</span>

<span class="n">rain</span> <span class="o">=</span> <span class="nc">DiscreteDistribution</span><span class="p">({</span>
    <span class="sh">"</span><span class="s">umbrella</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">no umbrella</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.1</span>
<span class="p">})</span>

<span class="n">states</span> <span class="o">=</span> <span class="p">[</span><span class="n">sun</span><span class="p">,</span> <span class="n">rain</span><span class="p">]</span>

<span class="c1"># Transition model
</span><span class="n">transitions</span> <span class="o">=</span> <span class="n">numpy</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="c1"># Tomorrow's predictions if today = sun
</span>     <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]]</span> <span class="c1"># Tomorrow's predictions if today = rain
</span><span class="p">)</span>

<span class="c1"># Starting probabilities
</span><span class="n">starts</span> <span class="o">=</span> <span class="n">numpy</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>

<span class="c1"># Create the model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">HiddenMarkovModel</span><span class="p">.</span><span class="nf">from_matrix</span><span class="p">(</span>
    <span class="n">transitions</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">starts</span><span class="p">,</span>
    <span class="n">state_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">sun</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">rain</span><span class="sh">"</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">bake</span><span class="p">()</span>
</code></pre></div></div>

<p>Note that our model has both the sensor model and the transition model. We need both for the hidden Markov model. In the following code snippet, we see a sequence of observations of whether people brought umbrellas to the building or not, and based on this sequence we will run the model, which will generate and print the most likely explanation (i.e. the weather sequence that most likely brought to this pattern of observations):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">model</span> <span class="kn">import</span> <span class="n">model</span>

<span class="c1"># Observed data
</span><span class="n">observations</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">umbrella</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">umbrella</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">no umbrella</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">umbrella</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">umbrella</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">umbrella</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">umbrella</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">no umbrella</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">no umbrella</span><span class="sh">"</span>
<span class="p">]</span>

<span class="c1"># Predict underlying states
</span><span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">observations</span><span class="p">)</span>
<span class="k">for</span> <span class="n">prediction</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">states</span><span class="p">[</span><span class="n">prediction</span><span class="p">].</span><span class="n">name</span><span class="p">)</span>
</code></pre></div></div>

<p>In this case, the output of the program will be rain, rain, sun, rain, rain, rain, rain, sun, sun. This output represents what is the most likely pattern of weather given our observations of people bringing or not bringing umbrellas to the building.</p>


                    
</body>
</html>