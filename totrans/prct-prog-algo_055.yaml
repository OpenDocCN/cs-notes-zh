- en: 9.6   Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://introcs.cs.princeton.edu/java/96optimization](https://introcs.cs.princeton.edu/java/96optimization)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This section under major construction.
  prefs: []
  type: TYPE_NORMAL
- en: Root finding.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Goal: given function f(x), find x* such that f(x*) = 0\. Nonlinear equations
    can have any number of solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Unconstrained optimization.** Goal: given function f(x), find x* such that
    f(x) is maximized or minimized. If f(x) is differentiable, then we are looking
    for an x* such that f''(x*) = 0\. However, this may lead to local minima, maxima,
    or saddle points.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bisection method.** Goal: given function f(x), find x* such that f(x*) =
    0\. Assume you know interval [a, b] such that f(a) < 0 and f(b) > 0.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Newton''s method.** Quadratic approximation. Fast convergence if close enough
    to answer. The update formulas below are for finding the root of f(x) and f''(x).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Newton''s method only reliable if started "close enough" to solution. Bad example
    (Smale): f(x) = x^3 - 2*x + 2\. If you start in the interval [-0.1, 0.1] , Newton''s
    method reaches a stable 2-cycle. If started to the left of the negative real root,
    it will converge.'
  prefs: []
  type: TYPE_NORMAL
- en: To handle general differentiable or twice differentiable functions of one variable,
    we might declare an interface
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Program [Newton.java](Newton.java.html) runs Newton's method on a differentiable
    function to compute points x* where f(x*) = 0 and f'(x*) = 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'The probability of finding an electron in the 4s excited state of hydrogen
    ar radius r is given by: *f(x) = (1 - 3x/4 + x²/8 - x³/192)² e^(-x/2)*, where
    *x* is the radius in units of the Bohr radius (0.529173E-8 cm). Program [BohrRadius.java](BohrRadius.java.html)
    contains the formula for f(x), f''(x), and f''''(x). By starting Newton''s method
    at 0, 4, 5, and 13, and 22, we obtain all three roots and all five local minima
    and maxima.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Newton''s method in higher dimensions.** [probably omit or leave as an exercise]
    Use to solve system of nonlinear equations. In general, there are no good methods
    for solving a nonlinear system of equations'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: where J is the Jacobian matrix of partial derivatives. In practice, we don't
    explicitly compute the inverse. Instead of computing y = J^(-1)f, we solve the
    linear system of equations Jy = f.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the method, suppose we want to find a solution (x, y) to the following
    system of two nonlinear equations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the Jacobian is given by
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If we start Newton's method at the point (-0.6, 0.6), we quickly obtain one
    of the roots (-1/2, sqrt(3)/2) up to machine accuracy. The other roots are (-1/2,
    -sqrt(3)/2) and (1, 0). Program [TestEquations.java](TestEquations.java.html)
    uses the interface [Equations.java](Equations.java) and [EquationSolver.java](EquationSolver.java.html)
    to solve the system of equations. We use the Jama matrix library to do the matrix
    computations.
  prefs: []
  type: TYPE_NORMAL
- en: '*Optimization.* Use same method to optimize a function of several variables.
    Good methods exist if multivariate function is sufficiently smooth.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Need gradient g(x) = ∇f(x) and Hessian H(x) = ∇²f(x). Method finds an x* where
    g(x*) = 0, but this could be a maxima, minima, or saddle point. If Hessian is
    positive definite (all eigenvalues are positive) then it is a minima; if all eigenvalues
    are negative, then it's a maxima; otherwise it's a saddle point.
  prefs: []
  type: TYPE_NORMAL
- en: Also, 2nd derivatives change slowly, so it may not be necessary to recalculate
    the Hessian (or its LU decomposition) at each step. In practice, it is expensive
    to compute the Hessian exactly, so other so called *quasi-Newton* methods are
    preferred, including the Broyden-Fletcher-Goldfarb-Shanno (BFGS) update rule.
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear programming.** Create matrix interface. Generalizes two-person zero-sum
    games, many problems in combinatorial optimization, .... [run AMPL from the web](http://www.ampl.com/TRYAMPL/startup.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Programming = planning. Give some history. Decision problem not known to be
    in P for along time. In 1979, Khachian resolved the question in the affirmative
    and made headlines in the New York Times with a geometric divide-and-conquer algorithm
    known as the *ellipsoid algorithm*. It requires O(N⁴L) bit operations where N
    is the number of variables and L is the number of bits in the input. Although
    this was a landmark in optimization, it did not immediately lead to a practical
    algorithm. In 1984, Karmarkar proposed a projective scaling algorithm that takes
    O(N^(3.5)L) time. It opened up the door for efficient implementations because
    by typically performing much better than its worst case guarantee. Various *interior
    point* methods were proposed in the 1990s, and the best known complexity bound
    is O(N³ L). More importantly, these algorithm are practical and competitive with
    the simplex method. They also extend to handle even more general problems.
  prefs: []
  type: TYPE_NORMAL
- en: '**Simplex method.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear programming solvers.** In 1947, George Dantzig proposed the simplex
    algorithm for linear programming. One of greatest and most successful algorithms
    of all time. [Linear programming](http://opsresearch.com/OR-Objects/api/drasys/or/mp/lp/package-summary.html),
    but not industrial strength. Program [LPDemo.java](LPDemo.java.html) illustrates
    how to use it. The classes `MPSReader` and `MPSWriter` can parse input files and
    write output files in the standard MPS format. Test LP data files in [MPS format](http://www.netlib.org/lp/data/).'
  prefs: []
  type: TYPE_NORMAL
- en: '**More applications.** OR-Objects also has graph coloring, traveling salesman
    problem, vehicle routing, shortest path.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Use Newton's method to find an x (in radians) such that x = cos(x).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use Newton's method to find an x (in radians) such that x² = 4 sin(x).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use Newton's method to find an x (in radians) that minimizes f(x) = sin(x) +
    x - exp(x).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use Newton's method to find (x, y) that solve
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Start at the point (0.1, -0.2), which is near the true root (0.09777, -2.325).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Use Newton's method to find (x, y) that solve
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Start at the point (1, 2), which is near the true root (0, 1).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Use Newton's method to find (x, y) that solve
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Start at the point (2, 7).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use Newton''s method to minimize f(x) = 1/2 - x e^(-x²). *Hint*: f''(x) = (2x²-1)e^(-x²),
    f''''(x) = 2x(3-2x²)e^(-x²).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use Newton's method to find all minima, maxima, and saddle points of the following
    function of two variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Creative Exercises
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Bernoulli numbers.** Bernoulli numbers appear in the Taylor expansion of
    the tangent function, the Euler-MacLaurin summation formula, and the Riemann zeta
    function. They can be defined recursively by B[0] = 1, and using the fact that
    the sum from j = 0 to N of binomial(N+1, j) B[j] = 0. For example B[1] = -1/2,
    B[2] = 1/6, B[3] = 0, and B[12] = -691/2730. Bernoulli computed the first 10 Bernoulli
    numbers by hand; Euler''s compute the first 30. In 1842, Ada Lovelace suggested
    to Charles Babbage that he devise an algorithm for computing Bernoulli numbers
    using his Analytic Engine. Write a program [Bernoulli.java](Bernoulli.java.html)
    that takes a command-line argument N and prints out the first N Bernoulli numbers.
    Use the [BigRational.java](../92symbolic/BigRational.java.html) data type from
    Section 9.2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Eccentricity anomaly.** (Cleve Moler) The *eccentricity anomaly* arises in
    Kepler''s model of planetary motion and satisfies M = E - e sin E, where M is
    the mean anomaly (24.851090) and e is the orbit eccentricity (0.1). Solve for
    E.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Newton''s method with complex numbers.** Implement Newton''s method for finding
    a *complex* root of an equation. Use Complex.java and implement Newton''s method
    exactly as when finding real roots, but use complex numbers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
