- en: Lecture 5
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讲座 5
- en: 原文：[https://cs50.harvard.edu/ai/notes/5/](https://cs50.harvard.edu/ai/notes/5/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://cs50.harvard.edu/ai/notes/5/](https://cs50.harvard.edu/ai/notes/5/)
- en: Neural Networks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: AI neural networks are inspired by neuroscience. In the brain, neurons are cells
    that are connected to each other, forming networks. Each neuron is capable of
    both receiving and sending electrical signals. Once the electrical input that
    a neuron receives crosses some threshold, the neuron activates, thus sending its
    electrical signal forward.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能神经网络受到神经科学的启发。在大脑中，神经元是相互连接的细胞，形成网络。每个神经元都能够接收和发送电信号。一旦一个神经元接收到的电输入超过某个阈值，该神经元就会被激活，从而发送其电信号。
- en: An **Artificial Neural Network** is a mathematical model for learning inspired
    by biological neural networks. Artificial neural networks model mathematical functions
    that map inputs to outputs based on the structure and parameters of the network.
    In artificial neural networks, the structure of the network is shaped through
    training on data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**是一种受生物神经网络启发的学习数学模型。人工神经网络通过网络的结构和参数来模拟将输入映射到输出的数学函数。在人工神经网络中，网络的结构是通过在数据上训练来塑造的。'
- en: 'When implemented in AI, the parallel of each neuron is a **unit** that’s connected
    to other units. For example, like in the last lecture, the AI might map two inputs,
    x₁ and x₂, to whether it is going to rain today or not. Last lecture, we suggested
    the following form for this hypothesis function: *h(x₁, x₂)* = *w₀ + w₁x₁ + w₂x₂*,
    where *w₁* and *w₂* are weights that modify the inputs, and *w₀* is a constant,
    also called **bias**, modifying the value of the whole expression.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当在人工智能中实现时，每个神经元的并行单元是连接到其他单元的**单元**。例如，就像在上一次讲座中提到的，人工智能可能会将两个输入 x₁ 和 x₂ 映射到今天是否会下雨。在上一次讲座中，我们提出了以下假设函数的形式：*h(x₁,
    x₂)* = *w₀ + w₁x₁ + w₂x₂*，其中 *w₁* 和 *w₂* 是修改输入的权重，*w₀* 是一个常数，也称为**偏差**，用于修改整个表达式的值。
- en: Activation Functions
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: To use the hypothesis function to decide whether it rains or not, we need to
    create some sort of threshold based on the value it produces.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用假设函数来决定是否下雨，我们需要根据其产生的值创建某种类型的阈值。
- en: One way to do this is with a step function, which gives 0 before a certain threshold
    is reached and 1 after the threshold is reached.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一点的其中一种方式是使用阶跃函数，它在达到某个阈值之前输出0，在达到阈值之后输出1。
- en: '![Step Function](../Images/c4b3745d5cdff65db794ac46e7b3c044.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![阶跃函数](../Images/c4b3745d5cdff65db794ac46e7b3c044.png)'
- en: Another way to go about this is with a logistic function, which gives as output
    any real number from 0 to 1, thus expressing graded confidence in its judgment.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用对数函数，它输出从0到1的任何实数，从而表达其判断的分级信心。
- en: '![Logistic Function](../Images/874b589eab56a2ce8516ed087b03f9dc.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![对数函数](../Images/874b589eab56a2ce8516ed087b03f9dc.png)'
- en: Another possible function is Rectified Linear Unit (ReLU), which allows the
    output to be any positive value. If the value is negative, ReLU sets it to 0.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可能的函数是修正线性单元（ReLU），它允许输出为任何正数值。如果值为负，ReLU将其设置为0。
- en: '![Rectified Linear Unit](../Images/aa4a5bb0469f2081f5eb83278e69c85b.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![修正线性单元](../Images/aa4a5bb0469f2081f5eb83278e69c85b.png)'
- en: Whichever function we choose to use, we learned last lecture that the inputs
    are modified by weights in addition to the bias, and the sum of those is passed
    to an activation function. This stays true for simple neural networks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们选择使用哪个函数，我们在上一次讲座中学到的是，输入除了偏差外还会通过权重进行修改，这些修改的总和传递给激活函数。这对于简单的神经网络来说也是成立的。
- en: Neural Network Structure
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络结构
- en: A neural network can be thought of as a representation of the idea above, where
    a function sums up inputs to produce an output.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将神经网络视为上述想法的一种表示，其中函数将输入求和以产生输出。
- en: '![Neural Network Structure](../Images/16bce6ff171b2324d28284130978de98.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络结构](../Images/16bce6ff171b2324d28284130978de98.png)'
- en: The two white units on the left are the input and the unit on the right is an
    output. The inputs are connected to the output by a weighted edge. To make a decision,
    the output unit multiplies the inputs by their weights in addition to the bias
    (*w₀*), and the uses function *g* to determine the output.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的两个白色单元是输入单元，右侧的单元是输出单元。输入单元通过加权边连接到输出单元。为了做出决定，输出单元将输入乘以其权重（除了偏差 *w₀*）并使用函数
    *g* 来确定输出。
- en: 'For example, an Or logical connective can be represented as a function *f*
    with the following truth table:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个或逻辑连接可以表示为一个具有以下真值表的函数 *f*：
- en: '| *x* | *y* | *f(x, y)* |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| *x* | *y* | *f(x, y)* |'
- en: '| --- | --- | --- |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 0 | 0 | 0 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 |'
- en: '| 0 | 1 | 1 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 1 |'
- en: '| 1 | 0 | 1 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 1 |'
- en: '| 1 | 1 | 1 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 1 |'
- en: We can visualize this function as a neural network. *x₁* is one input unit,
    and *x₂* is another input unit. They are connected to the output unit by an edge
    with a weight of 1\. The output unit then uses function *g(-1 + 1x₁ + 2x₂)* with
    a threshold of 0 to output either 0 or 1 (false or true).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个函数可视化为一个神经网络。*x₁* 是一个输入单元，*x₂* 是另一个输入单元。它们通过一个权重为 1 的边连接到输出单元。输出单元然后使用函数
    *g(-1 + 1x₁ + 2x₂)* 并以 0 为阈值来输出 0 或 1（假或真）。
- en: '![Neural Network of Or Function](../Images/a19d7a136529d8621595afa102ef2413.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![或函数的神经网络](../Images/a19d7a136529d8621595afa102ef2413.png)'
- en: For example, in the case where *x₁* = *x₂* = 0, the sum is (-1). This is below
    the threshold, so the function *g* will output 0\. However, if either or both
    of *x₁* or *x₂* are equal to 1, then the sum of all inputs will be either 0 or
    1\. Both are at or above the threshold, so the function will output 1.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在 *x₁* = *x₂* = 0 的情况下，总和是 (-1)。这低于阈值，所以函数 *g* 将输出 0。然而，如果 *x₁* 或 *x₂* 中的任何一个或两个等于
    1，那么所有输入的总和将是 0 或 1。两者都在或高于阈值，所以函数将输出 1。
- en: A similar process can be repeated with the And function (where the bias will
    be (-2)). Moreover, inputs and outputs don’t have to be distinct. A similar process
    can be used to take humidity and air pressure as input, and produce the probability
    of rain as output. Or, in a different example, inputs can be money spent on advertising
    and the month when it was spent to get the output of expected revenue from sales.
    This can be extended to any number of inputs by multiplying each input *x₁ … xₙ*
    by weight *w₁ … wₙ*, summing up the resulting values and adding a bias *w₀*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 可以用类似的过程重复使用与函数（其中偏差将是(-2)）。此外，输入和输出不必是不同的。可以使用类似的过程将湿度和气压作为输入，并输出降雨的概率。或者，在另一个例子中，输入可以是广告支出和支出的月份，以获得销售预期收入的输出。这可以通过将每个输入
    *x₁ … xₙ* 乘以权重 *w₁ … wₙ*，求和得到的值，并添加偏差 *w₀* 来扩展到任意数量的输入。
- en: Gradient Descent
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降
- en: 'Gradient descent is an algorithm for minimizing loss when training neural networks.
    As was mentioned earlier, a neural network is capable of inferring knowledge about
    the structure of the network itself from the data. Whereas, so far, we defined
    the different weights, neural networks allow us to compute these weights based
    on the training data. To do this, we use the gradient descent algorithm, which
    works the following way:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是一种在训练神经网络时最小化损失的计算算法。正如之前提到的，神经网络能够从数据中推断出关于自身结构的知识。而到目前为止，我们定义了不同的权重，神经网络允许我们根据训练数据来计算这些权重。为此，我们使用梯度下降算法，其工作原理如下：
- en: Start with a random choice of weights. This is our naive starting place, where
    we don’t know how much we should weight each input.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从一个随机的权重选择开始。这是我们天真的起点，我们不知道应该给每个输入多少权重。
- en: 'Repeat:'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复：
- en: Calculate the gradient based on all data points that will lead to decreasing
    loss. Ultimately, the gradient is a vector (a sequence of numbers).
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据所有会导致损失减少的数据点计算梯度。最终，梯度是一个向量（一系列数字）。
- en: Update weights according to the gradient.
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据梯度更新权重。
- en: The problem with this kind of algorithm is that it requires to calculate the
    gradient based on *all data points*, which is computationally costly. There are
    a multiple ways to minimize this cost. For example, in **Stochastic Gradient Descent**,
    the gradient is calculated based on one point chosen at random. This kind of gradient
    can be quite inaccurate, leading to the **Mini-Batch Gradient Descent** algorithm,
    which computes the gradient based on on a few points selected at random, thus
    finding a compromise between computation cost and accuracy. As often is the case,
    none of these solutions is perfect, and different solutions might be employed
    in different situations.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这种算法的问题在于它需要根据 *所有数据点* 计算梯度，这在计算上代价高昂。有多种方法可以最小化这种成本。例如，在 **随机梯度下降** 中，梯度是基于随机选择的一个点计算的。这种梯度可能相当不准确，导致
    **小批量梯度下降** 算法，它基于随机选择的几个点计算梯度，从而在计算成本和准确性之间找到一个折衷。正如通常情况下，没有哪种解决方案是完美的，不同的解决方案可能在不同的情境中被采用。
- en: Using gradient descent, it is possible to find answers to many problems. For
    example, we might want to know more than “will it rain today?” We can use some
    inputs to generate probabilities for different kinds of weather, and then just
    choose the weather that is most probable.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度下降，可以找到许多问题的答案。例如，我们可能想知道的不仅仅是“今天会下雨吗？”我们可以使用一些输入来生成不同天气类型的概率，然后只需选择最可能的天气。
- en: '![Neural Network for Weather](../Images/23e36a3342f04f9e0e3add496ec24067.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![天气神经网络](../Images/23e36a3342f04f9e0e3add496ec24067.png)'
- en: This can be done with any number of inputs and outputs, where each input is
    connected to each output, and where the outputs represent decisions that we can
    make. Note that in this kind of neural networks the outputs are not connected.
    This means that each output and its associated weights from all the inputs can
    be be seen as an individual neural network and thus can be trained separately
    from the rest of the outputs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以用于任意数量的输入和输出，其中每个输入都连接到每个输出，并且输出代表我们可以做出的决策。请注意，在这种类型的神经网络中，输出之间没有连接。这意味着每个输出及其从所有输入关联的权重可以被视为一个独立的神经网络，因此可以单独从其他输出中训练。
- en: So far, our neural networks relied on **perceptron** output units. These are
    units that are only capable of learning a linear decision boundary, using a straight
    line to separate data. That is, based on a linear equation, the perceptron could
    classify an input to be one type or another (e.g. left picture). However, often,
    data are not linearly separable (e.g. right picture). In this case, we turn to
    multilayer neural networks to model data non-linearly.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的神经网络依赖于**感知器**输出单元。这些单元只能学习线性决策边界，使用直线来分离数据。也就是说，基于线性方程，感知器可以将输入分类为一种类型或另一种类型（例如，左图）。然而，数据往往不是线性可分的（例如，右图）。在这种情况下，我们转向多层神经网络来非线性地建模数据。
- en: '![Linear and Non-Linear Models](../Images/d3e42ae63333f0c32ab4c595c266d3b6.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![线性和非线性模型](../Images/d3e42ae63333f0c32ab4c595c266d3b6.png)'
- en: Multilayer Neural Networks
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多层神经网络
- en: A multilayer neural network is an artificial neural network with an input layer,
    an output layer, and at least one **hidden** layer. While we provide inputs and
    outputs to train the model, we, the humans, don’t provide any values to the units
    inside the hidden layers. Each unit in the first hidden layer receives a weighted
    value from each of the units in the input layer, performs some action on it and
    outputs a value. Each of these values is weighted and further propagated to the
    next layer, repeating the process until the output layer is reached. Through hidden
    layers, it is possible to model non-linear data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 多层神经网络是一种具有输入层、输出层和至少一个**隐藏**层的人工神经网络。虽然我们提供输入和输出以训练模型，但我们人类不向隐藏层中的单元提供任何值。第一隐藏层中的每个单元从输入层中的每个单元接收加权值，对其进行一些操作并输出一个值。这些值被加权并进一步传播到下一层，重复此过程直到达到输出层。通过隐藏层，可以建模非线性数据。
- en: '![Multilayer Neural Network](../Images/a43df3c1288656d5abdbc31022728d42.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![多层神经网络](../Images/a43df3c1288656d5abdbc31022728d42.png)'
- en: Backpropagation
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播
- en: 'Backpropagation is the main algorithm used for training neural networks with
    hidden layers. It does so by starting with the errors in the output units, calculating
    the gradient descent for the weights of the previous layer, and repeating the
    process until the input layer is reached. In pseudocode, we can describe the algorithm
    as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是用于训练具有隐藏层的神经网络的主要算法。它通过从输出单元的误差开始，计算前一层权重的梯度下降，并重复此过程直到达到输入层来实现。在伪代码中，我们可以将算法描述如下：
- en: Calculate error for output layer
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算输出层的误差
- en: 'For each layer, starting with output layer and moving inwards towards earliest
    hidden layer:'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每一层，从输出层开始，向内移动到最早的隐藏层：
- en: Propagate error back one layer. In other words, the current layer that’s being
    considered sends the errors to the preceding layer.
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将误差反向传播一层。换句话说，当前正在考虑的层将误差发送到前一层。
- en: Update weights.
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新权重。
- en: This can be extended to any number of hidden layers, creating **deep neural
    networks**, which are neural networks that have more than one hidden layer.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以扩展到任意数量的隐藏层，创建**深度神经网络**，这些神经网络具有多个隐藏层。
- en: '![Deep Neural Network](../Images/98dd03477f48875575ba3a527f3b9fbb.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![深度神经网络](../Images/98dd03477f48875575ba3a527f3b9fbb.png)'
- en: Overfitting
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过度拟合
- en: 'Overfitting is the danger of modeling the training data too closely, thus failing
    to generalize to new data. One way to combat overfitting is by **dropout**. In
    this technique, we temporarily remove units that we select at random during the
    learning phase. This way, we try to prevent over-reliance on any one unit in the
    network. Throughout training, the neural network will assume different forms,
    each time dropping some other units and then using them again:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是指对训练数据建模过于紧密，因此无法推广到新数据的风险。对抗过拟合的一种方法是通过**dropout**。在这种技术中，我们在学习阶段随机选择并暂时移除一些单元。这样，我们试图防止网络对任何单个单元过度依赖。在整个训练过程中，神经网络将采取不同的形式，每次丢弃一些单元然后再使用它们：
- en: '![Dropout](../Images/36bffa36a5af73f7f573efd99830536f.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![Dropout](../Images/36bffa36a5af73f7f573efd99830536f.png)'
- en: Note that after the training is finished, the whole neural network will be used
    again.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，训练完成后，整个神经网络将再次使用。
- en: TensorFlow
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow
- en: 'Like often is the case in python, multiple libraries already have an implementation
    for neural networks using the backpropagation algorithm, and TensorFlow is one
    such library. You are welcome to experiment with TensorFlow neural networks in
    this [web application](http://playground.tensorflow.org/), which lets you define
    different properties of the network and run it, visualizing the output. We will
    now turn to an example of how we can use TensorFlow to perform the task we discussed
    last lecture: distinguishing counterfeit notes from genuine notes.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在 Python 中经常发生的那样，多个库已经实现了使用反向传播算法的神经网络，TensorFlow 就是这样的库之一。您可以在这个 [web 应用程序](http://playground.tensorflow.org/)
    中尝试 TensorFlow 神经网络，它允许您定义网络的不同属性并运行它，可视化输出。现在，我们将转向一个例子，说明我们如何使用 TensorFlow 来执行上次讲座中讨论的任务：区分假币和真币。
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We import TensorFlow and call it tf (to make the code shorter).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入 TensorFlow 并将其命名为 tf（以缩短代码）。
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We provide the CSV data to the model. Our work is often required in making the
    data fit the format that the library requires. The difficult part of actually
    coding the model is already implemented for us.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 CSV 数据提供给模型。我们的工作通常需要使数据符合库所需的格式。实际上编码模型的困难部分已经为我们实现了。
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Keras is an api that different machine learning algorithms access. A sequential
    model is one where layers follow each other (like the ones we have seen so far).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 是一个 API，不同的机器学习算法可以通过它访问。一个顺序模型是指层依次排列（就像我们之前看到的那样）。
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: A dense layer is one where each node in the current layer is connected to all
    the nodes from the previous layer. In generating our hidden layers we create 8
    dense layers, each having 4 input neurons, using the ReLU activation function
    mentioned above.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 密集层是指当前层中的每个节点都连接到前一层的所有节点。在生成我们的隐藏层时，我们创建了 8 个密集层，每个层有 4 个输入神经元，使用上面提到的 ReLU
    激活函数。
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In our output layer, we want to create one dense layer that uses a sigmoid activation
    function, an activation function where the output is a value between 0 and 1.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的输出层，我们希望创建一个使用 sigmoid 激活函数的密集层，这种激活函数的输出值介于 0 和 1 之间。
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Finally, we compile the model, specifying which algorithm should optimize it,
    what type of loss function we use, and how we want to measure its success (in
    our case, we are interested in the accuracy of the output). Finally, we fit the
    model on the training data with 20 repetitions (epochs), and then evaluate it
    on the testing data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们编译模型，指定哪个算法应该优化它，我们使用哪种类型的损失函数，以及我们如何衡量其成功（在我们的情况下，我们关注输出的准确性）。最后，我们使用
    20 次重复（周期）将模型拟合到训练数据，然后在测试数据上评估它。
- en: Computer Vision
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: Computer vision encompasses the different computational methods for analyzing
    and understanding digital images, and it is often achieved using neural networks.
    For example, computer vision is used when social media employs face recognition
    to automatically tag people in pictures. Other examples are handwriting recognition
    and self-driving cars.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉包括分析和理解数字图像的不同计算方法，通常使用神经网络实现。例如，当社交媒体使用面部识别自动标记图片中的人时，就会用到计算机视觉。其他例子包括手写识别和自动驾驶汽车。
- en: Images consist of pixels, and pixels are represented by three values that range
    from 0 to 255, one for red, one for green and one for blue. These values are often
    referred to with the acronym RGB. We can use this to create a neural network where
    each color value in each pixel is an input, where we have some hidden layers,
    and the output is some number of units that tell us what it is that was shown
    in the image. However, there are a few drawbacks to this approach. First, by breaking
    down the image into pixels and the values of their colors, we can’t use the structure
    of the image as an aid. That is, as humans, if we see a part of a face we know
    to expect to see the rest of the face, and this quickens computation. We want
    to be able to use a similar advantage in our neural networks. Second, the sheer
    number of inputs is very big, which means that we will have to calculate a lot
    of weights.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由像素组成，像素由三个范围从0到255的值表示，一个用于红色，一个用于绿色，一个用于蓝色。这些值通常用缩写RGB来表示。我们可以使用这一点来创建一个神经网络，其中每个像素中的颜色值都是一个输入，我们有一些隐藏层，输出是一些单位数，告诉我们图像中展示了什么。然而，这种方法有几个缺点。首先，通过将图像分解成像素及其颜色值，我们无法使用图像的结构作为辅助。也就是说，作为人类，如果我们看到脸部的一部分，我们知道应该期待看到脸的其余部分，这可以加快计算。我们希望能够在我们的神经网络中利用类似的优势。其次，输入的数量非常大，这意味着我们不得不计算很多权重。
- en: Image Convolution
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像卷积
- en: Image convolution is applying a filter that adds each pixel value of an image
    to its neighbors, weighted according to a kernel matrix. Doing so alters the image
    and can help the neural network process it.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图像卷积是将一个滤波器应用于图像的每个像素值，将其与邻居的像素值相加，并根据内核矩阵进行加权。这样做会改变图像，并有助于神经网络处理它。
- en: 'Let’s consider the following example:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下示例：
- en: '![Image Convolution](../Images/c799a904fb6834d95ca45cc3e68ae6e1.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图像卷积](../Images/c799a904fb6834d95ca45cc3e68ae6e1.png)'
- en: The kernel is the blue matrix, and the image is the big matrix on the left.
    The resulting filtered image is the small matrix on the bottom right. To filter
    the image with the kernel, we start with the pixel with value 20 in the top-left
    of the image (coordinates 1,1). Then, we will multiply all the values around it
    by the corresponding value in the kernel and sum them up (10*0 + 20*(-1) + 30*0
    + 10*(-1) + 20*5 + 30*(-1) + 20*0 + 30*(-1) + 40*0), producing the value 10\.
    Then we will do the same for the pixel on the right (30), the pixel below the
    first one (30), and the pixel to the right of this one (40). This produces a filtered
    image with the values we see on the bottom right.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 内核是蓝色的矩阵，图像是左侧的大矩阵。生成的过滤图像是右下角的小矩阵。要使用内核过滤图像，我们从图像左上角的值为20的像素（坐标1,1）开始。然后，我们将它周围的所有值乘以内核中的相应值并将它们相加（10*0
    + 20*(-1) + 30*0 + 10*(-1) + 20*5 + 30*(-1) + 20*0 + 30*(-1) + 40*0），得到值10。然后我们将对右侧的像素（30）、第一个像素下面的像素（30）以及这个像素右侧的像素（40）做同样的处理。这产生了一个具有我们在右下角看到的值的过滤图像。
- en: 'Different kernels can achieve different tasks. For edge detection, the following
    kernel is often used:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的内核可以完成不同的任务。对于边缘检测，以下内核经常被使用：
- en: '![Edge Detection Kernel](../Images/e7097bdb3ba65f5fbc91c1e133cd254c.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![边缘检测内核](../Images/e7097bdb3ba65f5fbc91c1e133cd254c.png)'
- en: 'The idea here is that when the pixel is similar to all its neighbors, they
    should cancel each other, giving a value of 0\. Therefore, the more similar the
    pixels, the darker the part of the image, and the more different they are the
    lighter it is. Applying this kernel to an image (left) results in an image with
    pronounced edges (right):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是，当像素与其所有邻居相似时，它们应该相互抵消，得到值为0。因此，像素越相似，图像的部分就越暗，它们越不同，就越亮。将此内核应用于图像（左侧）会产生具有明显边缘的图像（右侧）：
- en: '![Edge Detection](../Images/cab526550e684be351bb65469f561c07.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![边缘检测](../Images/cab526550e684be351bb65469f561c07.png)'
- en: Let’s consider an implementation of image convolution. We are using the PIL
    library (stands for Python Imaging Library) that can do most of the hard work
    for us.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑图像卷积的一个实现。我们使用的是PIL库（代表Python Imaging Library），它可以为我们完成大部分繁重的工作。
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Still, processing the image in a neural network is computationally expensive
    due to the number of pixels that serve as input to the neural network. Another
    way to go about this is **Pooling**, where the size of the input is reduced by
    sampling from regions in the input. Pixels that are next to each other belong
    to the same area in the image, which means that they are likely to be similar.
    Therefore, we can take one pixel to represent a whole area. One way of doing this
    is with **Max-Pooling**, where the selected pixel is the one with the highest
    value of all others in the same region. For example, if we divide the left square
    (below) into four 2X2 squares, by max-pooling from this input, we get the small
    square on the right.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，由于作为神经网络输入的像素数量众多，处理图像在神经网络中是计算密集型的。另一种方法是**池化**，通过从输入区域中采样来减少输入的尺寸。相邻的像素属于图像中的同一区域，这意味着它们很可能是相似的。因此，我们可以用一个像素来代表整个区域。一种实现方式是**最大池化**，其中选定的像素是该区域内所有其他像素中值最高的一个。例如，如果我们把下面的左方形（下方）分成四个2X2的小方形，通过从这个输入进行最大池化，我们得到右边的那个小方形。
- en: '![Max-Pooling](../Images/649b82e50bc6c5d591a18013af046bc2.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![最大池化](../Images/649b82e50bc6c5d591a18013af046bc2.png)'
- en: Convolutional Neural Networks
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: A convolutional neural network is a neural network that uses convolution, usually
    for analyzing images. It starts by applying filters that can help distill some
    features of the image using different kernels. These filters can be improved in
    the same way as other weights in the neural network, by adjusting their kernels
    based on the error of the output. Then, the resulting images are pooled, after
    which the pixels are fed to a traditional neural network as inputs (a process
    called **flattening**).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络是一种使用卷积的神经网络，通常用于分析图像。它首先应用过滤器，使用不同的核来帮助提取图像的一些特征。这些过滤器可以通过调整它们的核来改进，就像神经网络中的其他权重一样，基于输出的错误进行调整。然后，得到的图像被池化，之后像素被作为输入（称为**展平**）馈送到传统的神经网络。
- en: '![Convolutional Neural Network](../Images/57697e5d51a0467e73dc0518257e4c3d.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![卷积神经网络](../Images/57697e5d51a0467e73dc0518257e4c3d.png)'
- en: The convolution and pooling steps can be repeated multiple times to extract
    additional features and reduce the size of the input to the neural network. One
    of the benefits of these processes is that, by convoluting and pooling, the neural
    network becomes less sensitive to variation. That is, if the same picture is taken
    from slightly different angles, the input for convolutional neural network will
    be similar, whereas, without convolution and pooling, the input from each image
    would be vastly different.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积和池化步骤可以重复多次，以提取额外的特征并减少输入到神经网络的尺寸。这些过程的一个好处是，通过卷积和池化，神经网络对变化的敏感性降低。也就是说，如果从略微不同的角度拍摄相同的图片，卷积神经网络的输入将相似，而如果没有卷积和池化，每张图片的输入将大相径庭。
- en: In code, a convolutional neural network doesn’t differ by much from a traditional
    neural network. TensorFlow offers datasets to test our models on. We will be using
    MNIST, which contains pictures of black and white handwritten digits. We will
    train our convolutional neural network to recognize digits.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，卷积神经网络与传统神经网络差别不大。TensorFlow提供了测试我们模型的数据库。我们将使用MNIST，它包含黑白手写数字的图片。我们将训练我们的卷积神经网络来识别数字。
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Since the model takes time to train, we can save the already trained model to
    use it later.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型需要时间来训练，我们可以保存已经训练好的模型以供以后使用。
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, if we run a program that receives hand-drawn digits as input, it will be
    able to classify and output the digit using the model. For an implementation of
    such a program, refer to recognition.py in the source code for this lecture.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们运行一个接收手绘数字作为输入的程序，它将能够使用该模型对数字进行分类并输出结果。有关此类程序的实现，请参阅本讲座源代码中的recognition.py。
- en: Recurrent Neural Networks
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: '**Feed-Forward Neural Networks** are the type of neural networks that we have
    discussed so far, where input data is provided to the network, which eventually
    produces some output. A diagram of how feed-forward neural networks work can be
    seen below.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**前馈神经网络**是我们迄今为止讨论过的神经网络类型，其中输入数据被提供给网络，最终产生一些输出。下面可以看到前馈神经网络的工作原理图。'
- en: '![Feed-Forward Neural Networks Diagram](../Images/fc8205f851ee0468083abeec490cd916.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![前馈神经网络图](../Images/fc8205f851ee0468083abeec490cd916.png)'
- en: As opposed to that, **Recurrent Neural Networks** consist of a non-linear structure,
    where the network uses its own output as input. For example, Microsoft’s [captionbot](https://www.captionbot.ai)
    is capable of describing the content of an image with words in a sentence. This
    is different from classification in that the output can be of varying length based
    on the properties of the image. While feed-forward neural networks are incapable
    of varying the number of outputs, recurrent neural networks are capable to do
    that due to their structure. In the captioning task, a network would process the
    input to produce an output, and then continue processing from that point on, producing
    another output, and repeating as much as necessary.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相反，**循环神经网络**由一个非线性结构组成，其中网络使用其自身的输出作为输入。例如，微软的[captionbot](https://www.captionbot.ai)能够用句子中的词语描述图像的内容。这与分类不同，因为输出可以根据图像的特性具有不同的长度。虽然前馈神经网络无法改变输出的数量，但循环神经网络由于其结构，能够做到这一点。在字幕任务中，网络会处理输入以产生输出，然后从这个点继续处理，产生另一个输出，并重复必要的次数。
- en: '![Recurrent Neural Network](../Images/80dca3fe386751323896b69737ad9cb0.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![循环神经网络](../Images/80dca3fe386751323896b69737ad9cb0.png)'
- en: Recurrent neural networks are helpful in cases where the network deals with
    sequences and not a single individual object. Above, the neural network needed
    to produce a sequence of words. However, the same principle can be applied to
    analyzing video files, which consist of a sequence of images, or in translation
    tasks, where a sequence of inputs (words in the source language) is processed
    to produce a sequence of outputs (words in the target language).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络在处理序列而不是单个对象的情况下非常有用。上面提到的神经网络需要生成一系列词语。然而，同样的原理也可以应用于分析视频文件，这些文件由一系列图像组成，或者在翻译任务中，处理一系列输入（源语言中的词语）以产生一系列输出（目标语言中的词语）。
