- en: 5.5   Data Compression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://algs4.cs.princeton.edu/55compression](https://algs4.cs.princeton.edu/55compression)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This section under major construction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data compression: reduces the size of a file to save *space* when storing it
    and to save *time* when transmitting it. Moore''s law: # transistor on a chip
    doubles every 18-24 months. Parkinson''s law: data expands to fill available space.
    Text, images, sound, video, etc. Wikipedia provides [public dumps](http://meta.wikimedia.org/wiki/Data_dumps)
    of all content for academic research and republishing. Uses bzip and SevenZip''s
    LZMA. Can take a week to compress of 300GB of data.'
  prefs: []
  type: TYPE_NORMAL
- en: Ancient ideas.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Morse code, decimal number system, natural language, rotary phones (lower numbers
    were quicker to dial, so New York was 212 and Chicago 312).
  prefs: []
  type: TYPE_NORMAL
- en: Binary input and output streams.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We use [BinaryStdIn.java](BinaryStdIn.java.html), [BinaryStdOut.java](BinaryStdOut.java.html),
    [BinaryDump.java](BinaryDump.java.html), [HexDump.java](HexDump.java.html), and
    [PictureDump.java](PictureDump.java.html).
  prefs: []
  type: TYPE_NORMAL
- en: Fixed length codes.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Need ceil(lg R) bits to specify one of R symbols. [Genome.java](Genome.java.html).
    Uses [Alphabet.java](Alphabet.java.html).
  prefs: []
  type: TYPE_NORMAL
- en: Run length encoding.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[RunLength.java](RunLength.java.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Variable-length codes.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Desire unique decodable codes. One way to achieve this is to append a special
    stop symbol to each codeword. Better approach: prefix-free codes: no string is
    a prefix of another. For example, { 01, 10, 0010, 1111 } is prefix free, but {
    01, 10, 0010, 1010 } is not because 10 is a prefix of 1010.'
  prefs: []
  type: TYPE_NORMAL
- en: Give fax machine example.
  prefs: []
  type: TYPE_NORMAL
- en: Huffman codes.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Specific way to construct optimal prefix-free codes. Invented by David Huffman
    while a student at MIT in 1950. [Huffman.java](Huffman.java.html) implements Huffman
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '**Property A.** No prefix free code uses fewer bits.'
  prefs: []
  type: TYPE_NORMAL
- en: LZW compression.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using prefix match code from [TST.java](TST.java.html), [LZW.java](LZW.java.html)
    implements LZW compression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Real world: Pkzip = LZW + Shannon-Fano, GIF, TIFF, V.42bis modem, Unix compress.
    Practical issues:'
  prefs: []
  type: TYPE_NORMAL
- en: Encode everything in binary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limit the number of elements in the symbol table (GIF = throw away and start
    over, Unix compress = throw away when not effective).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initially dictionary has 512 elements (with 256 elements filled in for ASCII
    characters), so we transmit 9 bits per integer. When it fills up, we double it
    to 1024 and start transmitting 10 bits per integer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only traverse the tree once (might break our string table abstraction).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Practical issues: limit the number of elements in the symbol table.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Huffman: variable length code for fixed length symbols. LZW: fixed length code
    for variable length strings.'
  prefs: []
  type: TYPE_NORMAL
- en: Universal compression algorithm.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Impossible to compress all files (proof by simple counting argument). Intuitive
    argument: compress life work of Shakespeare, then compress result, then compress
    result again. If each file strictly shrinks, eventually you will be left with
    one bit.'
  prefs: []
  type: TYPE_NORMAL
- en: References.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Guy Blelloch of CMU has an excellent chapter on [data compression](http://www.cs.cmu.edu/afs/cs/project/pscico-guyb/realworld/www/compression.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Error correction / detection.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose channel for sending information is noisy and each bit gets flipped with
    probability p. Send each bit 3 times; to decode take the majority of the 3 bits.
    Decoded bit is correct with probability 3p^2 - 2p^3\. This is less than p (if
    p < 1/2). Can reduce probability of decoding the bit incorrectly by sending each
    bit k times, but this is wasteful in terms of the transmission rate.
  prefs: []
  type: TYPE_NORMAL
- en: Reed-Solomon codes.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Reference](http://www.cs.cornell.edu/Courses/cs722/2000sp/ReedSolomon.pdf).
    Used in mass storage systems (CDs and DVDs) and satellite transmissions (Voyager
    space probe, Mars Pathfinder) when the errors are bursty. Think of data to send
    as a degree d polynomial. Only need d+1 points to uniquely specify the polynomial.
    Send more points to enable error correction / detection. If code we want to send
    is a0, a1, ..., am-1 (each elements over finite field K), think of it as the polynomial
    p(x) = a0 + a1x + ... + am-1 x^m-1. Send p(0), p(b), p(b^2), ..., where b is a
    generator of multiplicative cyclic group over K.'
  prefs: []
  type: TYPE_NORMAL
- en: Shannon's coding theorem.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Roughly speaking, if channel capacity is C, then we can send bits at a rate
    slightly less than C with an encoding scheme that will reduce probability of a
    decoding error to any desired level. Proof is nonconstructive.
  prefs: []
  type: TYPE_NORMAL
- en: Q+A
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Which of the following codes are prefix free? Uniquely decodable? For those
    that are uniquely decodable, give the encoding of 1000000000000.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Given an example of a uniquely-decodable code that is not prefix free.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Solution.* Any suffix-free code is uniquely decodable, e.g., { 0, 01 }.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Given an example of a uniquely-decodable code that is not prefix free or suffix
    free.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Solution.* { 0011, 011, 11, 1110 } or { 01, 10, 011, 110 }.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Are { 1, 100000, 00 }, { 01, 1001, 1011, 111, 1110 }, and { 1, 011, 01110, 1110,
    10011 } uniquely decodable? If not, find a string with two encodings. *Solution.*
    The first set of codewords is uniquely decodable. The second set of codewords
    is not uniquely decodable because 111-01-1110-01 and 1110-111-1001 are two decodings
    of 11101111001. The third set of codewords ins not uniquely decodable because
    01110-1110-011 and 011-1-011-10011 are two decodings of 011101110011.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Test for uniquely decodability.** Implement the Sardinas-Patterson algorithm
    for testing whether a set of codewords is uniquely decodable: Add all of the codewords
    to a set. Examine all pairs of codewords to see if any one is a prefix of another;
    if so, extract the *dangling suffix* (i.e., the part of the longer string that
    is not a prefix of the shorter one). If the dangling suffix is a codeword, then
    the code is not uniquely decodable; otherwise, add the dangling suffix to the
    list (provided it is not already there). Repeat this process with the larger list
    until there are no remaining new dangling suffix.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The algorithm is finite because all dangling suffixes added to the list are
    suffixes of a finite set of codewords, and a dangling suffix can be added at most
    once.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '{ 0, 01, 11 }. The codeword 0 is a prefix of 01, so add the dangling suffix
    1. { 0, 01, 11, 1 }. The codeword 0 is a prefix of 01, but the dangling suffix
    1 is already in the list; the codeword 1 is a prefix of 11, but the dangling suffix
    1 is already in the list. There are no other dangling suffixes, so conclude that
    the set is uniquely decodable.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '{ 0, 01, 10 }. The codeword 0 is a prefix of 01, so add the dangling suffix
    1 to the list. { 0, 01, 10, 1 }. The codeword 1 is a prefix of 10, but the dangling
    suffix 0 is a codewords. So, conclude that the code is not uniquely decodeable.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kraft-McMillan inequality.** Conside a code C with N codewords of lengths
    n1, n2, ..., nN. Prove that if the code is uniquely decodable, then K(C) = sum_i
    = 1 to N 2^(-ni) ≤ 1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Kraft-McMillan construction.** Suppose that we have a set of integers n1,
    n2, ..., nN that satisfy the inequality sum_i = 1 to N 2^(-ni) ≤ 1\. Prove that
    it is always possible to find a prefix-free code with codewords lengths n1, n2,
    ..., nN. Thus, by restricting attention to prefix-free codes (instead of uniquely
    decodable codes), we do not lose much.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Kraft-McMillan equality for optimal prefix-free codes.** Prove that if C
    is an optimal prefix-free code then the Kraft-McMillan inequality is an equality:
    K(C) = sum_i = 1 to N 2^(-ni) = 1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose that all of the symbol probabilities are negative powers of 2. Describe
    the Huffman code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose that all of the symbol frequencies are equal. Describe the Huffman code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find a Huffman code where the length of a symbol with probability pi is greater
    than ceil(-lg pi).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Solution.* .01 (000), .30 (001), .34 (01), .35 (1). The codeword 001 has length
    greater than ceil(-lg .30).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: True or false. Any optimal prefix-free code can be obtained via Huffman's algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Solution.* False. Consider the following set of symbols and frequencies (A
    26, B 24, C 14, D 13, E 12, F 11).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In any Huffman code, the codings for A and B must begin with different bits,
    but the code C3 does not have this property (yet it is an optimal prefix-free
    code).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What is the LZW encoding of the following inputs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: T O B E O R N O T T O B E
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Y A B B A D A B B A D A B B A D O O
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A A A A A A A A A A A A A A A A A A A A A
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Characterize the tricky situation in LZW coding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Solution.* Whenever it encounteres cScSc, where c is a symbol, S is a string,
    cS is in the dictionary but cSc is not.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As a function of N, how many bits are needed to encode N copies of the symbol
    A? N copies of the sequence ABC?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let F(i) be the ith Fibonacci number. Consider N symbols, where the ith symbol
    has frequency F(i). Note that F(1) + F(2) + ... + F(N) = F(N+2) - 1. Describe
    the Huffman code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Solution.* Longest codeword has length N-1.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Show that there are at least 2^(N-1) different Huffman codes corresponding to
    a given set of N symbols.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Solution.* There are N-1 internal nodes and each one has an arbitrary choice
    to assign its left and right children.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Give a Huffman code where the frequency of 0s in the output is much much higher
    than the frequency of 1s.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Solution.* If the character ''A'' occurs one million times and the character
    ''B'' occurs once, the code word for ''A'' will be 0 and the codeword for ''B''
    will be 1.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Prove the following facts about Huffman tries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The two longest codewords have the same length.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the frequency of symbol i is strictly larger than the frequency of symbol
    j, then the length of the codeword for symbol i is less than or equal to the length
    of the codeword for symbol j.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe how to transmit a Huffman code (or optimal prefix-free code) on a set
    of symbols { 0, 1, ..., N-1 } using 2N - 1 + N ceil(lg N) bits.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Hint*: use 2N-1 bits to specify the structure of the corresponding trie.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Suppose that in an extended ASCII file (8-bit characters), the maximum character
    frequency is at most twice the minimum character frequency. Prove that and fixed-length
    8-bit extended ASCII code is optimal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Shannon-Fano coding.** Prove that the following top-down version of Huffman''s
    algorithm is not optimal. Split the set of codewords C into two subsets C1 and
    C2 with (almost) equal frequencies. Recursively build the tree for C1 and C2,
    starting all codewords for C1 with 0 and all codewords for C2 with 1. To implement
    the first step, Shannon and Fano propose sorting the codewords by frequency and
    breaking the set up into two subarrays as best as possible.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Solution.* S 32, H 25, A 20, N 18, O 5.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**LZMW coding (Miller-Wegman 1985).** LZ variant: search input for longest
    string already in the dictionary (the current match); add concatenation of previous
    match to current match to the dictionary. Dictionary entries grow more rapidly.
    Can also delete low-frequency entries when the dictionary fills up. Hard to implement.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**LZAP coding.** Similar to LZMW: instead of adding just the concatenation
    of the previous match with the current match, add the concatenation of the previous
    match with *all prefixes* of the current match. Easier than LZMW to implement,
    but even more dictionary entries.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify an optimal code that is not prefix-free.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Hint*: only need 3 symbols with equal frequencies.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Identify two optimal prefix-free codes for the same input that have a different
    distribution of codeword lengths.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Hint*: only need 4 symbols.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Minimum variance Huffman coding.** Due to the nondeterminism associated with
    tiebraking, Huffman''s algorithm may produce codes with different distributions
    of codeword lengths. When transmitting the compressed stream as it is being generated,
    it is desirable to transmit bits at a (near) constant rate. Find Huffman code
    that minimize sum_i (p_i (l_i - l_average(T)) ^2).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Solution.* When combining tries, break ties by picking the earliest produced
    trie with the smallest probability.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Two-queue algorithm for Huffman coding.** Prove that the following algorithm
    computes a Huffman code (and runs in linear time if the input symbols are already
    sorted by frequency). Maintain two FIFO queues: the first queue contains the input
    symbols, in ascending order of frequency, the second queue contains the internal
    nodes with combined weights. As long as there is more than one node in the two
    queues, dequeue the two nodes with the smallest weight by examining the fronts
    of both queues. Create a new internal node (left and right child = two nodes,
    weight = sum of weight of two nodes) and enqueue on the second queue.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To obtain a minimum variance Huffman code, break ties by choosing nodes from
    the first queue.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Hint*: prove that the second queue is sorted in ascending order of frequency.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Sibling property.** A binary tree has the *sibling property* if (i) every
    node (except the root) has a sibling and (ii) the binary tree can be listed in
    non-increasing order of probability such that, in the list, all siblings are adjacent.
    Prove that a binary tree represents a Huffman tree if and only if it has the sibling
    property.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Relative coding.** Instead of compressing each pixel in an image, consider
    the difference between a pixel and the previous one and encode the difference.
    Intuition: usually the pixels don''t change much. Use with LZW over color table
    alphabet.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Variable-width LZW codes.** Increase the width of the table from p to p+1
    after 2^p th codeword is inserted into table. Used with color table alphabet.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Adaptive Huffman coding.** One-pass algorithm and don''t need to send prefix-free
    code. Build Huffman tree based on frequency of characters read in so far. Update
    tree after reading in each character. Encoder and decoder need to coordinate on
    tie-breaking conventions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Shannon entropy.** The entropy H of a discrete random variable X with possible
    values x1, ..., xN that occur with probability p1, ..., pN is defined as H(X)
    = -p1 lg p1 - p2 lg p2 - ... - pN lg pN, where 0 lg 0 = 0 is consistent with the
    limit.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the entropy of a fair coin?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the entropy of a coin where both sides are heads?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the entropy of a six-sided die?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Solution.* -lg (1/6) which is about 2.584962.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: What is the entropy of the sum of two fair dice?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Given a random variable that takes on N values. What distribution maximizes
    the entropy?The entropy is a fundamental concept in information theory. Shannon's
    source coding theorem asserts that to compress the data from a stream of independent
    and identically distributed random variables requires at least H(X) bits per symbol
    in the limit. For example, to send the results of a sequence of fair die tosses
    requires at least 2.584962 bits per die toss.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Empirical entropy.** The *empirical entropy* of a piece of text is obtained
    by computing the frequency of occurrence of each symbol and using these as the
    probabilities for a discrete random variable. Compute the empirical entropy of
    your favorite novel. Compare it to the data compression rate achieved by a Huffman
    code.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Shannon experiment.** Perform the following experiment. Give a subject a
    sequence of k letters from a piece of text (or Leipzig corpus) and ask them to
    predict the next letter. Estimate the fraction of times the subject gets the answer
    right for k = 1, 2, 5, 100.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True or false. Fixed-length codes are uniquely decodable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Solution.* True, they are prefix free.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Give two different Huffman trees the string ABCCDD, with different heights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prefix-free codes.** Design an efficient algorithm to determine if a set
    of binary code words is prefix-free. *Hint*: use a binary trie or sort.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Uniquely decodable code.** Devise a uniquely decodable code that is not a
    prefix free code. Hint: suffix free codes = reverse of prefix free codes. Reverse
    of suffix free code is prefix free code -> can decode by reading compressed message
    in reverse order. Not very convenient.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Huffman tree.** Modify [Huffman.java](Huffman.java.html) so that the encoder
    prints out the lookup table instead of the preorder traversal, and modify the
    decoder so that it constructs the tree by reading in the lookup table.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True or false. In an optimal prefix-free ternary code, the three symbols that
    occur least frequently have the same length.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Solution.* False.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Ternary Huffman codes.** Generalize the Huffman algorithm to codewords over
    the ternary alphabet (0, 1, and 2) instead of the binary alphabet. That is, given
    a bytestream, find a prefix-free ternary code that uses as few trits (0s, 1s,
    and 2s) as possible. Prove that it yields optimal prefix-free ternary code.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Solution.* Combine smallest 3 probabilities at each step (instead of smallest
    2). This works when there are 3 + 2k symbols for some integer k. To reduce to
    this case, add 1 or 2 dummy symbols of probability 0. (Alternatively, combine
    fewer than 3 symbols in the first step if the number of symbols is not 3 + 2k.)
    Ex: { 0.1, 0.2, 0.2, 0.5 }.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Nonbinary Huffman codes.** Extend the Huffman algorithm to codewords over
    the m-ary alphabet (0, 1, 2, ..., m-1) instead of the binary alphabet.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Consider the following 21 character message that consists of 3 a's, 7c's, 6
    t's and 5 g's.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Are the following 43 bits a possible Huffman encoding of the message above?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Justify your answer as concisely and rigorously as possible.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Solution.* A Huffman encoding for a message produces an encoding that uses
    the fewest bits among any prefix free code. The 2-bit binary code a = 00, c =
    01, g = 10, t = 11 is a prefix free code that uses 21 * 2 = 42 bits. Thus, a Huffman
    code would use fewer than 43 bits.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A binary tree is *full* if every node that is not a leaf has two children. Prove
    that any binary tree corresponding to an optimal prefix-free code is full.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Hint*: if an internal node has only one child, replace that internal node
    with its unique child.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Move-to-front coding (Bentley, Sleator, Tarjan, and Wei 1986).** Write a
    program `MoveToFront` that implements move-to-front encoding and decoding. Maintain
    alphabet of symbols in a list, where frequently occurring symbols are towards
    the front. A symbol is encoded as the number of symbols that precede it in the
    list. After encoding a symbol, move it to the front of the list. [reference](../references/papers/move-to-front.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Move-ahead-k coding.** Same as move-to-front coding, but move symbol k positions
    toward the front.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Wait-c-and-move.** Same as move-to-front coding, but move symbol to the front
    only after it has been encountered c times since the last time it was moved to
    the front.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Double Huffman compression.** Find an input for which applying the `compress()`
    method in [Huffman.java](Huffman.java.html) twice leads to a strictly smaller
    output than applying `compress()` only once.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Merging k sorted arrays.** You have k sorted lists, of lenths n1, n2, ...,
    nk. Supposet that the only operation you can perform to combine lists is a 2-way
    merge: given one sorted array of length n1 and another sorted array of length
    n2, replace them with a sorted array of length n = n1 + n2\. Moreover, the 2-way
    merge operation takes exactly n units of time. What is the optimal way to merge
    the k sorted arrays?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Solution.* Sort the list lengths so that n1 < n2 < ... < nk. Repetedly take
    the two smallest lists and apply the 2-way merge operation. The proof of optimality
    is the same as the proof of optimality of Huffman codes: repeatedly applying 2-way
    merge operations induces a binary tree in which each leaf node corresponds to
    one of the original sorted lists and each internal node corresponds to a 2-way
    merge operation. The contribution of any original list to the overall cost is
    the length of the list multiplied by its tree depth (because that is the number
    of times its elements are involved in a 2-way merge).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
