- en: Lecture 4
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四讲
- en: 原文：[https://cs50.harvard.edu/ai/notes/4/](https://cs50.harvard.edu/ai/notes/4/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://cs50.harvard.edu/ai/notes/4/](https://cs50.harvard.edu/ai/notes/4/)
- en: Machine Learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习
- en: Machine learning provides a computer with data, rather than explicit instructions.
    Using these data, the computer learns to recognize patterns and becomes able to
    execute tasks on its own.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习为计算机提供数据，而不是明确的指令。利用这些数据，计算机学会识别模式，并能够自主执行任务。
- en: Supervised Learning
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习
- en: Supervised learning is a task where a computer learns a function that maps inputs
    to outputs based on a dataset of input-output pairs.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习是一个任务，其中计算机根据输入-输出对的训练集学习一个将输入映射到输出的函数。
- en: There are multiple tasks under supervised learning, and one of those is **Classification**.
    This is a task where the function maps an input to a discrete output. For example,
    given some information on humidity and air pressure for a particular day (input),
    the computer decides whether it will rain that day or not (output). The computer
    does this after training on a dataset with multiple days where humidity and air
    pressure are already mapped to whether it rained or not.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习下有多个任务，其中之一是**分类**。这是一个将输入映射到离散输出的函数的任务。例如，给定某一天湿度和大气的压力信息（输入），计算机决定那天是否会下雨（输出）。计算机在训练集上完成训练后，该训练集包含多天的湿度和大气的压力信息，并已映射到是否下雨。
- en: This task can be formalized as follows. We observe nature, where a function
    *f(humidity, pressure)* maps the input to a discrete value, either Rain or No
    Rain. This function is hidden from us, and it is probably affected by many other
    variables that we don’t have access to. Our goal is to create function *h(humidity,
    pressure)* that can approximate the behavior of function *f*. Such a task can
    be visualized by plotting days on the dimensions of humidity and rain (the input),
    coloring each data point in blue if it rained that day and in red if it didn’t
    rain that day (the output). The white data point has only the input, and the computer
    needs to figure out the output.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务可以形式化为以下内容。我们观察自然界，其中函数 *f(湿度, 压力)* 将输入映射到离散值，要么是雨，要么是无雨。这个函数对我们来说是隐藏的，它可能受到许多其他变量的影响，而我们无法获取这些变量。我们的目标是创建函数
    *h(湿度, 压力)*，它可以近似函数 *f* 的行为。这样的任务可以通过在湿度、降雨（输入）维度上绘制天数来可视化，如果那天下雨，则将每个数据点着色为蓝色，如果没有下雨，则着色为红色（输出）。白色数据点只有输入，计算机需要确定输出。
- en: '![Classification](../Images/415c76c2cc5395fc02a242cd72a7ff13.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![分类](../Images/415c76c2cc5395fc02a242cd72a7ff13.png)'
- en: Nearest-Neighbor Classification
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最近邻分类
- en: One way of solving a task like the one described above is by assigning the variable
    in question the value of the closest observation. So, for example, the white dot
    on the graph above would be colored blue, because the nearest observed dot is
    blue as well. This might work well some times, but consider the graph below.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 解决上述任务的一种方法是将相关变量分配给最近的观察点的值。例如，图上方的白色点应该着色为蓝色，因为最近的观察点也是蓝色。这可能在某些时候工作得很好，但考虑下面的图。
- en: '![Nearest Neighbor Classification](../Images/e4e1f26e17cefe113983d40072783e8f.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![最近邻分类](../Images/e4e1f26e17cefe113983d40072783e8f.png)'
- en: Following the same strategy, the white dot should be colored red, because the
    nearest observation to it is red as well. However, looking at the bigger picture,
    it looks like most of the other observations around it are blue, which might give
    us the intuition that blue is a better prediction in this case, even though the
    closest observation is red.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 按照同样的策略，白色点应该着色为红色，因为最近的观察点也是红色。然而，从更大的角度来看，它看起来周围的大多数其他观察点都是蓝色，这可能会给我们这样的直觉：在这种情况下，蓝色是一个更好的预测，尽管最近的观察点是红色。
- en: One way to get around the limitations of nearest-neighbor classification is
    by using **k-nearest-neighbors classification**, where the dot is colored based
    on the most frequent color of the *k* nearest neighbors. It is up to the programmer
    to decide what *k* is. Using a 3-nearest neighbors classification, for example,
    the white dot above will be colored blue, which intuitively seems like a better
    decision.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一种克服最近邻分类局限性的方法是通过使用**k-最近邻分类**，其中点根据最近的 *k* 个邻居中最频繁的颜色着色。程序员需要决定 *k* 的值。例如，使用3-最近邻分类，上面的白色点将被着色为蓝色，这直观上看起来是一个更好的决定。
- en: A drawback of the k-nearest-neighbors classification is that, using a naive
    approach, the algorithm will have to measure the distance of every single point
    to the point in question, which is computationally expensive. This can be sped
    up by using data structures that enable finding neighbors more quickly or by pruning
    irrelevant observations.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: k最近邻分类的一个缺点是，使用原始方法，算法将不得不测量每个单独的点与问题点的距离，这在计算上很昂贵。这可以通过使用能够更快找到邻居的数据结构或通过剪枝无关观察结果来加速。
- en: Perceptron Learning
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感知器学习
- en: Another way of going about a classification problem, as opposed to the nearest-neighbor
    strategy, is looking at the data as a whole and trying to create a decision boundary.
    In two-dimensional data, we can draw a line between the two types of observations.
    Every additional data point will be classified based on the side of the line on
    which it is plotted.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与最近邻策略相比，另一种处理分类问题的方式是将数据视为整体，并尝试创建一个决策边界。在二维数据中，我们可以在两种观察结果之间画一条线。每个额外的数据点都将根据其绘制在直线哪一侧进行分类。
- en: '![Decision Boundary](../Images/4bae1c5ac2678cfbcce14da1361b3c47.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![决策边界](../Images/4bae1c5ac2678cfbcce14da1361b3c47.png)'
- en: The drawback to this approach is that data are messy, and it is rare that one
    can draw a line and neatly divide the classes into two observations without any
    mistakes. Often, we will compromise, drawing a boundary that separates the observations
    correctly more often than not, but still occasionally misclassifies them.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是数据很杂乱，很少能画一条线，将类别干净利落地分成两个观察结果而没有错误。通常，我们会妥协，画出的边界大多数情况下能正确地分隔观察结果，但偶尔还是会错误分类。
- en: In this case, the input of
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，输入为
- en: '*x₁* = Humidity'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x₁* = 湿度'
- en: '*x₂* = Pressure'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x₂* = 压力'
- en: 'will be given to a hypothesis function *h(x₁, x₂)*, which will output its prediction
    of whether it is going to rain that day or not. It will do so by checking on which
    side of the decision boundary the observation falls. Formally, the function will
    weight each of the inputs with an addition of a constant, ending in a linear equation
    of the following form:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 将被提供给一个假设函数 *h(x₁, x₂)*，该函数将输出它对当天是否会下雨的预测。它将通过检查观察结果落在决策边界的哪一侧来完成。形式上，该函数将每个输入乘以一个常数的和，最终得到以下形式的线性方程：
- en: Rain w₀ + w₁x₁ + w₂x₂ ≥ 0
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rain w₀ + w₁x₁ + w₂x₂ ≥ 0
- en: No Rain otherwise
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无雨否则
- en: Often, the output variable will be coded as 1 and 0, where if the equation yields
    more than 0, the output is 1 (Rain), and 0 otherwise (No Rain).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，输出变量将被编码为1和0，其中如果方程结果大于0，输出为1（雨），否则为0（无雨）。
- en: 'The weights and values are represented by vectors, which are sequences of numbers
    (which can be stored in lists or tuples in Python). We produce a Weight Vector
    **w**: (w₀, w₁, w₂), and getting to the best weight vector is the goal of the
    machine learning algorithm. We also produce an Input Vector **x**: (1, x₁, x₂).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 权重和值由向量表示，这些是数字序列（在Python中可以存储在列表或元组中）。我们产生一个权重向量 **w**： (w₀, w₁, w₂)，得到最佳权重向量是机器学习算法的目标。我们还产生一个输入向量
    **x**： (1, x₁, x₂)。
- en: 'We take the dot product of the two vectors. That is, we multiply each value
    in one vector by the corresponding value in the second vector, arriving at the
    expression above: w₀ + w₁x₁ + w₂x₂. The first value in the input vector is 1 because,
    when multiplied by the weight vector w₀, we want to keep it a constant.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算两个向量的点积。也就是说，我们将一个向量中的每个值乘以第二个向量中相应的值，得到上面的表达式：w₀ + w₁x₁ + w₂x₂。输入向量中的第一个值是1，因为当我们将其与权重向量
    w₀ 相乘时，我们希望将其保持为常数。
- en: 'Thus, we can represent our hypothesis function the following way:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以用以下方式表示我们的假设函数：
- en: '![Dot Product Equation](../Images/6ddd450e60d388501b2e3f69a352d491.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![点积方程](../Images/6ddd450e60d388501b2e3f69a352d491.png)'
- en: 'Since the goal of the algorithm is to find the best weight vector, when the
    algorithm encounters new data it updates the current weights. It does so using
    the **perceptron learning rule**:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于算法的目标是找到最佳权重向量，当算法遇到新数据时，它会更新当前权重。它是通过使用 **感知器学习规则** 来做到这一点的：
- en: '![Perceptron Learning Rule](../Images/4b810c838181f701b060c777e781f826.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![感知器学习规则](../Images/4b810c838181f701b060c777e781f826.png)'
- en: The important takeaway from this rule is that for each data point, we adjust
    the weights to make our function more accurate. The details, which are not as
    critical to our point, are that each weight is set to be equal to itself plus
    some value in parentheses. Here, y stands for the observed value while the hypothesis
    function stands for the estimate. If they are identical, this whole term is equal
    to zero, and thus the weight is not changed. If we underestimated (calling No
    Rain while Rain was observed), then the value in the parentheses will be 1 and
    the weight will increase by the value of xᵢ scaled by α the learning coefficient.
    If we overestimated (calling Rain while No Rain was observed), then the value
    in the parentheses will be -1 and the weight will decrease by the value of x scaled
    by α. The higher α, the stronger the influence each new event has on the weight.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从这条规则中重要的收获是，对于每个数据点，我们调整权重以使我们的函数更准确。细节，虽然对我们论点不是那么关键，是每个权重都被设置为等于它自己加上括号中的某个值。在这里，y代表观察到的值，而假设函数代表估计。如果它们相同，这个整个项就等于零，因此权重不会改变。如果我们低估了（在观察到雨时称之为“无雨”），那么括号中的值将是1，权重将增加由xᵢ缩放的学习系数α的值。如果我们高估了（在观察到无雨时称之为“雨”），那么括号中的值将是-1，权重将减少由x缩放的学习系数α的值。α越高，每个新事件对权重的影响就越强。
- en: The result of this process is a threshold function that switches from 0 to 1
    once the estimated value crosses some threshold.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的成果是一个阈值函数，一旦估计值超过某个阈值，就会从0切换到1。
- en: '![Hard Threshold](../Images/d5ddcf3c98164b6fceb59359ea41b844.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![硬阈值](../Images/d5ddcf3c98164b6fceb59359ea41b844.png)'
- en: The problem with this type of function is that it is unable to express uncertainty,
    since it can only be equal to 0 or to 1\. It employs a **hard threshold**. A way
    to go around this is by using a logistic function, which employs a **soft threshold**.
    A logistic function can yield a real number between 0 and 1, which will express
    confidence in the estimate. The closer the value to 1, the more likely it is to
    rain.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型函数的问题在于它无法表达不确定性，因为它只能等于0或1。它采用**硬阈值**。一种绕过这个问题的方法是使用对数函数，它采用**软阈值**。对数函数可以产生一个介于0和1之间的实数，这将表达对估计的信心。值越接近1，下雨的可能性就越大。
- en: '![Soft Threshold](../Images/e9ef28e38b60063ef10fe413b4e6bbae.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![软阈值](../Images/e9ef28e38b60063ef10fe413b4e6bbae.png)'
- en: Support Vector Machines
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持向量机
- en: In addition to nearest-neighbor and linear regression, another approach to classification
    is the Support Vector Machine. This approach uses an additional vector (support
    vector) near the decision boundary to make the best decision when separating the
    data. Consider the example below.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 除了最近邻和线性回归之外，分类的另一种方法是支持向量机。这种方法使用决策边界附近的一个附加向量（支持向量）来在分离数据时做出最佳决策。考虑下面的例子。
- en: '![Support Vector Machine](../Images/893ce42b0da3cd6973dff8ec3618e39e.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](../Images/893ce42b0da3cd6973dff8ec3618e39e.png)'
- en: All the decision boundaries work in that they separate the data without any
    mistakes. However, are they equally as good? The two leftmost decision boundaries
    are very close to some of the observations. This means that a new data point that
    differs only slightly from one group can be wrongly classified as the other. As
    opposed to that, the rightmost decision boundary keeps the most distance from
    each of the groups, thus giving the most leeway for variation within it. This
    type of boundary, which is as far as possible from the two groups it separates,
    is called the **Maximum Margin Separator**.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的决策边界都在于它们在没有任何错误的情况下分离数据。然而，它们是否同样好？最左边的两个决策边界与一些观察值非常接近。这意味着一个只与一个组略有不同的新数据点可能会被错误地分类为另一组。相反，最右边的决策边界与每个组保持最大的距离，从而为它内部的变异提供了最大的灵活性。这种尽可能远离它所分离的两个组的边界，被称为**最大间隔分离器**。
- en: Another benefit of support vector machines is that they can represent decision
    boundaries with more than two dimensions, as well as non-linear decision boundaries,
    such as below.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机的另一个好处是，它们可以表示超过两个维度的决策边界，以及非线性决策边界，如下所示。
- en: '![Circle Decision Boundary](../Images/6aee74bd10dce4b3d0e5619b5875f8fe.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![圆形决策边界](../Images/6aee74bd10dce4b3d0e5619b5875f8fe.png)'
- en: To summarize, there are multiple ways to go about classification problems, with
    no one being always better than the other. Each has their drawbacks and might
    prove more useful than others in specific situations.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，处理分类问题有多种方法，没有哪一种方法总是比其他方法更好。每种方法都有其缺点，可能在某些特定情况下比其他方法更有用。
- en: Regression
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归
- en: Regression is a supervised learning task of a function that maps an input point
    to a continuous value, some real number. This differs from classification in that
    classification problems map an input to discrete values (Rain or No Rain).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 回归是一个监督学习任务，它将一个输入点映射到一个连续值，即某个实数。这与分类不同，因为分类问题将输入映射到离散值（例如，雨天或无雨）。
- en: For example, a company might use regression to answer the question of how money
    spent advertising predicts money earned in sales. In this case, an observed function
    *f(advertising)* represents the observed income following some money that was
    spent in advertising (note that the function can take more than one input variable).
    These are the data that we start with. With this data, we want to come up with
    a hypothesis function *h(advertising)* that will try to approximate the behavior
    of function *f*. *h* will generate a line whose goal is not to separate between
    types of observations, but to predict, based on the input, what will be the value
    of the output.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一家公司可能会使用回归来回答广告支出如何预测销售收入的疑问。在这种情况下，一个观测函数 *f(广告)* 代表在广告上花费一些钱之后的观测收入（注意该函数可以接受多个输入变量）。这些是我们开始时的数据。有了这些数据，我们希望提出一个假设函数
    *h(广告)*，该函数将尝试近似函数 *f* 的行为。*h* 将生成一条线，其目标不是区分观察类型，而是根据输入预测输出值。
- en: '![Regression](../Images/d6dd40f88472a54769b159ee3306d99e.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![回归](../Images/d6dd40f88472a54769b159ee3306d99e.png)'
- en: Loss Functions
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: Loss functions are a way to quantify the utility lost by any of the decision
    rules above. The less accurate the prediction, the larger the loss.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数是量化上述任何决策规则所损失效用的一种方法。预测越不准确，损失就越大。
- en: For classification problems, we can use a **0-1 Loss Function**.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，我们可以使用 **0-1 损失函数**。
- en: '*L*(actual, predicted):'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L*(实际，预测)：'
- en: 0 if actual = predicted
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 如果实际值等于预测值
- en: 1 otherwise
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 否则
- en: In words, this function gains value when the prediction isn’t correct and doesn’t
    gain value when it is correct (i.e. when the observed and predicted values match).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 用话来说，这个函数在预测不正确时增加价值，而在预测正确时不增加价值（即当观测值和预测值匹配时）。
- en: '![0-1 Loss Function](../Images/1c6a2f127e905c1c8569acce214ad7f1.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![0-1 损失函数](../Images/1c6a2f127e905c1c8569acce214ad7f1.png)'
- en: In the example above, the days that are valued at 0 are the ones where we predicted
    the weather correctly (rainy days are below the line and not rainy days are above
    the line). However, days when it didn’t rain below the line and days when it did
    rain above it are the ones that we failed to predict. We give each one the value
    of 1 and sum them up to get an empirical estimate of how lossy our decision boundary
    is.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，值为 0 的天数是我们正确预测天气的日子（雨天在线下方，非雨天在上方线）。然而，线下方没有下雨而上方线下雨的日子是我们未能预测到的。我们给每个这样的日子赋予值为
    1，并将它们加起来以得到一个经验估计，即我们的决策边界有多大的损失。
- en: L₁ and L₂ loss functions can be used when predicting a continuous value. In
    this case, we are interested in quantifying for each prediction *how much* it
    differed from the observed value. We do this by taking either the absolute value
    or the squared value of the observed value minus the predicted value (i.e. how
    far the prediction was from the observed value).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: L₁ 和 L₂ 损失函数可以用于预测连续值。在这种情况下，我们感兴趣的是量化每个预测与观测值差异的程度。我们通过取观测值减去预测值的绝对值或平方值（即预测值与观测值之间的距离）来实现这一点。
- en: 'L₁: *L*(actual, predicted) = |actual - predicted|'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'L₁: *L*(实际，预测) = |实际 - 预测|'
- en: 'L₂: *L*(actual, predicted) = (actual - predicted)²'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'L₂: *L*(实际，预测) = (实际 - 预测)²'
- en: 'One can choose the loss function that serves their goals best. L₂ penalizes
    outliers more harshly than L₁ because it squares the the difference. L₁ can be
    visualized by summing the distances from each observed point to the predicted
    point on the regression line:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 可以选择最适合自己目标的损失函数。L₂ 比起 L₁ 更严厉地惩罚异常值，因为它平方了差异。L₁ 可以通过将每个观测点到回归线上的预测点的距离求和来可视化：
- en: '![L₁](../Images/6e6e449dfb6a15a872a878e435757976.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![L₁](../Images/6e6e449dfb6a15a872a878e435757976.png)'
- en: Overfitting
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过拟合
- en: Overfitting is when a model fits the training data so well that it fails to
    generalize to other data sets. In this sense, loss functions are a double edged
    sword. In the two examples below, the loss function is minimized such that the
    loss is equal to 0\. However, it is unlikely that it will fit new data well.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是指模型对训练数据拟合得如此之好，以至于它无法泛化到其他数据集。从这个意义上说，损失函数是一把双刃剑。在下面的两个例子中，损失函数被最小化，使得损失等于0。然而，它不太可能很好地拟合新数据。
- en: '![Overfitting](../Images/3a08a564cadd1bc628714d43ff1b7334.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![过拟合](../Images/3a08a564cadd1bc628714d43ff1b7334.png)'
- en: For example, in the left graph, a dot next to the red one at the bottom of the
    screen is likely to be Rain (blue). However, with the overfitted model, it will
    be classified as No Rain (red).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在左边的图中，屏幕底部红色点旁边的点很可能是雨（蓝色）。然而，在过拟合的模型中，它将被分类为无雨（红色）。
- en: Regularization
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化
- en: Regularization is the process of penalizing hypotheses that are more complex
    to favor simpler, more general hypotheses. We use regularization to avoid overfitting.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是惩罚更复杂假设的过程，以有利于更简单、更一般的假设。我们使用正则化来避免过拟合。
- en: In regularization, we estimate the cost of the hypothesis function h by adding
    up its loss and a measure of its complexity.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在正则化中，我们通过将假设函数h的损失和其复杂度的度量相加来估计h的成本。
- en: '*cost*(h) = *loss*(h) + λ*complexity*(h)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*成本*(h) = *损失*(h) + λ*复杂度*(h)'
- en: Lambda (λ) is a constant that we can use to modulate how strongly to penalize
    for complexity in our cost function. The higher λ is, the more costly complexity
    is.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda (λ) 是一个常数，我们可以用它来调节我们在成本函数中对复杂性的惩罚强度。λ越高，复杂性的成本就越高。
- en: 'One way to test whether we overfitted the model is with **Holdout Cross Validation**.
    In this technique, we split all the data in two: a **training set** and a **test
    set**. We run the learning algorithm on the training set, and then see how well
    it predicts the data in the test set. The idea here is that by testing on data
    that were not used in training, we can a measure how well the learning generalizes.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 测试我们是否过拟合了模型的一种方法是用**保留交叉验证**。在这个技术中，我们将所有数据分成两部分：一个**训练集**和一个**测试集**。我们在训练集上运行学习算法，然后看看它预测测试集中数据的准确性。这里的想法是通过在未用于训练的数据上测试，我们可以衡量学习泛化的程度。
- en: The downside of holdout cross validation is that we don’t get to train the model
    on half the data, since it is used for evaluation purposes. A way to deal with
    this is using ***k*-Fold Cross-Validation**. In this process, we divide the data
    into k sets. We run the training k times, each time leaving out one dataset and
    using it as a test set. We end up with k different evaluations of our model, which
    we can average and get an estimate of how our model generalizes without losing
    any data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 保留交叉验证的缺点是我们无法在半数数据上训练模型，因为它被用于评估目的。解决这个问题的方法是用**k-折交叉验证**。在这个过程中，我们将数据分成k个集合。我们运行训练k次，每次留出一个数据集作为测试集。我们最终得到k个不同的模型评估，我们可以对这些评估进行平均，从而得到模型泛化的估计，而不会丢失任何数据。
- en: scikit-learn
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: scikit-learn
- en: As often is the case with Python, there are multiple libraries that allow us
    to conveniently use machine learning algorithms. One of such libraries is scikit-learn.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 与Python一样，有许多库允许我们方便地使用机器学习算法。其中之一是scikit-learn。
- en: As an example, we are going to use a [CSV](https://en.wikipedia.org/wiki/Comma-separated_values)
    dataset of counterfeit banknotes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 作为例子，我们将使用一个[CSV](https://en.wikipedia.org/wiki/Comma-separated_values)数据集的假币。
- en: '![Banknotes](../Images/809a5a374cc8f540fe96989708d34fc1.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![纸币](../Images/809a5a374cc8f540fe96989708d34fc1.png)'
- en: The four left columns are data that we can use to predict whether a note is
    genuine or counterfeit, which is external data provided by a human, coded as 0
    and 1\. Now we can train our model on this data set and see if we can predict
    whether new banknotes are genuine or not.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 四个左侧列是我们可以用来预测纸币是真还是假的数据，这是由人类提供的外部数据，编码为0和1。现在我们可以在这个数据集上训练我们的模型，看看我们能否预测新纸币是否为真。
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that after importing the libraries, we can choose which model to use. The
    rest of the code will stay the same. SVC stands for Support Vector Classifier
    (which we know as support vector machine). The KNeighborsClassifier uses the k-neighbors
    strategy, and requires as input the number of neighbors it should consider.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在导入库之后，我们可以选择使用哪个模型。其余的代码将保持不变。SVC代表支持向量分类器（我们称之为支持向量机）。KNeighborsClassifier使用k-邻居策略，并需要输入它应该考虑的邻居数量。
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This manual version of running the algorithm can be found in the source code
    for this lecture under banknotes0.py. Since the algorithm is used often in a similar
    way, scikit-learn contains additional functions that make the code even more succinct
    and easy to use, and this version can be found under banknotes1.py.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法的手动版本可以在本讲座的源代码中的banknotes0.py文件中找到。由于算法经常以类似的方式使用，scikit-learn 包含了额外的函数，使代码更加简洁且易于使用，这个版本可以在banknotes1.py文件中找到。
- en: Reinforcement Learning
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 状态集合 ***S***
- en: Reinforcement learning is another approach to machine learning, where after
    each action, the agent gets feedback in the form of reward or punishment (a positive
    or a negative numerical value).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习
- en: '![Reinforcement Learning](../Images/8d0216ae95409f63035447749b2929bc.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: 强化学习
- en: The learning process starts by the environment providing a state to the agent.
    Then, the agent performs an action on the state. Based on this action, the environment
    will return a state and a reward to the agent, where the reward can be positive,
    making the behavior more likely in the future, or negative (i.e. punishment),
    making the behavior less likely in the future.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习可以被视为一个马尔可夫决策过程，具有以下特性：
- en: This type of algorithm can be used to train walking robots, for example, where
    each step returns a positive number (reward) and each fall a negative number (punishment).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![马尔可夫决策过程演示](../Images/9b390c34fc086efdff8b5e38013fb13e.png)'
- en: Markov Decision Processes
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '动作集合 ***Actions(S)*** '
- en: 'Reinforcement learning can be viewed as a Markov decision process, having the
    following properties:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: Set of states ***S***
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种类型的算法可以用来训练行走机器人，例如，每一步都会返回一个正数（奖励）和每次跌倒都会返回一个负数（惩罚）。
- en: Set of actions ***Actions(S)***
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励函数 ***R(s, a, s’)***
- en: Transition model ***P(s’ | s, a)***
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '转移模型 ***P(s’ | s, a)*** '
- en: Reward function ***R(s, a, s’)***
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q学习是强化学习的一种模型，其中函数 ***Q(s, a)*** 输出在状态 *s* 采取动作 *a* 的价值估计。
- en: 'For example, consider the following task:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下任务：
- en: '![Markov Decision Process Demo](../Images/9b390c34fc086efdff8b5e38013fb13e.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: 强化学习是机器学习的另一种方法，在每次动作之后，代理都会以奖励或惩罚（正或负的数值）的形式获得反馈。
- en: The agent is the yellow circle, and it needs to get to the green square while
    avoiding the red squares. Every single square in the task is a state. Moving up,
    down, or to the sides is an action. The transition model gives us the new state
    after performing an action, and the reward function is what kind of feedback the
    agent gets. For example, if the agent chooses to go right, it will step on a red
    square and get negative feedback. This means that the agent will learn that, when
    in the state of being in the bottom-left square, it should avoid going right.
    This way, the agent will start exploring the space, learning which state-action
    pairs it should avoid. The algorithm can be probabilistic, choosing to take different
    actions in different states based on some probability that’s being increased or
    decreased based on reward. When the agent reaches the green square, it will get
    a positive reward, learning that it is favorable to take the action it took in
    the previous state.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 代理是黄色圆圈，它需要到达绿色方块，同时避开红色方块。任务中的每一个方块都是一个状态。向上、向下或向侧面移动是一个动作。转移模型给出了执行动作后的新状态，奖励函数是代理获得的反馈类型。例如，如果代理选择向右移动，它将踩到红色方块并得到负面反馈。这意味着代理将学会，当处于左下角方块的状态时，应该避免向右移动。这样，代理将开始探索空间，学习哪些状态-动作对应该避免。该算法可以是概率性的，根据奖励的增加或减少，在不同状态下选择不同的动作。当代理到达绿色方块时，它将获得正面奖励，学习到在之前的状态采取的动作是有利的。
- en: Q-Learning
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习过程从环境向代理提供一个状态开始。然后，代理在状态上执行一个动作。基于这个动作，环境将返回一个状态和一个奖励给代理，其中奖励可以是正的，使行为在未来更有可能发生，或者负的（即惩罚），使行为在未来不太可能发生。
- en: Q-Learning is one model of reinforcement learning, where a function ***Q(s,
    a)*** outputs an estimate of the value of taking action *a* in state *s*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![强化学习](../Images/8d0216ae95409f63035447749b2929bc.png)'
- en: 'The model starts with all estimated values equal to 0 (***Q(s,a)* = 0** for
    all *s, a*). When an action is taken and a reward is received, the function does
    two things: 1) it estimates the value of ***Q(s, a)*** based on current reward
    and expected future rewards, and 2) updates ***Q(s, a)*** to take into account
    both the old estimate and the new estimate. This gives us an algorithm that is
    capable of improving upon its past knowledge without starting from scratch.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 模型开始时所有估计的值都等于0（对于所有 *s, a*，***Q(s, a)* = 0**）。当采取一个动作并收到奖励时，函数做两件事：1）根据当前奖励和预期未来奖励估计
    ***Q(s, a)*** 的值，2）更新 ***Q(s, a)*** 以考虑旧估计和新估计。这给我们提供了一个算法，它能够在不从头开始的情况下改进其过去的知识。
- en: '***Q(s, a) ⟵ Q(s, a) + α(new value estimate - Q(s, a))***'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '***Q(s, a) ⟵ Q(s, a) + α(新值估计 - Q(s, a))***'
- en: The updated value of ***Q(s, a)*** is equal to the previous value of ***Q(s,
    a)*** in addition to some updating value. This value is determined as the difference
    between the new value and the old value, multiplied by α, a learning coefficient.
    When α = 1 the new estimate simply overwrites the old one. When α = 0, the estimated
    value is never updated. By raising and lowering α, we can determine how fast previous
    knowledge is being updated by new estimates.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后的 ***Q(s, a)*** 的值等于 ***Q(s, a)*** 的先前值加上一些更新值。这个值被确定为新值与旧值之间的差异，乘以学习系数 α。当
    α = 1 时，新估计简单地覆盖旧值。当 α = 0 时，估计值永远不会更新。通过提高和降低 α，我们可以确定旧知识通过新估计更新的速度。
- en: 'The new value estimate can be expressed as a sum of the reward (r) and the
    future reward estimate. To get the future reward estimate, we consider the new
    state that we got after taking the last action, and add the estimate of the action
    in this new state that will bring to the highest reward. This way, we estimate
    the utility of making action *a* in state *s* not only by the reward it received,
    but also by the expected utility of the next step. The value of the future reward
    estimate can sometimes appear with a coefficient gamma that controls how much
    future rewards are valued. We end up with the following equation:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 新的价值估计可以表示为奖励（r）和未来奖励估计的总和。为了得到未来奖励估计，我们考虑在执行最后一个动作后得到的新状态，并加上在这个新状态下执行的动作的估计，该动作将带来最高的奖励。这样，我们不仅通过接收到的奖励来估计在状态
    *s* 中执行动作 *a* 的效用，还通过下一步的预期效用来估计。未来奖励估计的值有时会与一个系数伽马（gamma）相关，该系数控制未来奖励的价值。最终我们得到以下方程：
- en: '![Q Learning Formula](../Images/e2cfe67996ea3c0e29e24a6daba1dcf3.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![Q 学习公式](../Images/e2cfe67996ea3c0e29e24a6daba1dcf3.png)'
- en: A **Greedy Decision-Making** algorithm completely discounts the future estimated
    rewards, instead always choosing the action ***a*** in current state ***s*** that
    has the highest ***Q(s, a)***.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**贪婪决策算法**完全忽略了未来估计的奖励，总是选择当前状态 *s* 中具有最高 ***Q(s, a)*** 的动作 ***a***。'
- en: This brings us to discuss the **Explore vs. Exploit** tradeoff. A greedy algorithm
    always exploits, taking the actions that are already established to bring to good
    outcomes. However, it will always follow the same path to the solution, never
    finding a better path. Exploration, on the other hand, means that the algorithm
    may use a previously unexplored route on its way to the target, allowing it to
    discover more efficient solutions along the way. For example, if you listen to
    the same songs every single time, you know you will enjoy them, but you will never
    get to know new songs that you might like even more!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了**探索与利用**的权衡。贪婪算法总是利用，采取已经确立的行动以带来好的结果。然而，它总是遵循相同的路径到解决方案，永远不会找到更好的路径。另一方面，探索意味着算法可能在通往目标的过程中使用之前未探索的路线，从而允许它沿途发现更有效的解决方案。例如，如果你每次都听相同的歌曲，你知道你会喜欢它们，但你永远不会了解你可能更喜欢的新歌曲！
- en: To implement the concept of exploration and exploitation, we can use the **ε
    (epsilon) greedy** algorithm. In this type of algorithm, we set ε equal to how
    often we want to move randomly. With probability 1-ε, the algorithm chooses the
    best move (exploitation). With probability ε, the algorithm chooses a random move
    (exploration).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现探索和利用的概念，我们可以使用**ε（epsilon）贪婪算法**。在这种类型的算法中，我们将 ε 设置为我们想要随机移动的频率。以 1-ε 的概率，算法选择最佳移动（利用）。以
    ε 的概率，算法选择一个随机移动（探索）。
- en: Another way to train a reinforcement learning model is to give feedback not
    upon every move, but upon the end of the whole process. For example, consider
    a game of Nim. In this game, different numbers of objects are distributed between
    piles. Each player takes any number of objects from any one single pile, and the
    player who takes the last object looses. In such a game, an untrained AI will
    play randomly, and it will be easy to win against it. To train the AI, it will
    start from playing a game randomly, and in the end get a reward of 1 for winning
    and -1 for losing. When it is trained on 10,000 games, for example, it is already
    smart enough to be hard to win against it.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 训练强化学习模型的另一种方法是，不是对每个移动给出反馈，而是在整个过程的结束时给出反馈。例如，考虑一个 Nim 游戏的例子。在这个游戏中，不同数量的物体分布在不同的堆中。每个玩家可以从任何单个堆中取走任意数量的物体，取走最后一个物体的玩家输。在这样的游戏中，未经训练的
    AI 会随机地玩，很容易战胜它。为了训练 AI，它将从随机玩游戏开始，并在最后获得1分的奖励（胜利）和-1分的奖励（失败）。例如，当它在10,000场比赛中训练后，它已经足够聪明，难以战胜。
- en: This approach becomes more computationally demanding when a game has multiple
    states and possible actions, such as chess. It is infeasible to generate an estimated
    value for every possible move in every possible state. In this case, we can use
    a **function approximation**, which allows us to approximate ***Q(s, a)*** using
    various other features, rather than storing one value for each state-action pair.
    Thus, the algorithm becomes able to recognize which moves are similar enough so
    that their estimated value should be similar as well, and use this heuristic in
    its decision making.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个游戏有多个状态和可能的行为，例如象棋时，这种方法在计算上变得更加复杂。在所有可能的状态中为每个可能的移动生成一个估计值是不切实际的。在这种情况下，我们可以使用**函数逼近**，这允许我们使用各种其他特征来逼近***Q(s,
    a)***，而不是为每个状态-动作对存储一个值。因此，算法能够识别出哪些移动足够相似，以至于它们的估计值也应该相似，并在其决策中使用这种启发式方法。
- en: Unsupervised Learning
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习
- en: In all the cases we saw before, as in supervised learning, we had data with
    labels that the algorithm could learn from. For example, when we trained an algorithm
    to recognize counterfeit notes, each banknote had four variables with different
    values (the input data) and whether it is counterfeit or not (the label). In unsupervised
    learning, only the input data is present and the AI learns patterns in these data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前看到的所有情况下，就像在监督学习中一样，我们都有算法可以从中学习的带标签的数据。例如，当我们训练一个算法来识别假币时，每张纸币都有四个不同值的变量（输入数据）以及它是否是假币（标签）。在无监督学习中，只有输入数据存在，AI从这些数据中学习模式。
- en: '**Clustering**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚类**'
- en: Clustering is an unsupervised learning task that takes the input data and organizes
    it into groups such that similar objects end up in the same group. This can be
    used, for example, in genetics research, when trying to find similar genes, or
    in image segmentation, when defining different parts of the image based on similarity
    between pixels.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种无监督学习任务，它将输入数据组织成组，使得相似的对象最终落在同一个组中。例如，在遗传学研究，当试图找到相似基因时，或者在图像分割中，根据像素之间的相似性定义图像的不同部分时，都可以使用这种方法。
- en: k-means Clustering
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-means 聚类
- en: k-means Clustering is an algorithm to perform a clustering task. It maps all
    data points in a space, and then randomly places k cluster centers in the space
    (it is up to the programmer to decide how many; this is the starting state we
    see on the left). Each cluster center is simply a point in the space. Then, each
    cluster gets assigned all the points that are closest to its center than to any
    other center (this is the middle picture). Then, in an iterative process, the
    cluster center moves to the middle of all these points (the state on the right),
    and then points are reassigned again to the clusters whose centers are now closest
    to them. When, after repeating the process, each point remains in the same cluster
    it was before, we have reached an equilibrium and the algorithm is over, leaving
    us with points divided between clusters.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 聚类是一种执行聚类任务的算法。它将空间中的所有数据点映射出来，然后在空间中随机放置k个聚类中心（由程序员决定数量；这是我们在左侧看到的起始状态）。每个聚类中心只是空间中的一个点。然后，每个聚类被分配所有比其他中心更接近其中心的点（这是中间的图片）。然后，在迭代过程中，聚类中心移动到所有这些点的中间（右侧的状态），然后点再次重新分配到中心现在最近的聚类。当重复这个过程后，每个点仍然保持在它之前所在的同一个聚类中，我们就达到了平衡，算法结束，我们得到了在聚类之间划分的点。
- en: '![k-means Clustering](../Images/3fbc91db5cb1da25bc49f6a2ef9c7f5e.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![k-means 聚类](../Images/3fbc91db5cb1da25bc49f6a2ef9c7f5e.png)'
